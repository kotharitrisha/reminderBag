entities={"216": {"session": "15:45 - 17:30", "abstract": "Multi-modal data is dramatically increasing with the fast growth of social media. Learning a good distance measure for data with multiple modalities is of vital importance for many applications, including retrieval, clustering, classification and recommendation. In this paper, we propose an effective and scalable multi-modal distance learning framework. Based on multi-wing harmonium model, our method provides a principled way to embed data of arbitrary modalities into a single latent space and distance supervision (e.g., labeled ``similar\\\" and ``dissimilar\\\" pairs ) are encoded in the latent space by minimizing the distance of similar pairs while pushing dissimilar pairs apart. The parameters are learned by jointly maximizing data likelihood of the random filed model and minimizing loss induced by distance supervision, which seeks balance of explaining data well and generating an effective distance metric and can naturally avoid overfitting. Our method is highly scalable and efficient. We specialize the general framework to text/image data. Experiments on retrieval and classification tasks demonstrate the effectiveness and scalability of our method.", "title": "Multi-modal Distance Learning", "authors": [{"name": "Xie Pengtao"}]}, "1269": {"session": "13:30 - 15:15", "abstract": "As social media become increasingly popular, more and more people express their opinions and comments (or collectively social content) on daily news, products, movies and various Web documents across different social media platforms such as news portals, forums, discussion groups, and blogs. Given a popular Web document discussing a couple of topics, it can attract huge amount of social contents, making it impossible to manually read through all of them to check which comments align with the corresponding topics. In this paper, we propose a novel two-phase framework that can be used to automatically detect the topics from Web document as well as align the detected topics with the social content, which can provide an overall picture on which topics in the news are \"hot\" that many people are discussing, and allow users to focus on those topics/social content that they are interested. Extensive experimental results show that our proposed framework significantly outperforms the existing state-of-the-art methods in social content alignment.", "title": "What Users Care about: a Framework for Social Content Alignment", "authors": [{"name": "Lei Hou"}, {"name": " Juanzi Li"}, {"name": " Xiaoli Li"}, {"name": " Jiangfeng Qu"}, {"name": " Xiaofei Guo"}, {"name": " Ou Hui"}, {"name": " Jie Tang"}]}, "215": {"session": "15:45 - 17:30", "abstract": "The emergence and ubiquity of online social net- works have enriched this data with evolving inter- actions and communities both at mega-scale and in real-time. This data offers an unprecedented oppor- tunity for studying the interaction between society and disease outbreak. In other words, the challenge we describe in this data paper is how to extract and leverage epidemic outbreak insights from mas- sive amounts of social media data to benefit medi- cal professionals, patients, and policymakers alike. Of the characteristics associated with this data, the size of the data, the speed at which the data is be- ing generated, and the complex heterogeneity of the structure of the data are three primary issues to the so-called \u0089\u00db\u00cfbig data\u0089\u00db\u009d problem, each of which is ade- quately addressed in this paper.", "title": "Detecting and Tracking Disease Outbreaks in Real-time through Social Media", "authors": [{"name": "Yusheng Xie"}]}, "1265": {"session": "13:30 - 15:15", "abstract": "Relational topic models have shown promise on analyzing document network structures and discovering latent topic representations. This paper presents three extensions: 1) unlike the common link likelihood model with a diagonal weight matrix that allows the-same-topic interactions only, we generalize it to use a full weight matrix that captures all pairwise topic interactions and is applicable to asymmetric networks; 2) instead of doing standard Bayesian inference, we perform regularized Bayesian inference with a regularization parameter to deal with the imbalanced link structure problem in common sparse networks; and 3) instead of using variational approximation methods with normally strict mean-field assumptions, we present a collapsed Gibbs sampling algorithm for the generalized relational topic models without making any restricting assumptions. Experimental results show that these extensions are important and they can significantly improve the prediction performance and time efficiency.", "title": "Generalized Relational Topic Models with Fast Sampling Algorithms", "authors": [{"name": "Ning Chen"}, {"name": " Jun Zhu"}, {"name": " Fei Xia"}, {"name": " Bo Zhang"}]}, "1264": {"session": "15:45 - 17:30", "abstract": "To accurately and actively provide users with their potentially interested information or services is the main task of a recommender system. Collaborative filtering is one of the most widely adopted recommender algorithms, whereas it is suffering the issues of data sparsity and cold start that will severely degrade quality of recommendations. To address such issues, this article proposes a novel methods, TrustMF, trying to improve the performance of collaborative filtering recommendation by means of elaborately integrating twofold sparse information, i.e., the conventional rating data given by users and the social trust network among the same users. TrustMF is a model-base method based on matrix factorization, which maps users into low-dimensional latent feature spaces in terms of their trust relationship, enabling to reflect users\u0089\u00db\u00aa reciprocal influence on their own ratings more reasonably. The validations against real world datasets show that TrustMF performs much better than state-of-the-art recommendation algorithms of social collaborative filtering by trust.", "title": "Social Collaborative Filtering by Trust", "authors": [{"name": "Bo Yang"}, {"name": " Yu Lei"}, {"name": " Dayou Liu"}, {"name": " Jiming Liu"}]}, "131": {"session": "15:45 - 17:30", "abstract": "Preventative and sustainable urban water pipe maintenance is a key activity as the pipe failure can bring significant environmental issue and economic loss, especially regarding with the large-scale urban networks. To achieve optimized replacement and rehabilitation, accurate pipe failure prediction plays an important role and has been attracting attention from both academia and industry, mostly from the civil engineering field.\n\n This paper presents a having-deployed industrial learning based pipe failure prediction system. As an alternative to the traditional risk matrix method depending on ad-hoc heuristics, computational models are evaluated using the massive attributes data such as pipe physical properties, environmental factors, operational conditions etc. However, challenge arises when attribute data is missing as is common in practice. The self-exciting stochastic process model is adopted to capture the temporal clustering pattern of pipe failures. To our best knowledge, we are unaware of any prior work has identified this formulation towards temporal pipe failure data modeling. And we show that it outperforms a baseline model assuming linear aging with respect to failure probability.", "title": "Towards Effective Prioritizing Water Pipe Replacement and Rehabilitation", "authors": [{"name": "Junchi Yan"}, {"name": " Yu Wang"}, {"name": " Ke Zhou"}, {"name": " Jin Huang"}, {"name": " Tian Chunhua"}, {"name": " Hongyuan Zha"}, {"name": " Weishan Dong"}]}, "139": {"session": "13:30 - 15:15", "abstract": "The obvious way to use several admissible heuristics in A* is to take their maximum. However, computing the additional heuristics incurs a significant overhead. We examine methods that reduces this overhead, with little or no\n\ndecrease in the effectiveness of the heuristics, thereby speeding up search. \n\n\n\nLazy A* is a variant of A* where heuristics are evaluated lazily, i.e, only when they become essential to a decision to be made in the A* search process.  We then present a new rational meta-reasoning based scheme, rational lazy A*. It uses a filter that decides whether to compute the more expensive heuristics at all, based on a myopic value of information estimate. \n\n\n\nBoth methods are examined theoretically, resulting in guidelines for predicting how well these methods are expected to perform given a search domain and set of heuristics. Empirical evaluation on several domains support the theoretical results, and show the benefit of the proposed approaches.\n\n", "title": "Towards rational deployment of multiple heuristics in A*", "authors": [{"name": "Solomon Shimony"}, {"name": " Erez Karpas"}, {"name": " Ariel Felner"}, {"name": " David Tolpin"}, {"name": " Tal Beja"}]}, "1775": {"session": "15:45 - 17:30", "abstract": "Schulze voting is a recently introduced voting system enjoying unusual\n\npopularity and a high degree of real-world use, with users including\n\nthe Wikimedia foundation, several branches of the Pirate Party, and\n\nMTV.  It is a Condorcet voting system that determines the winners of\n\nan election using information about paths in a graph representation of\n\nthe election.  We fully characterize the worst-case behavior of\n\nSchulze voting under control.  We find that it falls short of the best\n\nknown voting systems in terms of control resistance, demonstrating\n\nvulnerabilities of concern to some prospective users of the system.\n\n", "title": "Control Complexity of Schulze Voting", "authors": [{"name": "Curtis Menton"}, {"name": " Preetjot Singh"}]}, "1696": {"session": "15:45 - 17:30", "abstract": "Probabilistic logics combine the expressive power of logic with the ability to reason with uncertainty. Several probabilistic logic languages have been proposed in the past, each of them with their own features. In this paper, we propose a new probabilistic constraint logic programming language, which combines constraint logic programming with probabilistic reasoning. The language supports the modeling of discrete as well as continuous probability distributions by expressing constraints on random variables. We introduce the declarative semantics of this language, present an exact inference algorithm to derive bounds on the joint probability distribution, and give experimental results. The results obtained are encouraging, indicating that exact inference using this language is feasible for real-world problems.", "title": "Probabilistic Constraint Logic Programming", "authors": [{"name": "Steffen Michels"}, {"name": " Arjen Hommersom"}, {"name": " Peter Lucas"}, {"name": " Marina Velikova"}, {"name": " Pieter Koopman"}]}, "1776": {"session": "15:45 - 17:30", "abstract": "Dispersing a team of robots into an unknown and dangerous environment, such as a collapsed building, can provide information about structural damage and\n\nlocations of survivors and help rescuers plan their actions. We propose a rolling dispersion algorithm, which makes use of a small number of robots and achieves full exploration. The robots disperse as much as possible while maintaining communication, and then advance as a group, leaving behind beacons to mark explored areas and provide a path back to the entrance.  The novelty of this algorithm comes from manner in which the robots continue their exploration as a group after reaching the maximum dispersion possible while staying in contact with each other.  We use simulation to show that the algorithm works in multiple environments and for varying numbers of robots.", "title": "Rolling Dispersion for Robot Teams", "authors": [{"name": "Elizabeth Jensen"}, {"name": " Maria Gini"}]}, "1837": {"session": "15:45 - 17:30", "abstract": "The next generation of power systems face significant challenges, both in coping with increased loading of an aging infrastructure and incorporating renewable energy sources.  Meeting these challenges requires a fundamental change in the operation of power systems by replacing human-in-the-loop operations with autonomous systems.  This is especially acute in distribution systems, where renewable integration often occurs.  This paper investigates the automation of power supply restoration (PSR), that is, the process of optimally reconfiguring a faulty distribution grid to resupply customers.  The key contributions of the paper are (1) a flexible mixed-integer programming framework for solving PSR, (2) a model decomposition to obtain high-quality solutions within the required time constraints, and (3) an experimental validation of the potential benefits of incorporating optimisation technology into PSR operations.\n\n", "title": "Planning with MIP for Supply Restoration in Power Distribution Systems", "authors": [{"name": "Sylvie Thiebaux"}, {"name": " Carleton Coffrin"}, {"name": " Hassan Hijazi"}, {"name": " John Slaney"}]}, "29": {"session": "15:45 - 17:30", "abstract": "", "title": "Preference-based CBR: General Ideas and Basic Principles", "authors": [{"name": "Eyke Huellermeier and Weiwei Cheng"}]}, "543": {"session": "13:30 - 15:15", "abstract": "The ability to understand the goals and plans underlying the observed behaviour of other agents is an important characteristic of intelligent behaviours in many contexts.  One of the approaches used to endow agents with this capability is the weighed model counting approach. Given a plan library and a sequence of observations, this approach exhaustively lists plan executions models that are consistent with the observed behaviour. The probability that the agent might be pursuing a particular goal is then computed as a proportion of plan execution models satisfying the goal. This approach combines many interesting features that have proven difficult with alternative plan-library based approaches, such as recognizing multiple interleaved plans or handling temporal constraints and unobserved actions. However, the approach suffers from a combinatorial explosion of plan execution models, which impedes its application to real-world domains.  In this paper, we present a heuristic weighted model counting algorithm that limits the number of generated plan execution models in order to recognize most likely goals quickly, albeit initially with lower and upper bound estimates in terms of likelihood. ", "title": "Controlling the Hypothesis Space in Probabilistic Plan Recognition", "authors": [{"name": "Froduald Kabanza"}, {"name": " Julien Filion"}, {"name": " Abder Rezak Benaskeur"}, {"name": " Hengameh Irandoust"}]}, "540": {"session": "13:30 - 15:15", "abstract": "Nowadays, online social media has become ubiquitous in our daily\n\nlife, and there has been growing interest in mining useful\n\ninformation therein. Twitter, for example, can be viewed as a\n\ndistributed network of human sensors who may provide localized and\n\ntimely information that can be analyzed and aggregated to provide\n\nbroader insight. For example, people are inclined to tweet that they\n\nare currently stuck in traffic, or to tweet other information (such\n\nas their near-term travel plans) that may impact traffic patterns.\n\n\n\nIn this paper, we examine whether it is possible to use such information to develop improved near-term traffic predictions. We first analyze the correlation between traffic volume and tweet counts with various granularities. We then propose an optimization framework to extract traffic indicators based on tweet semantics using a sparse matrix, and incorporate them into traffic prediction via linear regression. Experimental results using traffic and Twitter data originated from the San Francisco Bay area of California demonstrate the effectiveness of our proposed framework.\n\n", "title": "Improving Traffic Prediction with Tweet Semantics", "authors": [{"name": "Jingrui He"}]}, "340": {"session": "15:45 - 17:30", "abstract": "Many formalisms discussed in the literature on qualitative spatial reasoning are\n\nonly designed for expressing static spatial constraints.\n\nHowever, dynamic situations arise in virtually all applications of these\n\nformalisms, which makes it necessary to study variants and extensions involving\n\nchange.\n\n\n\nThis paper presents a study on the computational complexity of qualitative\n\nchange. More precisely, we discuss the reasoning task to find a solution to a\n\ntemporal sequence of static reasoning problems where this sequence is subject to additional transition constraints.\n\nOur focus is primarily on smoothness and continuity constraints:\n\nwe show how such transitions can be defined as relations and expressed within\n\nqualitative constraint formalisms.\n\nOur results demonstrate that for point-based constraint formalisms the\n\ninteresting fragments become NP-hard in the presence of continuity constraints,\n\neven if the satisfiability problem of its static descriptions is tractable.", "title": "Transition Constraints: A Study on the Computational Complexity of Qualitative Change", "authors": [{"name": "Matthias Westphal"}, {"name": " Julien Hu&eacute;"}, {"name": " Stefan W&ouml;lfl"}, {"name": " Bernhard Nebel"}]}, "4": {"session": "13:30 - 15:15", "abstract": "", "title": "Declarative Pattern Mining using Constraint Programming", "authors": [{"name": "Tias Guns"}]}, "545": {"session": "13:30 - 15:15", "abstract": "This paper provides algorithms for predicting the size of the Expanded Search Tree (EST) of Depth-first Branch and Bound algorithms for optimization tasks. The prediction algorithm is implemented and evaluated in the context of solving combinatorial optimization over graphical models such as Bayesian and Markov networks. Our method extends to DFBnB the approaches provided by Knuth-Chen schemes that were designed and applied for predicting the size of ESTs of backtracking search algorithms. Our empirical results demonstrate good predictions which are superior to competing schemes.", "title": "Predicting the Size of Depth-First Branch and Bound Search Tree", "authors": [{"name": "Levi Lelis"}, {"name": " Lars Otten"}, {"name": " Rina Dechter"}]}, "1674": {"session": "13:30 - 15:15", "abstract": "While function symbols are widely acknowledged as an important feature in logic programming, they make common inference tasks undecidable. To cope with this problem, recent research has focused on identifying classes of programs imposing restrictions on the use of function symbols, but guaranteeing decidability of inference tasks. This has led to several criteria, called termination criteria, providing sufficient conditions for a program to admit finitely many finite stable models.\n\nThis paper introduces the new class of ``bounded programs'' guaranteeing the existence of finitely many finite stable models for programs in the class. The class of bounded programs is decidable and strictly includes the classes determined by current termination criteria. Different results on the correctness, the expressiveness, and the complexity of the class of bounded programs are presented.", "title": "Bounded Programs: A New Decidable Class of Logic Programs with Function Symbols", "authors": [{"name": "Sergio Greco"}, {"name": " Cristian Molinaro"}, {"name": " Irina Trubitsyna"}]}, "811": {"session": "15:45 - 17:30", "abstract": "This paper presents a reranking approach for combining constituent and dependency parsing, for the purpose of improving parsing performance on both sides. Most previous combination methods rely on complicated joint decoding to integrate together graph- and transition-based dependency models. Instead, our approach first makes use of a high-performance PCFG model to output k-best candidate constituent trees, and then a dependency parsing model to rerank the trees by summing up their scores from both models, so as to get the most probable parse. We evaluate this approach using the English (WSJ) and Chinese (CTB5.1) treebanks. Experimental results show that our reranking approach achieves the highest accuracy of constituent and dependency parsing on CTB5.1 and is comparable to the state-of-the-art systems on WSJ.", "title": "Combine Constituent and Dependency Parsing via Reranking", "authors": [{"name": "Xiaona Ren"}]}, "282": {"session": "15:45 - 17:30", "abstract": "We study Bayesian reinforcement learning (RL) as a solution of the exploration-exploitation dilemma. As full Bayesian planning is intractable except for special cases, previous work has proposed several approximation methods. However, these were often computationally expensive or limited to Dirichlet priors. In this paper, we propose a new algorithm that is fast and of polynomial time for near Bayesian optimal policy with any prior distributions that are not greatly misspecified. Perhaps even more interestingly, the proposed algorithm can naturally avoid being misled by incorrect beliefs, while effectively utilizing useful parts of prior information. It can work well even when an utterly misspecified prior is assigned. In that case, the algorithm will follow PAC-MDP behavior instead, if an existing PAC-MDP algorithm does so. The proposed algorithm naturally outperformed other existing algorithms on a standard benchmark problem.", "title": "Prior-Free Exploration Bonus for and beyond near Bayes-Optimal Behavior", "authors": [{"name": "Kenji Kawaguchi"}, {"name": " Hiroshi Sato"}]}, "1461": {"session": "15:45 - 17:30", "abstract": "Human activity recognition is a challenging machine vision task, with applications in human-robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. With the availability of the depth data from cheap sensors, like Microsoft\\'s Kinect, real-time skeleton joints extraction became available. In this paper, we exploit the 3D skeleton sequences obtained through a Kinect sensor to develop our recognition approach. A novel fixed-length trajectory descriptor is proposed. The descriptor consists of a hierarchical histogram of the directions in each projection of the 3D trajectory. Experiments on many datasets show that the descriptor outperforms the state of the art when using off-the-shelf classification tools.", "title": "Histogram of Oriented Displacements (HOD): Describing Trajectories of Human Joints for Action Recognition", "authors": [{"name": "Mohammad Gowayyed"}, {"name": " Marwan Torki"}, {"name": " Mohamed Hussein"}, {"name": " Motaz El-Saban"}]}, "1460": {"session": "13:30 - 15:15", "abstract": "The construction of a strong heuristic function is a central problem in heuristic search.  This paper studies the case where a set of heuristics are combined by maximizing over their values.  Due to resource constraints, this set of heuristics is often just a subset of a larger set of possible candidates.  Several algorithms have been proposed to select good subsets, but most lack an optimality criterion or are tied to a specific type of heuristic.  We propose an optimization approach that applies to any, possibly heterogeneous, set of candidate heuristics.  We equate minimizing a natural loss function with maximizing a utility function that is submodular and monotonic.  This implies that simple greedy selection under this utility function is guaranteed to be near-optimal.  We then extend the approach with a sampling scheme that retains provable optimality.  Our empirical results show significant improvements over existing strategies and bring new insight into constructing heuristics for directed domains.", "title": "Heuristic Subset Selection", "authors": [{"name": "Chris Rayner"}, {"name": " Nathan Sturtevant"}, {"name": " Michael Bowling"}]}, "1673": {"session": "13:30 - 15:15", "abstract": "We propose a method for learning causal relations within high-dimensional data in the form of multi-variate time-series as they are typically recorded in non-experimental databases. The method allows the simultaneous inclusion of numerous dimensions within the data analysis such as samples, time and domain variables construed as tensors. In such tensor data our method exploits and integrates results from non-Gaussian models and tensor analytic algorithms in a novel way. The work proves that we can determine simple causal relations independently of how complex the dimensionality of the data is. More specifically, we rely on a statistical decomposition that flattens higher dimensional data tensors into matrices. This decomposition preserves the causal information and is therefore suitable to be included in the structure learning process of causal graphical models, where a causal relation can be generalised beyond dimension, for example, over all points in time. Related methods either focus on a set of samples for instantaneous effects or look at one sample for effects at certain points in time. We evaluate the resulting algorithm and discuss its performance both with synthetic and real-world data.", "title": "Multi-dimensional Causal Discovery", "authors": [{"name": "Ulrich Schaechtle"}, {"name": " Kostas Stathis"}, {"name": " Stefano Bromuri"}]}, "1799": {"session": "15:45 - 17:30", "abstract": "Users may ask a service robot to accomplish various tasks so that the designer of the robot cannot program each of the tasks beforehand. Robot autonomous planning has been proposed as a means to overcome this difficulty, but in turn leads to the notorious bottleneck problem of knowledge acquisition. As more and more open-source knowledge resources become available on the web, it is worthwhile trying to make use of open knowledge, i.e., the knowledge from these open-source knowledge resources for robot planning. This paper presents our up-to-date progress along this research line. To provide a basis for searching the missing knowledge, we formulize the knowledge gaps between a user task and the robot\u0089\u00db\u00aas local knowledge and provide the complexity results of the main reasoning problems for filling the knowledge gaps. A second contribution is to introduce techniques for recursive planning with the open knowledge provided by non-experts in semi-structured natural language. Techniques for translating the semi-structured knowledge from an open-source knowledge base, OMICS, are also presented. Experiments on robot task planning based on the proposed techniques are reported with some lessons on handling open knowledge for service robots.", "title": "Handling Open Knowledge for Service Robots", "authors": [{"name": "Xiao-Ping Chen"}, {"name": " Jian-Min Jin"}, {"name": " Zhi-Qiang Sui"}, {"name": " Jiong-Kun Xie"}]}, "628": {"session": "15:45 - 17:30", "abstract": "We study the complexity of (approximate) winner determination under Monroe\u0089\u00db\u00aas and Chamberlin-Courant\u0089\u00db\u00aas multiwinner voting rules, where we focus on the total (dis)satisfaction of the voters (the utilitarian case) or the (dis)satisfaction of the worst-off voter (the egalitarian case). We show good approximation algorithms for the satisfaction-based utilitarian cases, and inapproximability results for the remaining settings.", "title": "Fully Proportional Representation as Resource Allocation: Approximability Results", "authors": [{"name": "Piotr Skowron"}, {"name": " Piotr Faliszewski"}, {"name": " Arkadii Slinko"}]}, "719": {"session": "15:45 - 17:30", "abstract": "We investigate the problem of deciding whether a given preference profile is close to a nicely structured preference profile of a certain type, as for instance single-peaked, single-caved, single-crossing, value-restricted, best-restricted, worst-restricted, medium-restricted, or group-separable profiles. We measure this distance by the number of voters or alternatives that have to be deleted so as to reach a nicely structured profile. Our results classify all considered problem variants with respect to their computational complexity, and draw a clear line between computationally tractable (polynomial time solvable) and computationally intractable (NP-hard) questions.", "title": "Are there any nicely structured preference profiles nearby?", "authors": [{"name": "Robert Bredereck"}, {"name": " Jiehua Chen"}, {"name": " Gerhard J. Woeginger"}]}, "1691": {"session": "13:30 - 15:15", "abstract": "We propose a unified approach to plan execution and schedule dispatching that converts a plan, which has been augmented with temporal constraints, into a policy for dispatching. Our approach generalizes the original plan and temporal constraints so that the executor need only consider the subset of state that is relevant to successful execution of a large multitude of valid plan fragments. We can accommodate a variety of calamitous and serendipitous changes to the state of the world by supporting the seamless re-execution or omission of plan fragments, without the need for costly replanning. Our methodology for plan generalization and online dispatching is a novel combination of plan execution and schedule dispatching techniques. We demonstrate the effectiveness of our method through a prototype implementation and a series of experiments.", "title": "Flexible Execution of Partial Order Plans With Temporal Constraints", "authors": [{"name": "Christian Muise"}, {"name": " J. Christopher Beck"}, {"name": " Sheila McIlraith"}]}, "1098": {"session": "15:45 - 17:30", "abstract": "In this paper, we consider the inclusion-exclusion rule -- a known yet forgotten rule of probabilistic inference. Unlike the widely used sum rule of probabilistic inference which requires easy access to all joint probability values (i.e., a value proportional to the probability of the assignment of values to all variables), the inclusion-exclusion rule requires easy access to several marginal probability values (i.e., the probability of the assignment of values to a subset of variables). We therefore develop a new representation of the joint distribution that is amenable to the inclusion-exclusion rule. We compare the relative strengths and weaknesses of the inclusion-exclusion rule with the sum rule and develop a hybrid rule called the inclusion-exclusion-sum (IES) rule, which combines their power. We apply the IES rule to junction trees, treating the latter as a target for knowledge compilation and show that it greatly reduces query complexity (online step) in some cases at increased compilation cost (offline step). Our experiments demonstrate the power of our new approach. In particular, at query time, our new scheme was an order of magnitude faster than the junction tree algorithm on several benchmark probabilistic graphical models.", "title": "The Inclusion-Exclusion Rule and its Application to the Junction Tree Algorithm", "authors": [{"name": "David Smith"}, {"name": " Vibhav Gogate"}]}, "120": {"session": "13:30 - 15:15", "abstract": "Change-point detection is the problem of finding abrupt changes in time-series, and it is attracting a lot of attention in the artificial intelligence and machine learning communities. In this paper, we present a supervised learning based change-point detection approach in which we use separability of the past and future data at time t (they are labeled as +1 and -1) as a plausibility of a change-points. Based on this framework, we propose a detection measure called \\\\emph{additive Hilbert-Schmidt Independence Criterion} (aHSIC), which is defined as an weighted sum of HSIC scores between each feature and its corresponding binary labels. Here, HSIC is a kernel-based independence measure. A novelty of the aHSIC score is that it can incorporate feature selection during its detection measure estimation. More specifically, we first select features that are responsible for an abrupt change by supervised manner, and then compute the aHSIC score by those selected features. Thus, compared with traditional detection measures, our approach tends to be robust to noise features, and thus the aHSIC is suited for a high-dimensional time-series data.  Through extensive experiments on synthetic and real-world human activity data set, we demonstrated that the proposed change-point detection method is promising.", "title": "Change-Point Detection with Feature Selection  in High-Dimensional Time-Series Data", "authors": [{"name": "Makoto Yamada"}, {"name": " Akisato Kimura"}, {"name": " Futoshi Naya"}, {"name": " Hiroshi Sawada"}]}, "123": {"session": "15:45 - 17:30", "abstract": "Pose variation is one of the challenging factors for face recognition. In this paper, we propose a novel cross pose face recognition method named as Regularized Latent Least Square Regression (RLLSR). The basic assumption is that the images captured under different poses of one person can be viewed as pose-specific transforms of a single ideal object. We treat the observed images as regressor, the ideal object as response, and then formulate this assumption in the least square regression framework, so as to learn the multiple pose-specific transforms. Specifically, we incorporate some prior knowledge as two regularization terms into the least square approach: 1) the smoothness regularization, as the transforms for nearby poses should not differ too much; 2) the local consistency constraint, as the distribution of the latent ideal objects should pre-serve the geometric structure in the observed image space. We develop an iterative algorithm to simul-taneously solve for the ideal objects of the training individuals and a set of pose-specific transforms. The experimental results on the Multi-PIE dataset demonstrate the effectiveness of the proposed method and superiority over the previous methods.", "title": "Regularized Latent Least Square Regression for Cross Pose Face Recognition ", "authors": [{"name": "xinyuan cai"}, {"name": " chunheng wang"}, {"name": " Baihua Xiao"}]}, "124": {"session": "13:30 - 15:15", "abstract": "In the AGM framework, a revision function can be defined directly\n\nfrom systems of spheres, epistemic entrenchment,\n\nand etc., or indirectly through a contraction\n\noperation via the Levi identity. A recent trend is\n\nto construct AGM style contraction and revision\n\nfunctions that operate under Horn logic. A direct\n\nconstruction of Horn revision is given in [Delgrande\n\nand Peppas, 2011]. However, it is unknown\n\nwhether Horn revision can be defined indirectly\n\nfrom Horn contraction. In this paper, we address\n\nthis problem by obtaining a model-based Horn revision\n\nthrough the model-based Horn contraction\n\nstudied in [Zhuang and Pagnucco, 2012]. Our result\n\nshows that, under proper restriction, Horn revision\n\nis definable through Horn contraction.", "title": "Definability of Horn Revision from Horn Contraction", "authors": [{"name": "Zhiqiang Zhuang"}, {"name": " Maurice Pagnucco"}, {"name": " Yan Zhang"}]}, "127": {"session": "15:45 - 17:30", "abstract": "Part of the long lasting cultural heritage of China is the classical ancient Chinese poems following strict formats and linguistic representations. Automatic Chinese poetry composition is viewed as a difficult problem in computational linguistics and requires high Artificial Intelligence assistance, and has not been well addressed. In this paper, we formulate the poetry composition task as an optimization problem based on a generative summarization framework under several constraints. Given the user specified writing intents and selected formats, the system retrieves candidate terms from large poem thesaurus, and then fits the format with possible combinations of candidate terms. The optimization process under constraints is conducted via iterative term substitutions till convergence, and outputs the subset with the highest utility as the generated poem. We perform generation on large datasets of 61,960 classic poems from Tang and Song Dynasty of China. A comprehensive evaluation, using both human judgments and ROUGE scores, has been conducted, and the results demonstrate the effectiveness of our proposed approach.", "title": "i, Poet: Chinese Poetry Composition through a Summarization Framework under Constrained Optimization", "authors": [{"name": "Rui Yan"}, {"name": " Han Jiang"}, {"name": " Mirella Lapata"}, {"name": " Shou-De Lin"}, {"name": " Xiaoming Li"}]}, "1498": {"session": "15:45 - 17:30", "abstract": "Bracketing induction is the unsupervised learning of hierarchical constituents without labeling their syntactic categories such as verb phrase (VP) from natural raw sentences. Constituent Context Model (CCM) is an effective generative model for the bracketing induction, but the CCM computes probability of a constituent in a very straightforward way no matter how long this constituent is. Such method causes severe data sparse problem because long constituents are more unlikely to appear in test set. To overcome the data sparse problem, this paper proposes to define a non-parametric Bayesian prior distribution, namely the Pitman-Yor Process (PYP) prior, over constituents for constituent smoothing. The PYP prior functions as a back-off smoothing method through using a hierarchical smoothing scheme (HSS). Various kinds of HSS are proposed in this paper. We find that two kinds of HSS are effective, attaining or significantly improving the state-of-the-art performance of the bracketing induction evaluated on standard treebanks of various languages, while the easiest coming to mind HSS by using n-gram Markovization on constituents is not effective for improving the bracketing induction performance.", "title": "Smoothing for Bracketing Induction", "authors": [{"name": "Duan Xiangyu"}]}, "1605": {"session": "15:45 - 17:30", "abstract": "We present a bimodal method for online planning in partially observable multiagent settings as formalized by finitely-nested interactive partially observable Markov decision process. The approach is significantly faster on larger environments than the previous online planning approach. An agent planning in an environment shared with another updates beliefs both over the physical state and the other agents\u0089\u00db\u00aa models. In problems where we do not observe other\u0089\u00db\u00aas actions explicitly but must infer it from sensing its effect on the state, observations are more informative on the other agent when the belief over the state space has reduced uncertainty. For initial beliefs with high uncertainty about the state, we model the agent as if it were acting alone and utilize fast online planning for POMDPs. Subsequently, the agent switches to online planning in multiagent settings. We maintain lower and upper bounds at each step that are computed quickly, and switch over when the difference between them reduces to less than &amp;#491;. If the online planning is performed exactly, we may bound the error of this approach. We show favorable results in a large UAV reconnaissance problem.\n\n", "title": "Bimodal Switching for Online Planning in Multiagent Settings", "authors": [{"name": "Ekhlas Sonu"}, {"name": " Prashant Doshi"}]}, "1550": {"session": "13:30 - 15:15", "abstract": "We address in this work two significant drawbacks of state-of-the-art solvers of decentralized POMDPs (DEC-POMDPs): the reliance on complete knowledge of the model and the limited scalability as the complexity of the domain grows.  We extend a recently proposed approach for solving DEC-POMDPs via a reduction to the maximum likelihood problem, which is then solved using EM.  We introduce a model-free version of this approach that employs Monte-Carlo EM (MCEM).  While a naive application of MCEM is inadequate in multi-agent settings, we introduce several improvements in sampling that produce high-quality results on a variety of DEC-POMDP benchmarks.", "title": "Monte-Carlo Expectation Maximization for Decentralized POMDPs", "authors": [{"name": "Feng Wu"}, {"name": " Shlomo Zilberstein"}]}, "59": {"session": "13:30 - 15:15", "abstract": "In the seminal work [Plaza 89], Plaza proposed the public announcement logic (PAL), which is considered as the pilot logic in the field of dynamic epistemic logic. In the same paper, Plaza also introduced an interesting \"know-value\" operator Kv  and proposed a proof system of PAL+Kv. However, the completeness of that system has been open since then. In this paper, we show that this proof system is not complete. Moreover, we generalize the Kv operator and give a complete proof system for PAL plus this generalized operator based on a complete axiomatization of epistemic logic with the same operator in the single-agent setting. We also make a thorough comparison of the expressive powers of the various related logics.", "title": "Knowing That, Knowing What, and Public Communication: Public Announcement Logic with Kv Operators", "authors": [{"name": "Yanjing Wang"}, {"name": " Jie Fan"}]}, "57": {"session": "13:30 - 15:15", "abstract": "During the past decade, finite mixture modeling has become a well-established technique in data analysis and clustering. This paper focus on developing a variational inference framework to learn finite Beta-Liouville mixture models that have been proposed recently as an efficient way for proportional data clustering. In contrast to the conventional expectation maximization (EM) algorithm, commonly used for learning finite mixture models, the proposed algorithm has the advantages that it is more efficient from a computational point of view and by preventing over- and under-fitting problems. Moreover, the complexity of the mixture model (i.e. the number of components) can be determined automatically and simultaneously with the parameters estimation in a closed form as part of the Bayesian inference procedure. The merits of the proposed approach are shown using both artificial data sets and two interesting and challenging real applications namely dynamic textures clustering and facial expression recognition.", "title": "Learning Finite Beta-Liouville Mixture Models via Variational Bayes for Proportional Data Clustering", "authors": [{"name": "Wentao Fan"}, {"name": " Nizar Bouguila"}]}, "53": {"session": "13:30 - 15:15", "abstract": "Sequential anomaly detection is a challenging problem due to the one-class nature of the data (i.e., data is collected from only one class) and the temporal dependence in sequential data. We present one-class conditional random fields (OCCRF) for sequential anomaly detection that learn from a one-class dataset and capture the temporal dependence structure in it. We propose a hinge loss in a regularized risk minimization framework which requires that for each sequence the difference between the sequence being ``normal\\'\\' rather than ``abnormal\\'\\' be as large as possible. This allows our model to accept most of the training data, yet keeping the solution as tight as possible. Experimental results on various real-world datasets show our model outperforming several baselines, including Active Outlier, Local Outlier Factor, One-class SVM, and HMM. We also report an exploratory study on detecting abnormal organizational behavior in enterprise social networks.", "title": "One-Class Conditional Random Fields for Sequential Anomaly Detection", "authors": [{"name": "Yale Song"}, {"name": " Zhen Wen"}, {"name": " Ching-Yung Lin"}, {"name": " Randall Davis"}]}, "537": {"session": "15:45 - 17:30", "abstract": "Persuasion is a common social and economic activity. It usually arises when conflicting interests among agents exist, and one of the agents wishes to sway the opinions of others. This paper considers the problem of an automated agent which needs to influence the decision of a group of self-interested agents that must reach an agreement on a joint action. For example, consider an automated agent that aims to reduce the energy consumption of a nonresidential building, by convincing a group of people who share an office to agree on an economy mode of the air-conditioning and low light intensity. In this paper we present four problems that address issues of minimality and safety of the persuasion process. We present the relationships to similar problems from social choice, and show that if the agents are using Plurality or Veto as their voting rule all of our problems are in P. We also show that with $k$-Approval, Bucklin and Borda voting rules some problems become intractable. We thus present heuristics for an efficient persuasion with Borda, and evaluate them through simulations.", "title": "How to Persuade a Group to Change its Collective Decision?", "authors": [{"name": "Noam Hazon"}, {"name": " Raz Lin"}, {"name": " Sarit Kraus"}]}, "373": {"session": "13:30 - 15:15", "abstract": "The cake cutting problem models the fair division of a heterogeneous good between multiple agents. Previous work assumes that each agent derives value only from its own piece. However, agents may also care about the pieces assigned to other agents; such externalities naturally arise in fair division settings. We extend the classical model to capture externalities, and generalize the classical fairness notions of proportionality and envy-freeness. Our technical results characterize the relationship between these generalized properties, establish the existence or nonexistence of fair allocations, and explore the computational feasibility of fairness in the face of externalities. ", "title": "Externalities in Cake Cutting", "authors": [{"name": "Simina Branzei"}, {"name": " Ariel Procaccia"}, {"name": " Jie Zhang"}]}, "292": {"session": "15:45 - 17:30", "abstract": "In this paper, the fixed point semantics developed in [Lobo et al., 1992, Foundations of Disjunctive Logic Programming] is generalized to disjunctive logic programs with default negation and over non-Herbrand structures. Based on this semantics, the boundedness of a disjunctive logic program is naturally defined. By using the tool of ultraproducts, a preservation theorem, which asserts that a disjunctive logic program without default negation is bounded if and only if it has a first-order equivalent, is then obtained. For disjunctive logic programs with default negation, a sufficient condition to assure the first-order expressibility is also proposed.", "title": "First-Order Expressibility and Boundedness of Disjunctive Logic Programs", "authors": [{"name": "Heng Zhang"}, {"name": " Yan Zhang"}]}, "377": {"session": "15:45 - 17:30", "abstract": "There is increasing awareness in the planning community that the burden of specifying complete domain models is too high, which impedes the applicability of planning technology in many real world domains. Although there have been many learning approaches that help automatically creating domain models, they all assume plan traces (training data) are \\emph{correct}. In this paper, we would like to remove the assumption, allowing plan traces to be with noises. Compared to collecting large amount of correct plan traces, it is much easier to collect noisy plan traces, e.g., sensors can be used to observe noisy plan traces. We consider a novel solution for this challenge that can learn action models from noisy plan traces. We create a set of random variables to capture the possible correct plan traces behind the observed noisy ones, and build a graphical model to describe the physics of the domain. We will present the details of our approach and present an empirical evaluation of our method in comparison to a state-of-the-art learning system that depends on correct plan traces.", "title": "Action-Model Acquisition from Noisy Plan Traces", "authors": [{"name": "Hankz Hankui Zhuo"}, {"name": " Rao Kambhampati"}]}, "927": {"session": "15:45 - 17:30", "abstract": " In this paper, we revisit the idea of splitting a planning problem into subproblems hopefully easier to solve with the help of landmark analysis. This technique initially proposed in the first approaches related to landmarks in classical planning has been outperformed by landmark-based heuristics. But we believe that it is still a promising research direction. To this end, we propose a new method for problem splitting based on landmarks, which has two advantages over the original technique: it is complete (if a solution exists, the algorithm finds it), and it uses the precedence relations over the landmarks in a more flexible way. We lay in this paper the foundations of a meta best-first      search algorithm, which explores the landmark orderings and can use any embedded planner to solve each subproblem. It opens up avenues for future research: among them are new heuristics for guiding the meta search towards the most promising orderings, different policies for expanding nodes of the meta search, influence of the embedded subplanner, and parallelization strategies of the meta search.\n\n", "title": "Problem Splitting using Heuristic Search in Landmark Orderings", "authors": [{"name": "Simon Vernhes"}, {"name": " Guillaume Infantes"}, {"name": " Vincent Vidal"}]}, "199": {"session": "15:45 - 17:30", "abstract": "We present various new concepts and results related to abstract dialectical frameworks (ADFs), a powerful generalization of Dung's argumentation frameworks (AFs). In particular, we show how the existing definitions of stable and preferred semantics which are restricted to the subcase of so-called bipolar ADFs can be improved and generalized to arbitrary frameworks. Furthermore, we introduce preference handling methods for ADFs, allowing for both reasoning with and about preferences. Finally, we present an implementation based on an encoding in answer set programming.", "title": "Abstract Dialectical Frameworks Revisited", "authors": [{"name": "Gerhard Brewka"}, {"name": " Stefan Ellmauthaler"}, {"name": " Hannes Strass"}, {"name": " Johannes Wallner"}, {"name": " Stefan Woltran"}]}, "596": {"session": "15:45 - 17:30", "abstract": "Abstract Parsing human poses in images is one of the most important computational problems in deriving critical visual information for artificial intelligent agents. This paper focuses the learning of the body part models, in this parsing process, which we call visual symbols. These visual symbols are self-contained, the context between symbols are well defined, and they can be easily manipulated and intepreted. The key idea is to compute symbols effectively by categorizing visual information using cross validation, and then define geometric context by the agreements between symbols. In our experiment, we present a three level hierarchy representation. Following human kinematic strucuture, we derive an approach to effectively estimate human poses. Our method outperform the state of art method, yet the parsing results are still simple enough to easily intepretate and understand.", "title": "Learning Visual Symbols for Parsing Human Pose in Images", "authors": [{"name": "Fang Wang"}, {"name": " Yi Li"}]}, "194": {"session": "13:30 - 15:15", "abstract": "Parameterized linear systems allow %are a representation formalism\n\nfor modelling and reasoning over classes of polyhedra. Collections\n\nof squares, rectangles, polytopes, and so on, can readily be\n\ndefined by means of linear systems with parameters. In this paper,\n\nwe investigate the problem of learning a parameterized linear\n\nsystem whose class of polyhedra includes a given set of example\n\npolyhedral sets and it is minimal.", "title": "Learning from Polyhedral Sets", "authors": [{"name": "Salvatore Ruggieri"}]}, "981": {"session": "15:45 - 17:30", "abstract": "Monitoring and forecast of global spread of infectious diseases is difficult, mainly due to lack of fine-grained and timely data. Previous work in computational epidemiology has shown that mining data from the web can improve the predictability of high-level aggregate patterns of epidemics. By contrast, this paper explores how individuals \\emph{contribute} to the global spread of disease. We consider the important task of predicting the prevalence of flu-like diseases in a given city based on interpersonal interactions of the city's residents with the outside world. We use the geo-tagged status updates of traveling Twitter users to infer properties of the flow of individuals between cities.  While previous research considered only the volume of passenger flow, by data mining the Twitter stream we can estimate a number of latent variables, including the number of sick (symptomatic) travelers and the number of sick individuals to whom each traveler was exposed. Our experiments involve over 51,000 individuals traveling between 75 cities before and during a severe ongoing flu epidemic (November 2012 - January 2013). We show that 73\\% of variance in local flu indices of individual cities is explained by tracking infected travelers---a significant improvement over baseline alternatives and past results. Our model leverages the text and interpersonal interactions recorded in over 6.5 million online status updates without any active user participation, enabling scalable public health applications.", "title": "Towards Understanding Global Spread of Disease from Everyday Interpersonal Interactions", "authors": [{"name": "Sean Brennan"}, {"name": " Adam Sadilek"}, {"name": " Henry Kautz"}]}, "1325": {"session": "15:45 - 17:30", "abstract": "The paper focuses on an extension of the CSP optimization framework tailored to identify \"fair\" solutions to instances involving multiple optimization functions. Two approaches are studied, based on selecting the solutions maximizing the minimum value over all the given functions (Max-Min approach) and on the lexicographical refinement where, over all solutions maximizing the minimum value, those maximizing the second minimum value are chosen, and so on, until all functions are considered (lexMax-Min). For both approaches, the computational complexity of computing an optimal solution is analyzed and the tractability frontier is charted, according to the structure of the constraint scope interactions and the number and the domains of the functions to be optimized. While the setting turns out to be NP-hard in general, the paper identifies large islands of tractability based on a notion of guardedness.", "title": "Constraint Satisfaction and Fair Multi-Objective Optimization Problems", "authors": [{"name": "Gianluigi Greco"}, {"name": " Francesco Scarcello"}]}, "111": {"session": "15:45 - 17:30", "abstract": "Consensus clustering emerges as a promising solution to find cluster structures from data. As an efficient approach for consensus clustering, the Kmeans based method has garnered attention in the literature, but the existing research is still preliminary and fragmented. In this paper, we provide a systematic study on the framework of K-meansbased Consensus Clustering (KCC). We first formulate the general definition of KCC, and then reveal a necessary and sufficient condition for utility functions that work for KCC, on both complete and incomplete basic partitionings. Experimental results on various real-world data sets demonstrate that KCC is highly efficient and is comparable to the state-of-the-art methods in terms of clustering quality. In addition, KCC shows high robustness to incomplete basic partitionings with substantial missing values.", "title": "A Theoretic Framework of K-means-based Consensus Clustering", "authors": [{"name": "Junjie Wu"}, {"name": " Hongfu Liu"}, {"name": " Hui Xiong"}]}, "275": {"session": "15:45 - 17:30", "abstract": "In this paper, a new unsupervised feature selection method, i.e., Robust Unsupervised Feature Selection (RUFS), is proposed. Unlike traditional unsupervised feature selection methods, pseudo cluster labels are learned via local learning regularized robust nonnegative matrix factorization. During the cluster label learning process, feature selection is performed simultaneously by robust joint $l_{2, 1}$ norms minimization. To learn more accurate cluster labels, we add an orthogonal constraint on the indicator matrix, resulting in more discriminative and ideal cluster labels. Since RUFS utilizes $l_{2, 1}$ norm minimization on both label learning and feature learning, outliers and noise could be effectively handled and redundant or noisy features could be effectively reduced. Our method adopts the advantages of both robust nonnegative matrix factorization, local learning, and robust feature learning. In order to make RUFS be more applicable practically, we design a (projected) limited-memory BFGS based iterative algorithm to efficiently solve the optimization problem of RUFS in terms of both memory consumption and computation complexity. Experimental results on different benchmark real world datasets show the promising performance of RUFS over the state-of-the-arts.", "title": "Robust Unsupervised Feature Selection", "authors": [{"name": "Mingjie Qian"}, {"name": " Chengxiang Zhai"}]}, "276": {"session": "15:45 - 17:30", "abstract": "Answer set programming is the most appreciated framework for non-monotonic reasoning. Stable model semantics, as the semantics behind this success, has been subject to many extensions. Equilibrium models and FLP semantics are the two main extensions to stable model semantics. Despite their very interesting foundations, they both still suffer from two problems: intended models according to such extensions (1) are not guaranteed to be minimal, and (2) more importantly, may have self-justifications (i.e., inclusion of an atom in an intended model might be justfied by its own pertinence). Both of these properties directly violate the spirit of stable model semantics.\n\n\n\nPresent paper introduces an extension to stable model semantics, called supported semantics (based on derivability in intuitionistic propositional logic), that guarantees both the minimality of a model and its being well-justified. We also discuss how supported models relate to other existing semantics for non-monotonic reasoning including equilibrium models. Last, but not the least, we discuss the complexity of reasoning about supported models and show that, despite using full intuitionistic reasoning for the wide class of propositional programs, the complexity of brave/cautious reasoning in supported semantics remains the same as such reasonings in equilibrium models, i.e., the property of being well-justified comes for no additional computational cost.", "title": "Extending Stable Models to the Full Propositional Language with Justifications", "authors": [{"name": "Tasharrofi Shahab"}]}, "83": {"session": "15:45 - 17:30", "abstract": "In many real world scenarios, active learning methods are used to select the most representative  points for labeling to reduce the expensive human actions. Traditional active learning methods fail to get a better performance without considering the local geometrical structure properly. In this paper, we propose a novel framework named  Active Learning based on Local Representation (ALLR) by taking into account the locality information directly during the sampling. Different from most of active learning methods adopting a greedy sequential strategy for optimization, we further develop an efficient two-stage iterative procedure to solve the final optimization problem efficiently. Our empirical study shows encouraging results of the proposed algorithms in comparison to other state-of-the-art active learning algorithms on both synthetic and real visual data sets.", "title": "Active Learning based on Local Representation", "authors": [{"name": "Yao Hu"}, {"name": " Debing Zhang"}]}, "118": {"session": "13:30 - 15:15", "abstract": "Among the many approaches for reasoning about degrees of belief in\n\nthe presence of noisy sensing and acting, the logical account\n\nproposed by Bacchus, Halpern, and Levesque is perhaps the most expressive.\n\nWhile their formalism is quite general, it is restricted to fluents\n\nwhose values are drawn from discrete countable domains, as opposed to\n\nthe continuous domains seen in many robotic applications. In this\n\npaper, we show how this limitation in their approach can be lifted.\n\nBy dealing seamlessly with both discrete distributions and continuous\n\ndensities within a rich theory of action, we provide a very general\n\nlogical specification of how belief should change after acting and\n\nsensing in complex noisy domains.", "title": "Reasoning about Continuous Uncertainty in the Situation Calculus", "authors": [{"name": "Vaishak Belle"}, {"name": " Hector Levesque"}]}, "84": {"session": "13:30 - 15:15", "abstract": "Nowadays, Nearest Neighbor Search becomes more and more important when facing the challenge of big data. Traditionally, to solve this problem, Reserchers are mainly focusing on building effective tree structures such as kd-tree or using hashing methods to acclerate the query time. In this paper, we propose a novel unified approximate nearest neighbor search scheme to combine the advantages of both the effective tree structure and the fast hamming distance computation in hashing methods. In this way, the searching procedure in the tree structures can be further acclerated. Computational complexity analysis and extensive experiments have demonstrated the effectiveness of our proposed scheme.", "title": "A Unified Approximate Nearest Neighbor Search Scheme by Combining Data Structure and Hashing", "authors": [{"name": "Debing Zhang"}, {"name": " Genmao Yang"}, {"name": " Yao Hu"}, {"name": " Deng Cai"}, {"name": " Xiaofei He"}]}, "1859": {"session": "15:45 - 17:30", "abstract": "  When using graphical models for decision making, often the presence \n\n  of unobserved variables may hinder our ability to reach the correct\n\n  decision. For a decision maker, a fundamental question is whether or not\n\n  one is ready to make a decision (stopping criteria), and if not, what\n\n  observations should be made in order to better prepare for a decision\n\n  (selection criteria).  A recently introduced notion, called the\n\n  Same-Decision Probability (SDP), has been shown to be useful\n\n  as both a stopping criteria and selection criteria.\n\n  This query has been proven to be highly intractable,\n\n  as it has been   shown to be a PP^PP-complete problem. \n\n  We propose a novel algorithm for computing the SDP, and demonstrate\n\n  its effectiveness  on several  real and synthetic networks.\n\n  Furthermore, we present a new complexity result for computing the SDP.", "title": "An Exact Algorithm for Computing the Same-Decision Probability", "authors": [{"name": "Suming Chen"}, {"name": " Arthur Choi"}, {"name": " Adnan Darwiche"}]}, "1634": {"session": "15:45 - 17:30", "abstract": "This paper is devoted to complexity results regarding specific measures of proximity to single-peakedness and single-crossingness, called \"single-peaked width\" and \"single-crossing width\" [Cornaz et al., 2012]. Thanks to the use of the PQ-tree data structure [Booth and Lueker, 1976], we show that boths problems are polynomial time solvable in the general case (while it was only known for single-peaked width and in the case of \\emph{narcissistic preferences}). Furthermore, we establish one of the first results (to our knowledge) concerning the effect of nearly single-peaked electorates on the complexity of an NP-hard voting system, namely we show the fixed-parameter tractability of Kemeny elections with respect to the parameters \"single-peaked width\" and \"single-crossing width\".", "title": "Kemeny Elections with Bounded Single-peaked or Single-crossing Width", "authors": [{"name": "Denis Cornaz"}, {"name": " Lucie Galand"}, {"name": " Olivier Spanjaard"}]}, "799": {"session": "13:30 - 15:15", "abstract": "The Description Logic (DL) EL is used to formulate several large biomedical ontologies. Fuzzy extensions of EL can express the vagueness inherent in many biomedical concepts. We study the reasoning problem of deciding positive subsumption in fuzzy EL with semantics based on general t-norms. We show that the complexity of this problem depends on the specific t-norm chosen. More precisely, if the t-norm has zero divisors, then the problem is NP-hard; otherwise, it can be decided in polynomial time.", "title": "Positive Subsumption in Fuzzy EL with General t-norms", "authors": [{"name": "Stefan Borgwardt"}, {"name": " Rafael Pe&ntilde;aloza"}]}, "1256": {"session": "15:45 - 17:30", "abstract": "Accurate estimates of daily crop evapotranspiration (ET) are needed for efficient irrigation management in regions where crop water demand exceeds rainfall. Daily grass or alfalfa reference ET values and crop coefficients are widely used to estimate crop water demand. Inaccurate reference ET estimates can hence have a tremendous impact on irrigation costs and the demands on freshwater resources. ET networks calculate reference ET using precise measurements of meteorological data. These networks are typically characterized by gaps in spatial coverage and lack of sufficient funding, creating an immediate need for alternative sources that can fill data gaps without high costs. Although non-agricultural weather stations provide publicly accessible meteorological data, there are concerns that the data may not be suitable for estimating reference ET due to factors such as weather station siting, data formats, and quality control issues. The objective of our research is to enable the use of alternative data sources by modeling the nonlinear relationships between non-ET weather station data and the reference ET computed by ET networks. This paper considers the representative example of the Texas High Plains region in the U.S., using sophisticated machine learning algorithms such as Gaussian process models and neural networks to formulate the problem of predicting reference ET based on data from non-ET weather stations.  Experimental results show significant improvement in prediction accuracy in comparison with the baseline regression models typically used for irrigation management applications.", "title": "Accurate Estimates of Reference Evapotranspiration for Irrigation Management in the Texas High Plain", "authors": [{"name": "Daniel Holman"}, {"name": " Mohan Sridharan"}, {"name": " Prasanna Gowda"}, {"name": " Dana Porter"}, {"name": " Thomas Marek"}, {"name": " Terry Howell"}, {"name": " Jerry Moorhead"}]}, "3": {"session": "15:45 - 17:30", "abstract": "", "title": "Finding Your Friends and Following Them to Where You Are", "authors": [{"name": "Adam Sadilek"}, {"name": " Henry Kautz and Jeffrey Bigham"}]}, "910": {"session": "13:30 - 15:15", "abstract": "In this paper we present a computationally efficient approach to enrich the Open Information Extraction paradigm with syntactic and semantic features. To obtain this goal, we combine deep syntactic analysis and distributional semantics using a kernel method and soft clustering. The output of our system is a set of automatically discovered and ontologized semantic relations.", "title": "Integrating Syntactic and Semantic Analysis into the Open Information Extraction Paradigm", "authors": [{"name": "Andrea Moro"}, {"name": " Roberto Navigli"}]}, "1338": {"session": "15:45 - 17:30", "abstract": "We study the problem of diverse promoting recommendation task: selecting a subset of diverse items that can better predict a given user\\\\\\'s preference. Recommendation techniques primarily based on user or item similarity can suffer from the risk that users cannot get expected information from the over-specified recommendation lists. In this paper, we propose an entropy regularizer to capture the notion of diversity. The entropy regularizer has good properties in that it satisfies monotonicity and submodularity, such that when we combine it with a modular rating set function, we get submodular objective function, which can be maximized approximately by efficient greedy algorithm, with provable constant factor guarantee of optimality. We apply our approach on the top-$N$ prediction problem and evaluate its performance on MovieLens data set, which is a standard database containing movie rating data collected from a popular online movie recommender system. We compare our model with the state-of-the-art recommendation algorithms. Our experiments show that (1) users do have wider interest that traditional recommendation techniques cannot cover, (2) the entropy regularizer effectively captures diversity and hence improves the performance of recommendation task.", "title": "Promoting Diversity in Recommendation by Entropy Regularizer", "authors": [{"name": "Lijing Qin"}, {"name": " Xiaoyan Zhu"}]}, "441": {"session": "15:45 - 17:30", "abstract": "Collaborative filtering, a widely-used user-centric recommendation technique, predicts an item's rating for an active user by aggregating its ratings from similar users. User similarity is usually calculated by cosine similarity or Pearson correlation coefficient. However, both of them consider only the direction of rating vectors, and suffer from a range of drawbacks. To solve these issues, we propose a novel Bayesian similarity measure based on the Dirichlet distribution, taking into consideration both direction and length of rating vectors. Our principled method, further, reduces correlation due to chance and thus reveals users' true similarity. Experimental results on six real-world data sets show that our method achieves superior accuracy.", "title": "A Novel Bayesian Similarity Measure for Recommender Systems", "authors": [{"name": "Guibing Guo"}, {"name": " Jie Zhang"}, {"name": " Neil Yorke-Smith"}]}, "1332": {"session": "15:45 - 17:30", "abstract": "A long-standing problem in Beijing's taxi market is that most taxi drivers intentionally avoid working during peak hours despite of the huge customer demand within these peak periods.  The existence of this dilemma is mainly triggered by the ignorance of taxi drivers' congestion costs (i.e., wasting time, burning fuel, and risk of rear-end collision in congestion) in the present taxi fare structure.  To resolve this problem, we propose a new pricing scheme to provide taxi drivers extra incentives to work over peak hours, which differs from the previous studies of taxi market by considering market variance over multiple time periods, taxi drivers' choosing strategies to maximize their individual revenues, and taxi drivers' scheduling constraints with regard to the interdependence among different periods.  The major challenge of this research is to determine the optimal pricing structure to improve the taxi system's efficiency against taxi drivers' selfish decisions.  Another difficulty is the computational intensiveness to identify the optimal strategy of taxi drivers due to the exponentially large size of the taxi drivers' strategy space and constraints.  To fulfill this end, we develop an atom schedule method to overcome issues aforementioned.  As a result, the proposed methodology reduces the problem magnitude while preserves constraints to filter out infeasible or unrealistic pure strategies.  Simulation results based on real data show the effectiveness of the proposed methods, which open a new door to improved efficiency of megacities' (e.g., Beijing) taxi market.", "title": "Optimal Pricing for Improving Efficiency of Taxi Systems", "authors": [{"name": "Jiarui Gan"}, {"name": " Bo An"}, {"name": " Haizhong Wang"}, {"name": " Xiaoming Sun"}, {"name": " Zhongzhi Shi"}]}, "247": {"session": "13:30 - 15:15", "abstract": "We present Breakout Local Search (BLS), the first heuristic approach for the vertex separator problem (VSP). BLS is a recent metaheuristic that follows the general framework of the popular Iterated Local Search (ILS) with a particular focus  on the perturbation strategy. Based on some relevant information on search history, it tries to introduce the most suitable degree of diversification by determining adaptively the number and type of moves for the next perturbation phase. The proposed heuristic is highly competitive with the exact state-of-art approaches from the literature on the current VSP benchmark. Moreover, we are the first ones to use for this problem a set of large publicly available graphs with up to 3000 vertices, which constitutes a new challenging benchmark for VSP approaches.", "title": "Breakout local search for the vertex separator problem", "authors": [{"name": "Benlic Una"}, {"name": " Jin-Kao Hao"}]}, "387": {"session": "13:30 - 15:15", "abstract": "The bin-packing problem is to partition a multiset of n numbers\n\n  into as few bins of capacity C as possible, such that the sum of\n\n  the numbers in each bin does not exceed C.  We compare two\n\n  existing algorithms for solving this problem: bin completion and\n\n  branch-and-cut-and-price. We show experimentally that the problem\n\n  difficulty and dominant algorithm are a function of n, the\n\n  magnitude of the input elements and the number of bins in an optimal\n\n  solution. We describe two improvements to bin completion which\n\n  result in up to six orders of magnitude speedup as compared to the\n\n  original algorithm. We also report up to six orders of magnitude\n\n  speedup compared to a state of the art branch-and-cut-and-price\n\n  algorithm written by Gleb Belov. We also show instances of bin\n\n  packing for which branch-and-cut-and-price outperforms bin\n\n  completion. We then explore a closely related problem, the\n\n  number-partitioning problem, and show that an algorithm based on\n\n  improved bin packing is up to three orders of magnitude faster than\n\n  the state of the art solver called DIMM. Finally, we describe how to\n\n  use number partitioning to generate difficult bin-packing instances.", "title": "Improved Bin Completion for Optimal Bin Packing and Number Partitioning", "authors": [{"name": "Ethan Schreiber"}, {"name": " Richard Korf"}]}, "388": {"session": "15:45 - 17:30", "abstract": "Bayesian Matrix factorization has proven to be useful in collaborative prediction,\n\nsince Bayesian approaches alleviate the overfitting problem by integrating out all model parameters.\n\nHowever, in collaborative prediction problems Bayesian approaches still suffer from\n\nthe cold-start problem in which the users or items do not have sufficient number of given ratings.\n\nTo resolve this limitation of Bayesian approaches to matrix factorization,\n\nwe propose hierarchical Bayesian matrix factorization methods,\n\nwhere the side information, such as content information or demographic user data,\n\nis associated to each latent factor through a normal-Wishart prior distribution\n\nwhose parameters are directly modeled by this side information with a regressor.\n\nWe have developed two matrix factorization methods according to how to learn\n\nthe regressors in the variational Bayesian framework.\n\nIn addition, we provide Bayesian Cram\\'{e}r-Rao Bounds for our matrix factorization models,\n\nshowing that the hierarchical Bayesian matrix factorization with side information\n\nimproves reconstruction over the conventional hierarchical Bayesian matrix factorization without side information.\n\nNumerical experimental results demonstrate that our proposed methods outperform\n\nexisting matrix factorization methods with side information\n\nfor the prediction of missing rating entries in the cold-start condition.", "title": "Hierarchical Bayesian Matrix Factorization with Side Information", "authors": [{"name": "Sunho  Park"}, {"name": " Yong-Deok Kim"}, {"name": " Seungjin Choi"}]}, "103": {"session": "15:45 - 17:30", "abstract": "One-class collaborative filtering or collaborative ranking with implicit feedback has been steadily receiving more attention, mostly due to the \"one-class\" characteristics of data in various services, e.g., \"like\" in Facebook and \"bought\" in Amazon. Previous works for solving this problem include pointwise regression methods based on absolute rating assumptions and pairwise ranking methods with relative score assumptions, where the latter was empirically found performing much better because it models users' ranking-related preferences more directly. However, the two fundamental assumptions made in the pairwise ranking methods, (1) individual pairwise preference over two items and (2) independence between two users, may not always hold. As a response, we propose a new and improved assumption, group Bayesian personalized ranking (GBPR), via introducing richer interactions among users. In particular, we introduce group preference, to relax the aforementioned individual and independence assumptions. We then design a novel algorithm correspondingly, which can recommend items more accurately as shown by various ranking-oriented evaluation metrics on four real-world datasets in our experiments.", "title": "Group Preference based Bayesian Personalized Ranking for One-Class Collaborative Filtering", "authors": [{"name": "Weike Pan"}, {"name": " Li Chen"}]}, "1938": {"session": "13:30 - 15:15", "abstract": "Relative direction information is very commonly used. Observers typically describe their environment by specifying the relative directions in which they see other objects or other people from their point of view. Or they receive navigation instructions with respect to their point of view, for example, turn left at the next intersection. \n\nHowever, it is surprisingly hard to integrate relative direction information obtained from different observers, and to reconstruct a model of the environment or the locations of the observers based on this information. Despite intensive research, there is currently no algorithm that can effectively integrate this information: this problem is known to be NP-hard, but not known to be in NP, even if the only information we use is left and right. \n\n\n\nIn this paper we present a novel qualitative representation, StarVars, that can solve these problems. It is an extension of the STAR calculus (Renz and Mitra, 2004) by a VARiable interpretation of the orientation of observers. We show that reasoning in StarVars is in NP and present the first algorithm that allows us to effectively integrate relative direction information from different observers. \n\n\n\n", "title": "StarVars -- Effective Reasoning about Relative Direction Information", "authors": [{"name": "Jae Hee Lee"}, {"name": " Jochen Renz"}, {"name": " Diedrich Wolter"}]}, "107": {"session": "15:45 - 17:30", "abstract": "In complex dynamic systems, accurate forecasting of extreme events, such as hurricanes, is a highly underdetermined, yet very important sustainability problem. While physics-based models deserve their own merits, they often provide unreliable predictions for variables highly related to extreme events. In this paper, we propose a new supervised machine learning problem, which we call a forecast oriented classification of spatio-temporal extreme events. We formulate three important real-world extreme event classification tasks, including seasonal forecasting of (a) tropical cyclones in Northern Hemisphere, (b) hurricanes and landfalling hurricanes in North Atlantic, and (c) North African rainfall. Corresponding predictor and predictand data sets are constructed. These data present unique characteristics and challenges that could potentially motivate future machine learning research.", "title": "Forecast Oriented Classification of Spatio-Temporal Extreme Events", "authors": [{"name": "Zhengzhang Chen"}, {"name": " Ankit Agrawal"}, {"name": " Alok Choudhary"}, {"name": " Wei-keng Liao"}]}, "972": {"session": "15:45 - 17:30", "abstract": "Constraint satisfaction problems may be nearly tractable. For instance, when the set of relations of a problem is a known tractable language with a few extra relations outside of this language. However, such an observation is in general of little use. \n\n\n\n  In this paper we introduce a method to take advantage of the fact that removing only a few constraints yields a tractable subproblem. This method can be viewed as computing a backdoor and can be applied to many tractable classes, providing that membership test for the class is itself tractable.\n\n  \n\n  We introduce two polynomial detection algorithms, to check if a language is closed under a majority or Mal'tsev polymorphism, respectively. Then we show that computing a minimal backdoor for such classes is fixed parameter tractable (FPT) if the tractable subset of relations is given, and W[2]-complete otherwise. Finally, we report preliminary empirical results showing that some (although, very few) problems in the XCSP repository are nearly closed under a majority polymorphism and small backdoors can be computed for them.", "title": "Detecting and Exploiting Subproblem Tractability", "authors": [{"name": "Christian Bessiere"}, {"name": " Clement Carbonnel"}, {"name": " Emmanuel Hebrard"}, {"name": " George Katsirelos"}, {"name": " Toby Walsh"}]}, "346": {"session": "15:45 - 17:30", "abstract": "High-throughput experimental techniques provide a wide variety of heterogeneous proteomic data. To utilize the information spread across multiple sources in a combined fashion, several methods follow a two-phased approach: they first optimize the weights on individual graph kernels (or networks) to  produce a composite kernel, and then train a classifier on the composite kernel. As such, these methods result in an optimal composite kernel, but not necessarily in an optimal classifier. On the other hand, some methods optimize the loss of binary classifiers, and learn weights for the different kernels iteratively. A protein has multiple functions, and each function can be viewed as a label. These methods solve the problem of optimizing weights on the input kernels for each of the labels. This is computationally expensive and ignores inter-label correlations.\n\n\n\nIn this paper, we propose a method called Protein Function Prediction using Network Integration (ProNet). ProNet iteratively optimizes the phases of learning optimal weights and reducing the empirical loss of a multi-label classifier for each of the labels simultaneously, using a combined objective function. ProNet can assign larger weights to smooth graph kernels and downgrade the weights on noisy kernels. We evaluate the ability of ProNet to predict the function of proteins using several standard benchmarks. We show that our approach performs better than previously proposed protein function prediction approaches that use multiple networks integration, and multi-label multiple kernel learning methods.\n\n\n\n", "title": "Protein Function Prediction using Multiple Kernels", "authors": [{"name": "Guoxian Yu"}, {"name": " Huzefa Rangwala"}, {"name": " Carlotta Domeniconi"}, {"name": " Guoji Zhang"}, {"name": " Zili Zhang"}]}, "843": {"session": "15:45 - 17:30", "abstract": "The New Forest cicada is the only species of cicadidae present in the UK and is in great danger of becoming extinct. Despite this, few entomologists are searching for it and, as its call is difficult for humans to hear, the chances of saving it are slim. We propose a smartphone-based citizen science approach is proposed to equip visitors to the New Forest with a smartphone app that can detect the presence of the cicada in real-time. However, current automated insect detection systems are aimed at batch classification and not suited for real-time detection in a noisy environment. To address this shortcoming, we propose an efficient approach based on a hand crafted hidden Markov model (HMM), to which we feed as a single feature vector a narrow band DFT computed with a Goertzel filter on the central frequency of the insect\u0089\u00db\u00aas song. We compare this approach to a complex state-of-the-art multi-species system for batch classification. The evaluation is performed on a large set of raw recordings taken with a smartphone, all unprocessed and subject to different kinds of noise. Our results show the robustness of the proposed approach for individual or few species, and the considerable gain in computational complexity that can be achieved.", "title": "Outstanding student paper: A Hidden Markov Model-Based Cicada Detector for Crowdsourced Smartphone Biodiversity Monitoring", "authors": [{"name": "Davide Zilli"}, {"name": " Oliver Parson"}, {"name": " Geoff V Merrett"}, {"name": " Alex Rogers"}]}, "1643": {"session": "13:30 - 15:15", "abstract": "Single-peakedness is the most common domain restriction used in social choice theory, and allows the use of mechanisms with desirable properties. However, it is not clear to what extent agent preferences are single-peaked in practice, and whether recent proposals for approximate single-peakedness (in one-dimensional settings) help in this regard.  We address this issue by assessing the ability of both single-dimensional and multi-dimensional approximations to explain preference profiles drawn from several real-world elections.  We develop a simple \\emph{branch-and-bound} algorithm for finding multi-dimensional single-peaked axes that best fit a given profile, and that works with several forms of approximation. Empirical results show that agent preferences in these elections are far from single-peakedness in any one-dimensional space, but are approximately single-peaked in two-dimensions.  Our algorithms are quite efficient as well, despite the NP-completeness of finding one-dimensional single-peaked approximations, and show excellent anytime performance.", "title": "Multi-dimensional Single-peaked Consistency and its Approximations", "authors": [{"name": "Xin Sui"}, {"name": " Alex Francois-Nienaber"}, {"name": " Craig Boutilier"}]}, "32": {"session": "15:45 - 17:30", "abstract": "", "title": "Bayesian Probabilities for Constraint-based Causal Discovery", "authors": [{"name": "Tom Claassen and Tom Heskes"}]}, "36": {"session": "15:45 - 17:30", "abstract": "", "title": "Three Semantics for the Core of the Distributed Ontology Language", "authors": [{"name": "Till Mossakowski"}, {"name": " Christoph Lange and Oliver Kutz"}]}, "1241": {"session": "13:30 - 15:15", "abstract": "Keywords extraction attracts much attention for its significant role in various natural language processing tasks. While some existing methods for keywords extraction have considered semantic relatedness between words and inherent features of words separately, almost all of them ignore two important issues:\n\n1) how to fuse multiple semantic relations between words into a uniform\n\nsemantic measurement, and 2) how to integrate multiple semantic relations\n\nbetween words and inherent various features of words into a unified model. In this work, we extend the supervised random walk method to tackle the above two issues, which combines the merits of semantic relatedness information and words' specific information simultaneously. We conducted extensive experimental study on established benchmarks and the experimental results demonstrate that our hybrid method outperforms both the state of the art supervised and unsupervised methods.", "title": "Integrating Semantic Relatedness and Words\u2019 Intrinsic Features for Keyword Extraction", "authors": [{"name": "Wei Zhang"}, {"name": " Wei Feng"}, {"name": " Jianyong Wang"}]}, "405": {"session": "15:45 - 17:30", "abstract": "AI planners have to compromise between the speed of the planning process\n\nand the quality of the generated plan. Planners based on greedy heuristic\n\nsearch are often able to find plans quickly, but the quality of their\n\nsolutions is often poor. Planners that guarantee solution optimality,\n\nor bounded sub-optimality, on the other hand, do not scale up to large\n\nproblems. Anytime planners try to balance these objectives by producing\n\nplans of better quality over time, but current anytime planners often\n\nare not effective at making use of increasing runtime beyond the first\n\nfew minutes.\n\nWe present a new method of continuing plan improvement, by decomposing\n\na given plan into subplan and iteratively optimising each subplan locally.\n\nThe decomposition exploits block-structured plan deordering to identify\n\ncoherent subplans, which make sense to treat as units.\n\nCoupled with a fast, non-optimal, planner to generate an initial, possibly\n\nlow-quality, solution, this gives a new approach to anytime planning.\n\nWe show that this approach is able to make better use of available time\n\nthan previous anytime planners, often continuing to improve plan quality\n\nbeyond the best that other planners find.\n\n", "title": "Plan Quality Optimisation via Block Decomposition", "authors": [{"name": "Fazlul Siddiqui"}, {"name": " Patrik Haslum"}]}, "439": {"session": "15:45 - 17:30", "abstract": "This paper concerns building probabilistic models with an underlying ontology that defines the classes and properties used in the model. In particular, it considers the problem of reasoning with properties with non-trivial domains. The properties may not always be defined, and we may be uncertain about whether an individual is in the domain of a property. One approach is to explicitly add a value \"undefined\" to the range of random variables, forming extended belief networks; however, adding an extra value to a variable's range has a large computational overhead. In this paper, we propose an alternative, ontologically-grounded belief networks, where all properties are only used when they are defined, and we show how probabilistic reasoning can be carried out without explicitly using the value \"undefined\" during inference. We prove this is equivalent to  reasoning with the corresponding extended belief network and empirically demonstrate that inference becomes more efficient.\n\n", "title": "Probabilistic Reasoning with Undefined Properties in Ontologically-Grounded Belief Networks", "authors": [{"name": "Chia-Li Kuo"}, {"name": " David Buchman"}, {"name": " Arzoo Katiyar"}, {"name": " David Poole"}]}, "437": {"session": "15:45 - 17:30", "abstract": "While  being crucial  for human-robot interaction and other natural and dynamic contexts, directing a robot's attention to recognise activities and to anticipate events  like  goal-directed actions is complicated by intrinsic time constraints and spatially distributed sources of information.\n\nThe problem,  demanding for a  broader informational context  than the current perception and task,  clashes with the limits of the current attention control systems. In fact, it   requires an integrated solution for tracking, exploration and recognition, which  traditionally have been seen as separate problems in active-vision.\n\nWe propose a probabilistic generative framework  based on a mixture of Kalman filters and  information gain maximisation to use predictions in both recognition and attention-control. \n\nThis framework can efficiently use the observations of one element in a dynamic environment to provide information on other elements, and consequently enables guided exploration.\n\nInterestingly, the  sensors control policy,   directly derived from first principles,   represents the intuitive trade-off between finding the most discriminative  clues  and  maintaining overall awareness.\n\nExperiments on a simulated humanoid robot  observing a human executing goal-oriented actions demonstrated  improvement on recognition time and precision over baseline systems.", "title": "Towards active event recognition", "authors": [{"name": "Dimitri Ognibene"}, {"name": " Yiannis Demiris"}]}, "435": {"session": "15:45 - 17:30", "abstract": "Non-negative Matrix Factorization (NMF) is a traditional unsupervised machine learning technique for decomposing a matrix into a set of bases and coefficients under the non-negative constraint. NMF with sparse constraints is also known for extracting reasonable components from noisy data. However, NMF tends to give  undesired results in the case of highly sparse data, because the information included in the data is insufficient to decompose. Our key idea is that we can ease this problem if complementary data are available that we could integrate into the estimation of the bases and coefficients. In this paper, we propose a novel matrix factorization method called Non-negative Multiple Matrix Factorization (NM2F), which utilizes complementary data as additional matrices that share the row or column indices of the original matrix. The data sparseness is improved by decomposing the original and additional matrices simultaneously, since additional matrices provide information about the bases and coefficients. We formulate NM2F as a generalization of NMF, and then present a parameter estimation procedure derived from the multiplicative update rule. We examined NM2F in both synthetic and real data experiments. The effect of the additional matrices appeared in the improved NM2F performance. We also confirmed that the bases that NM2F obtained from the real data were intuitive and reasonable thanks to the non-negative constraint.", "title": "Non-negative Multiple Matrix Factorization", "authors": [{"name": "Koh Takeuchi"}, {"name": " Katsuhiko Ishiguro"}, {"name": " Akisato Kimura"}, {"name": " Hiroshi Sawada"}]}, "430": {"session": "13:30 - 15:15", "abstract": "We provide both a semantic interpretation and logical (inferential)  characterization of the Markov principle that underlies the main action theories in AI. This principle will be shown to  constitute a nonmonotonic assumption that justifies the actual restrictions on action descriptions in these theories, as well as constraints on allowable queries. It will be shown also that the well-known regression principle is a consequence of the Markov assumption, and it is valid also for non-deterministic domains.", "title": "The Markov Assumption: Formalization and Impact", "authors": [{"name": "Alexander Bochman"}]}, "1348": {"session": "13:30 - 15:15", "abstract": "Dynamic epistemic logic (DEL) provides a very expressive framework for multi-agent planning that can deal with non-determinism, partial observability, sensing actions, and arbitrary nesting of beliefs about other agents. However, as we show in this paper, this expressiveness comes at a price. The planning framework is undecidable, even if we allow only purely epistemic actions (actions that change only beliefs, not ontic facts). Undecidability holds already in the S5 setting with 2 agents, and even with 1 agent when allowing arbitrary frames. It shows that multi-agent planning is robustly undecidable if we assume that agents can reason with an arbitrary nesting of beliefs about beliefs. We also prove a corollary showing undecidability of model checking of DEL with the star operator on actions (iteration). \n\n", "title": "Undecidability of epistemic multi-agent planning", "authors": [{"name": "Guillaume Aucher"}, {"name": " Thomas Bolander"}]}, "579": {"session": "13:30 - 15:15", "abstract": "Multiple task learning (MTL) is to improve\n\ngeneralization performance by exploiting\n\nthe intrinsic relationship among\n\nrelated tasks. One common assumption\n\nin most MTL algorithms is that all the\n\ntasks are related definitely. However, it\n\nis usually not such in real applications.\n\nIn this paper, we propose a novel robust\n\nmultiple task learning algorithm to\n\n. We also theoretically analysis the convergence\n\nand effectiveness of the proposed\n\nmethod. Experimental results on\n\ntoy data as well as two benchmark data\n\nset demonstrate the effectiveness of the\n\nproposed method.", "title": "Robust Multiple Task Regression via Reweighted Least Square", "authors": [{"name": "Jian Pu"}, {"name": " Yugang Jiang"}, {"name": " Jun Wang"}, {"name": " Xiangyang Xue"}]}, "452": {"session": "15:45 - 17:30", "abstract": "Hashing function learning has been recently received more and more attentions for fast search for large scale data. However, existing popular learning based hashing methods are batch model learning models and thus incur large scale computational problem for learning an optimized model on a large scale of labelled data. In this paper, we address the problem by develop an online hashing learning algorithm based on passive-aggressive strategy. To accommodate new coming data, the proposed online learning hashing update gets hashing model adapted to those new data and at the same time the newly updated model is penalized by the most previously learned model in order to retain important information learned in previous rounds. We also derive a tight bound for loss of our proposed online learning algorithm. We demonstrate the proposed online hashing model on searching both metric distance neighbors and semantical similar neighbors in the experiments.", "title": "Online Hashing", "authors": [{"name": "Longkai Huang"}, {"name": " Qiang Yang"}, {"name": " Wei-Shi  Zheng"}]}, "242": {"session": "15:45 - 17:30", "abstract": "Iterated games have been widely studied in the game theory literature. In this paper, we study iterated Boolean games. These are games in which players repeatedly choose values for the propositional variables over which they are assigned control. Our  model of iterated Boolean games assumes that players have goals represented as formulae of Linear Temporal Logic (LTL), a widely used formalism for expressing properties of state sequences.  To  model the strategies that players use in such games, we use a finite state machine model. After introducing and  formally defining iterated Boolean games, we investigate the  computational complexity of their associated game theoretic decision  problems as well as semantic conditions characterising the kinds of   LTL properties that are preserved by equilibrium points (pure Nash equilibria) whenever they exist. ", "title": "Iterated Boolean Games", "authors": [{"name": "Julian Gutierrez"}, {"name": " Paul Harrenstein"}, {"name": " Michael Wooldridge"}]}, "454": {"session": "13:30 - 15:15", "abstract": "We study trade networks with a tree structure, where a seller with a single indivisible good is connected to buyers, each with some value for the good, via a unique path of intermediaries. Agents in the tree make multiplicative revenue share offers to their parent nodes, who choose the best offer and offer part of it to their parent, and so on; the winning path is determined by who finally makes the highest offer to the seller. In this paper, we investigate how these revenue shares might be set via a natural bargaining process between agents on the tree.\n\n\n\nWe define a bargaining game where the agents at the endpoints of each edge in the tree negotiate the share on that edge using egalitarian bargaining, taking shares elsewhere in the tree as given. We investigate the fixed point of this system of bargaining equations and show that: (i) a fixed point always exists, and is  unique, (ii) the winner is always a buyer with highest value (efficiency), (iii) the payoff vector specified by the fixed point belongs to the core of the associated cooperative game, (iv) if the bargaining power of an agent on the winning path increases, then her final payoff strictly increases as well (strict monotonicity), and (v) the fixed point can be efficiently computed in polynomial time.  Finally, we present numerical evidence that asynchronous dynamics with randomly ordered updates always converges to the fixed point, indicating that the fixed point shares might arise from decentralized bargaining amongst agents on the trade network.", "title": "Bargaining for Revenue Shares on Tree Trading Networks", "authors": [{"name": "Arpita Ghosh"}, {"name": " Satyen Kale"}, {"name": " Kevin Lang"}, {"name": " Benjamin Moseley"}]}, "60": {"session": "13:30 - 15:15", "abstract": "This paper presents a novel semantic regularized matrix factorization method for learning descriptive visual bag-of-words (BOW) representation. Although very influential in image classification, the traditional visual BOW representation has one distinct drawback. That is, for efficiency purposes, the visual vocabulary is commonly constructed for visual BOW generation by directly clustering the low-level visual feature vectors extracted from local keypoints or regions, without considering the high-level semantics of images. In other words, the traditional visual BOW representation still suffers from the semantic gap and may lead to significant performance degradation in more challenging tasks (e.g., classification of community-contributed images with large intra-class variations). To overcome this drawback, we develop a semantic regularized matrix factorization method for learning descriptive visual BOW representation by adding Laplacian regularization defined with the tags (easy to access although noisy) of community-contributed images into matrix factorization. Experimental results on two benchmark datasets show the promising performance of the proposed method.", "title": "Learning Descriptive Visual Representation by Semantic Regularized Matrix Factorization", "authors": [{"name": "Zhiwu Lu"}, {"name": " Yuxin Peng"}]}, "61": {"session": "13:30 - 15:15", "abstract": "This work is motivated by the following concern. Suppose we have a\n\ngame exhibiting multiple Nash equilibria, with little to distinguish\n\nthem except that one of these equilibria can be verified\n\nwhile the others cannot. That is, one of these equilibria carries\n\nsufficient information that, if this is the outcome, then\n\nthe players can tell that an equilibrium has been played. This\n\nprovides an argument for this equilibrium being played, instead of\n\nthe alternatives. Verifiability can thus serve to make an\n\nequilibrium a focal point in the game. We formalise and investigate\n\nthis concept using a model of Boolean games with incomplete\n\ninformation. We define and investigate three increasingly strong\n\ntypes of verifiable equilibria, characterise the \n\ncomplexity of checking these, and show how\n\nchecking for the existence of verifiable equilibria can be captured\n\nin a variant of modal epistemic logic.", "title": "Verifiable Equilibria in Boolean Games", "authors": [{"name": "Thomas &Aring;gotnes"}, {"name": " Paul Harrenstein"}, {"name": " Wiebe van der Hoek"}, {"name": " Michael Wooldridge"}]}, "1633": {"session": "15:45 - 17:30", "abstract": "We consider the mechanism design problem for agents with single-peaked preferences over multi-dimensional domains when multiple alternatives can be chosen. Facility location and committee selection are classic embodiments of this problem. We propose a class of \\emph{percentile mechanisms}, a form of generalized median mechanisms, that are strategy-proof, and derive worst-case approximation ratios for social cost and maximum load for $L_1$ and $L_2$ cost models. More importantly, we propose a sample-based framework for optimizing the choice of percentiles relative to any prior distribution over preferences, while maintaining strategy-proofness. Our empirical investigations, using social cost and maximum load as objectives, demonstrate the viability of this approach and the value of such optimized mechanisms \\emph{vis-\\`{a}-vis} mechanisms derived through worst-case analysis.", "title": "Analysis and Optimization of Multi-dimensional Percentile Mechanisms", "authors": [{"name": "Xin Sui"}, {"name": " Craig Boutilier"}, {"name": " Tuomas Sandholm"}]}, "850": {"session": "13:30 - 15:15", "abstract": "When using ontologies to access instance data, it can be useful to make a closed world assumption for some predicates and an open world assumption for others. A main problem with such a setup is that conjunctive query (CQ) answering becomes intractable regarding data complexity already for inexpressive description logics such as DL-Lite and EL. We take a non-uniform perspective to analyze this situation in more detail, that is, we consider the data complexity of conjunctive query (CQ) answering on the level of individual ontologies.  It turns out that, whenever CQ answering with closed and open predicates w.r.t. a DL-Lite or EL ontology is tractable, then it coincides with CQ answering where all predicates are open.  In this sense, CQ answering with closed predicates is inherently intractable in these logics. Our analysis also yields a dichotomy between AC0 and coNP for CQ answering w.r.t. ontologies formulated in DL-Lite and a dichotomy between PTime and coNP for EL. Interestingly, the situation is less dramatic for ontologies formulated in the more expressive description logic ELI, where we find ontologies for which CQ answering is in PTime, but does not coincide with CQ answering where all predicates are open.", "title": "Ontology-Based Data Access with Closed Predicates is Inherently Intractable (Sometimes)", "authors": [{"name": "Carsten Lutz"}, {"name": " Inanc Seylan"}, {"name": " Frank Wolter"}]}, "472": {"session": "15:45 - 17:30", "abstract": "Constraint propagation is one of the key techniques in constraint programming, and a large body of work has built up around it.  Special-purpose constraint propagation algorithms frequently make implicit use of short supports --- by examining a subset of the variables, they can infer support (a justification that a variable-value pair may still form part of a solution to the constraint) for all other variables and values and save substantial work. Recently short supports have been used in general purpose propagators, and (when the constraint is amenable to short supports) speed ups of more than three orders of magnitude have been demonstrated. \n\n\n\nIn this paper we present ShortSTR2, a development of the Simple Tabular Reduction algorithm STR2. We show that ShortSTR2 is complementary to the existing algorithms ShortGAC and HaggisGAC that exploit short supports, while being much simpler. When a constraint is amenable to short supports, the short support set can be exponentially smaller than the full-length support set. Therefore ShortSTR2 can efficiently propagate many constraints that STR2 cannot even load into memory. \n\n\n\nWe also show that ShortSTR2 can be combined with a simple algorithm to identify short supports from full-length supports, to provide a superior drop-in replacement for STR2. \n\n", "title": "Extending Simple Tabular Reduction with Short Supports", "authors": [{"name": "Christopher Jefferson"}, {"name": " Peter Nightingale"}]}, "1581": {"session": "13:30 - 15:15", "abstract": "One drawback of Hierarchical Task Network (HTN) planning is the difficulty of providing complete domain knowledge, i.e., a complete and correct set of HTN methods for every task. To provide a principled way to overcome this difficulty, we define a formalism for Partial-Knowledge Goal Network (PGN) planning, and a planning algorithm based on this formalism.\n\n\n\nLike HTN planning, PGN planning includes hierarchical decomposition using methods. But PGN methods specify ways to achieve goals rather than tasks, and goals may be achieved even if the domain knowledge is incomplete or nonexistent (i.e., if there isn't a complete set of methods for every goal).\n\n\n\nOur planning algorithm, GoDeL (Goal Decomposition using Landmarks), has several desirable theoretical guarantees, including soundness and completeness irrespective of whether the domain knowledge (i.e., the set of methods given to the planner) is complete. By comparing GoDeL's performance with varying amounts of domain knowledge across three benchmark planning domains, we show experimentally that (1) GoDeL works correctly with partial planning knowledge, (2) GoDeL's performance improves as more planning knowledge is given, and (3) when given full domain knowledge, GoDel matches the performance of state-of-the-art hierarchical planners.", "title": "The GoDeL Planning System: A more perfect union of Domain-Independent and Hierarchical Planning", "authors": [{"name": "Vikas Shivashankar"}, {"name": " Ron Alford"}, {"name": " Ugur Kuter"}, {"name": " Dana Nau"}]}, "1734": {"session": "13:30 - 15:15", "abstract": "We present an online expectation-maximization (EM) algorithm for learning the regionalized pol- icy representation (RPR), a parametric policy for model-free reinforcement learning in POMDPs. In the E-step, the algorithm performs a partial evaluation of the policy based on the most recent episodes, updating a noisy objective function; in the M-step, the algorithm improves the policy by maximizing the updated objective function. A thorough analysis is given for the convergence of the algorithm. The algorithm is demonstrated on several bench- mark POMDP problems.", "title": "Online Expectation Maximization for Reinforcement Learning in POMDPs", "authors": [{"name": "Miao Liu"}, {"name": " Xuejun Liao"}, {"name": " Lawrence  Carin"}]}, "731": {"session": "15:45 - 17:30", "abstract": "The knowledge compilation map introduced by Darwiche and Marquis takes advantage of a number of concepts (mainly queries and transformations, expressiveness, and succinctness) to compare the relative adequacy of representation languages to some AI problems. However, the corresponding framework is limited to the comparison of languages that are interpreted in a homogeneous way (formulae are interpreted as Boolean functions.) This prevents one from comparing on a formal basis, for the representation purpose, languages which are close in essence, such as OBDD, MDD, and ADD.\n\nTo fill the gap, we present a generalized framework into which comparing formally heterogeneous representation languages becomes feasible. In particular, we explain how the key notions of queries and transformations, \n\nexpressiveness, and succinctness can be lifted to the generalized setting.", "title": "Towards a Knowledge Compilation Map for Heterogeneous Representation Languages", "authors": [{"name": "H&eacute;l&egrave;ne Fargier"}, {"name": " Pierre Marquis"}, {"name": " Alexandre Niveau"}]}, "2": {"session": "15:45 - 17:30", "abstract": "", "title": "Case Adaptation with Qualitative Algebras", "authors": [{"name": "Valmi Dufour-Lussier"}, {"name": " Florence Le Ber"}, {"name": " Jean Lieber and Laura Martin"}]}, "508": {"session": "15:45 - 17:30", "abstract": "The road network design problem is to optimize the road network by selecting paths to improve or adding paths in the existing road network, under certain constraints, e.g., the weighted sum of modifying costs. Since its multi-objective nature, the road network design problem is often challenging for designers. Empirically, the smaller diameter a road network has, the more connected and efficient the road network is. Based on this observation, we propose a set of constrained convex models for designing road networks with small diameters. To be specific, we theoretically prove that the diameter of the road network, which is evaluated w.r.t the travel times in the network, can be bounded by the algebraic connectivity in spectral graph theory since that the upper and lower bounds of diameter are inversely proportional to algebraic connectivity. Then we can focus on increasing the algebraic connectivity instead of reducing the network diameter, under the budget constraints. The above formulation leads to a semi-definite program, in which we can get its global solution easily. Then, we present some simulation experiments to show the correctness of our method. At last, we compare our method with an existing method based on the genetic algorithm.", "title": "A global constrained optimization method for designing road networks with small diameters", "authors": [{"name": "Teng Ma"}, {"name": " Yuexian Hou"}, {"name": " Xiaozhao Zhao"}]}, "651": {"session": "13:30 - 15:15", "abstract": "Weighted voting games (WVGs) model decision making bodies such as parliaments and councils. One is often interested in measuring the influence\n\na player has on the vote. Two highly popular measures of influence are the Shapley value [Shapley, 1953], and the Banzhaf power index [Banzhaf,\n\n1964]. Given a power measure, proportional representation is the property of having players\u0089\u00db\u00aa voting power proportional to their weight. Approximate\n\nproportional representation (w.r.t. the Banzhaf power index) can be ensured by changing player weights [Penrose, 1946]; however, a simpler way\n\nof achieving approximate proportional representation is by changing the quota, i.e. the number of votes required in order to pass a bill [S&amp;#322;omczynski and\n\n&amp;#729; Zyczkowski, 2006].\n\nIt is known that when one chooses a quota at random, proportional representation w.r.t. the Shapley value holds in expectation [Mann and Shapley,\n\n1960]. In our work, we show bounds on the variance of the Shapley value when the quota is chosen at random; our bounds imply conditions that ensure\n\nthat the variance is low under certain assumptions.\n\n\n\nFinally, we provide asymptotic and empirical analysis of the case when weights are sampled i.i.d. from a binomial distribution.", "title": "On the Variance of the Shapley value in Weighted Voting Games", "authors": [{"name": "Yair Zick"}]}, "1216": {"session": "15:45 - 17:30", "abstract": "Localization of a mobile robot is crucial for autonomous navigation. Using laser scanners, this can be facilitated by the pairwise alignment of consecutive scans. In this paper, we are interested in improving this scan alignment in challenging natural environments, using solely the scans as sensor data. For this purpose, local descriptors are generally effective as they facilitate point matching. However, we show that in some natural environments, many of them are likely to be unreliable, which affects the accuracy and robustness of the results. Therefore, we propose to filter the unreliable descriptors as a prior step to alignment. Our approach uses a fast machine learning algorithm, trained on-the-fly under the Positive and Unlabeled learning paradigm without the need for human intervention. Our results show that the number of descriptors can be significantly reduced, while increasing the proportion of reliable ones, thus speeding-up and increasing the robustness of the scan alignment process.", "title": "Accelerated Robust Point Cloud Registration in Natural Environments through Positive and Unlabeled Learning", "authors": [{"name": "Maxime Latulippe"}, {"name": " Alexandre Drouin"}, {"name": " Philippe Gigu&egrave;re"}, {"name": " Fran&ccedil;ois Laviolette"}]}, "754": {"session": "15:45 - 17:30", "abstract": "A robot continually learns from interacting with humans in multiclass prediction tasks. The human feedbacks over the robot predictions naturally vary from giving the true labels, to just partially as correct/incorrect replies. Our algorithms allow the robot to efficiently explore informative predictions, while trying to predict as correctly as it has learned. Good classification accuracies are obtained when the interactive learning proceeds, outperforming recent algorithms running in the same partial feedback setting, and even beating existing algorithms running in full feedback. Furthermore, our algorithms allow an informed way for knowledge transfer between robots learning in parallel on the same task, thus reducing duplicate learning efforts considerably. While our experiments focus on several benchmark datatsets, our methods apply to any multi-class visual learning and recognition task in robotics.", "title": "Upper Confidence Weighted Learning for Efficient Exploration in Multiclass Prediction with Binary Feedback", "authors": [{"name": "Hung Ngo"}, {"name": " Matthew Luciw"}, {"name": " Ngo Anh Vien"}, {"name": " Juergen Schmidhuber"}]}, "1353": {"session": "13:30 - 15:15", "abstract": "We consider a simple sequential allocation  procedure for sharing indivisible goods between agents in which agents take turns to pick item.  Supposing additive utilities and independence between the agents, we  show that the expected social welfare can be computed in polynomial time. Using this result, we prove that the expected social welfare is maximized when agents take turns in a fixed order. We also argue that this mechanism remains optimal when \n\nagents behave strategically. ", "title": "A Social Welfare Optimal Sequential Allocation Procedure", "authors": [{"name": "Thomas  Kalinowski"}, {"name": " Nina Narodytska"}, {"name": " Toby Walsh"}]}, "1351": {"session": "15:45 - 17:30", "abstract": "We study the complexity of electing a committee under variants of the Chamberlin-Courant rule when the voters' preferences are single-peaked on a tree. We first show that this problem is easy for the Rawlsian, or \"minimax\" version of this problem, for arbitrary trees and misrepresentation functions. For the standard (utilitarian) version of this problem we provide an algorithm for an arbitrary misrepresentation function whose running time is polynomial in the input size as long as the number of leaves of the underlying tree is bounded by a constant. We argue that the constraint on the number of leaves cannot be removed, by proving that our problem remains computationally hard on trees that have bounded degree, diameter or pathwidth. Finally, we show how to modify Trick's (1989) algorithm to check whether an election is single-peaked on a tree whose number of leaves does not exceed a given parameter C. ", "title": "Proportional Representation Under Preferences That Are Single-peaked on a Tree ", "authors": [{"name": "Lan Yu"}, {"name": " Hau Chan"}, {"name": " Edith Elkind"}]}, "6": {"session": "15:45 - 17:30", "abstract": "", "title": "Decision Generalisation from Game Logs in No Limit Texas Hold'em", "authors": [{"name": "Jonathan Rubin and Ian Watson"}]}, "461": {"session": "15:45 - 17:30", "abstract": "The behavior composition problem involves the automatic synthesis of a controller that is able to \"realize\" (i.e., implement) a desired target behavior specification by suitably coordinating a set of already available behaviors. \n\n\n\nWhile the problem has been thoroughly studied, one open issue has resisted a principled solution for a long time:  if the  target specification is not fully realizable, is there a way to realize it \"at best\"?\n\n\n\nIn this paper we answer positively, by showing that there exists an unique supremal realizable target behavior satisfying the specification. More importantly we give an effective procedure to actually\n\ncompute such a target.\n\n\n\nThen, we introduce exogenous events, and show that the supremal can again be computed, though this time, it comes into two variants, depending on the ability to observe such events. ", "title": "On the Supremal Realizability of Behaviors with Uncontrollable Exogenous Events", "authors": [{"name": "Nitin Yadav"}, {"name": " Giuseppe De Giacomo"}, {"name": " Paolo Felli"}, {"name": " Sebastian Sardina"}]}, "566": {"session": "13:30 - 15:15", "abstract": "Recent years have witnessed the growing popularity of hashing in large-scale vision problems. Although most existing hashing-based methods have been proven to obtain high accuracy, they are regarded as passive hashing and assume that the labelled pairs are provided in advance. In this paper, we consider updating a hashing model upon gradually increased labeled data in a fast response to users, called smart hashing update (SHU), which selects a small set of hashing functions to relearn and only updates the corresponding hash bits of all data points. More specifically, we put forward two selection methods for performing efficient and effective update. In addition, in order to accelerate the speed of convergence and reduce the response time for a stable hashing algorithm, we also propose an accelerated method in order to further reduce interactions between users and computer. We evaluate our proposals on two benchmark data sets. Our experimental results show it is not necessary to update all hash bits in order to adapt the model for new input data and meanwhile we obtain better or similar performance without sacrificing much accuracy against the batch mode update.", "title": "Smart Hashing Update for Fast Response", "authors": [{"name": "Qiang Yang"}, {"name": " Longkai Huang"}, {"name": " Wei-Shi  Zheng"}, {"name": " Ling Yingbiao"}]}, "99": {"session": "13:30 - 15:15", "abstract": "Eliciting the preferences of users in a reliable, efficient, and minimally intrusive manner is a major step towards developing automated assistants, recommender systems, and decision support tools. Assuming that preferences are \\emph{ceteris paribus} --- that, all other things being equal, a variable's preferred value depends only on the values of a subset of the other variables --- allows for their concise representation as Conditional Preference Networks (CP-nets), which one can reason with, and learn. This work offers an empirical investigation of an algorithm for \\emph{reliably} and \\emph{efficiently} learning CP-nets. At the same time, it introduces a novel process for \\emph{efficiently reasoning} with (the learned) preferences.\n\n", "title": "An Empirical Investigation of Ceteris Paribus Learnability", "authors": [{"name": "Loizos Michael"}, {"name": " Elena Papageorgiou"}]}, "169": {"session": "13:30 - 15:15", "abstract": "In this paper we look into the assumption of interpreting LTL over finite traces. In particular we show that LTLf, i.e., LTL under this assumption, is less expressive than what might appear at first sight, and that at  essentially no computational cost one can make a significant increase in expressiveness and maintain the same intuitiveness of \\LTLf interpreted over finite traces. Indeed, we propose a  logic, LDLf, for Lineal Dynamic Logic over finite traces,  which borrows the syntax from Propositional Dynamic Logic (PDL), but is interpreted over finite traces. Satisfiability, validity and logical implication (as well as model checking) for LDLf are PSPACE-complete as for LTLf (and LTL). ", "title": "Linear Temporal Logic and Linear Dynamic Logic on Finite Traces", "authors": [{"name": "Giuseppe De Giacomo"}, {"name": " Moshe Vardi"}]}, "227": {"session": "15:45 - 17:30", "abstract": "Matrix factorization (MF) is a popular collaborative filtering approach for recommender systems due to its simplicity and effectiveness.  Existing MF methods either assume that all latent features are uncorrelated or assume that all are correlated.  To address the issue of what structure should be imposed on the features, we investigate the covariance matrix of the latent features learned from real data.  Based on the finding, we propose an MF model with a sparse covariance prior that favors a sparse yet non-diagonal covariance matrix.  Not only can this reflect the semantics more properly, but imposing sparsity can also have a side effect of preventing overfitting.  Starting from a probabilistic generative model with a sparse covariance prior, we formulate the model inference problem as a maximum a posteriori (MAP) estimation problem.  The optimization procedure makes use of stochastic gradient descent and majorization-minimization.  For empirical validation, we conduct experiments using the MovieLens and Netflix datasets to compare the proposed method with two strong baselines which use different priors.  The results show that our sparse covariance prior can lead to performance improvement.", "title": "Probabilistic Matrix Factorization with Sparse Covariance Prior for Collaborative Filtering", "authors": [{"name": "Jianping Shi"}, {"name": " Naiyan Wang"}, {"name": " Yang Xia"}, {"name": " Dit-Yan Yeung"}, {"name": " Irwin King"}, {"name": " Jiaya Jia"}]}, "1910": {"session": "15:45 - 17:30", "abstract": "Object segmentation and description is key to\n\nmany perceptual and manipulation tasks. In this paper, we propose\n\na novel method for object discovery and dense modelling\n\nin RGB-D image sequences using motion cues. Our approach\n\nsimultaneously segments motion within key views, and discovers\n\nobjects and hierarchical relations between object parts. The\n\nposes of the key views are optimized in a graph of spatial\n\nrelations to recover the rigid body motion trajectories of the\n\ncamera with respect to the object segments. In experiments, we\n\ndemonstrate that our approach finds moving segments, aligns\n\npartial views on the objects, and retrieves hierarchical relations\n\nbetween the objects.", "title": "Hierarchical Object Discovery and Dense Modelling From Motion Cues in RGB-D Video", "authors": [{"name": "J&ouml;rg St&uuml;ckler"}, {"name": " Sven Behnke"}]}, "167": {"session": "15:45 - 17:30", "abstract": "Recent advances in Bayesian reinforcement learning (BRL) have shown that Bayes-optimality is theoretically achievable by assuming the environment's latent dynamics to be represented using Flat-Dirichlet-Model (FDM). However, the independence assumption made in FDM appears ill-suited in self-interested multi-agent systems whose transition dynamics are caused mainly by the other agent's unknown (stochastic) behaviors (Section 1). This consequently creates a gap in applying BRL into self-interested multi-agent settings. To bridge this gap, we present a generalization of BRL to integrate the general class of finitely parametric models and model priors, thus allowing the practitioners greater flexibility in encoding their prior domain knowledge regarding the other agent's behavior. Empirical evaluations of its application to the transportation domain show that our approach outperforms existing work in multi-agent contexts such as BPVI [Chalkiadakis and Boutilier, 2003].", "title": "A General Framework for Interacting Bayes-Optimally with Self-Interested Agents using Any Models and", "authors": [{"name": "Trong Nghia Hoang"}, {"name": " Bryan Kian Hsiang Low"}]}, "1621": {"session": "13:30 - 15:15", "abstract": "A robust system for ontology-based data access should provide meaningful answers to queries even when the data conflicts with the ontology. This can be accomplished by adopting an inconsistency-tolerant semantics, with the consistent query answering (CQA) semantics being the most prominent example. Unfortunately, query answering under the CQA semantics has been shown to be computationally intractable, even when extremely simple ontology languages are considered. In this paper, we address this problem by proposing two new families of inconsistency-tolerant semantics which approximate the CQA semantics from above (complete approximations) and from below (sound approximations) and converge to the CQA semantics in the limit. We study the data complexity of conjunctive query answering under these new semantics and show a general tractability result for the whole class of first-order rewritable ontology languages, which includes, among others, several description logics of the DL-Lite family and rule-based languages of the Datalog+/- family. We also analyze the combined complexity of query answering for ontology languages of the DL-Lite family.\n\n", "title": "Tractable Approximations of Consistent Query Answering for Robust Ontology-based Data Access", "authors": [{"name": "Meghyn Bienvenu"}, {"name": " Riccardo Rosati"}]}, "12": {"session": "15:45 - 17:30", "abstract": "", "title": "A Sound and Complete Backward Chaining Algorithm for Existential Rules", "authors": [{"name": "Melanie Konig"}, {"name": " Michel Leclere"}, {"name": " Marie-Laure Mugnier and Michael Thomazo"}]}, "18": {"session": "15:45 - 17:30", "abstract": "The number partitioning problem seeks to divide a set of n numbers across k distinct subsets so as to minimize the sum of the largest partition. In recent years, several efficient algorithms have been developed that offer substantially improved performance, especially in the general case where k &gt; 2. In this work, we develop a new optimal algorithm for multi-way partitioning. A critical observation motivating our methodology is that a globally optimal k-way partition may be recursively constructed by obtaining suboptimal solutions to subproblems of size k - 1. We also demonstrate how to further prune unpromising partial assignments by detecting and eliminating dominated solutions. Our approach outperforms the previous state-of-the-art by up to three orders of magnitude, reducing average runtime on the largest benchmarks from several hours to less than a second.", "title": "Search Strategies for Optimal Multi-Way Number Partitioning", "authors": [{"name": "Michael Moffitt"}]}, "1182": {"session": "13:30 - 15:15", "abstract": "Function-free rules with existential quantifiers in\n\nrule heads have attracted much recent interest in\n\nknowledge representation. In this work, we extend\n\nexistential rules with nonmonotonic negation\n\nunder a stable model semantics. Computing entailments\n\nunder this semantics is difficult, since a set\n\nof existential rules can have multiple, potentially\n\ninfinite stable models. To address this, we analyse\n\nwhether one rule relies on another, in the sense that\n\nit might either be \u0089\u00db\u00f7triggered\u0089\u00db\u00aa or \u0089\u00db\u00f7inhibited\u0089\u00db\u00aa by the\n\nother rule\u0089\u00db\u00aas application. These relationships allow\n\nus to define R-acyclicity and R-stratification, which\n\nencompass large classes of rule sets that have finite,\n\nunique stable models for arbitrary sets of input\n\nfacts. Realistic datasets are rarely arbitrary, and\n\nwe further extend our criteria to take advantage of\n\nknown constraints. Checking each of these criteria\n\nis computationally feasible, and we provide tight\n\ncomplexity bounds. We apply our results to a realworld\n\nknowledge base from biochemistry that involves\n\ncomplex relational structures. Our methods\n\nallow us to solve reasoning problems over this data\n\nusing an off-the-shelf answer set programming engine.\n\nThe entailments expose various errors in the\n\noriginal dataset, thus illustrating the practical utility\n\nof logical reasoning in this domain.", "title": "Computing Stable Models for Nonmonotonic Existential Rules", "authors": [{"name": "Despoina Magka"}, {"name": " Markus Kr&ouml;tzsch"}, {"name": " Ian Horrocks"}]}, "867": {"session": "15:45 - 17:30", "abstract": "Product defects and rework efforts due to flawed specifications represent major issues for a project\\'s performance, so that there is a high motivation for providing effective means that assist designers in assessing and ensuring a specification\\'s quality. Recent research in the context of formal specifications, e.g. on coverage and vacuity, offers important means to tackle related issues. In the currently underrepresented research direction of diagnostic reasoning on a specification, we propose a scenario-based diagnosis at a specification\\'s operator level using weak or strong fault models. Drawing on efficient SAT encodings, we show in this paper how to achieve that effectively for specifications in LTL. Our experimental results illustrate our approach\\'s validity and attractiveness.\n\n", "title": "Behavioral Diagnosis of LTL Specifications at Operator Level", "authors": [{"name": "Ingo Pill"}, {"name": " Thomas Quaritsch"}]}, "936": {"session": "15:45 - 17:30", "abstract": "Merge-and-Shrink (M&amp;S) and Symbolic search are two related approaches to derive admissible heuristics for optimal planning. We present a new combination of these techniques, Symbolic Merge-and-Shrink (SM&amp;S) which uses M&amp;S abstractions as a relaxation criteria for a symbolic backward search. Empirical evaluation shows that SM&amp;S combines the strengths of both techniques deriving heuristics at least as good as the best of them for most domains.", "title": "Symbolic Merge-and-Shrink for Cost-Optimal Planning", "authors": [{"name": "&Aacute;lvaro Torralba"}, {"name": " Carlos Linares L&oacute;pez"}, {"name": " Daniel Borrajo"}]}, "1386": {"session": "13:30 - 15:15", "abstract": "En-route charging stations allow electric vehicles to charge away from home, thus greatly extending their range. However, as a full charge takes a considerable amount of time, there may be significant congestion and long waiting times at peak times. To address this problem, we propose a novel navigation system, which uses intentions (i.e. routing policies) of other agents employing the system to accurately predict congestion at charging stations, and suggest the most efficient route. We achieve this by extending time-dependent stochastic routing algorithms from the transportation literature to include the battery\\'s state-of-charge and charging stations. Furthermore, we combine historic information with agent intentions to model the queues at charging stations and predict their waiting times. Through simulations we show that an agent significantly benefits from using the intention-aware system compared to using only historic information, and that average waiting times decrease as more agents use the system. \n\n", "title": "Intention-Aware Routing to Minimise Delays at Electric Vehicle Charging Stations", "authors": [{"name": "Mathijs de Weerdt"}, {"name": " Enrico Gerding"}, {"name": " Sebastian Stein"}, {"name": " Valentin Robu"}, {"name": " Nick Jennings"}]}, "720": {"session": "13:30 - 15:15", "abstract": "Mining opinion targets from online reviews is an important and challenging task in opinion mining. This paper proposes a novel approach to extract opinion targets by using partial-supervised word alignment model (PSWAM). At first, we apply PSWAM in a monolingual scenario to mine opinion relations in sentences and estimate the associations between opinion target candidates and opinion words. Then, a graph-based algorithm is exploited to estimate the confidence of each candidate where the candidates with higher confidence will be extracted as the opinion targets. Compared with previous syntax-based methods, PSWAM can effectively avoid errors from parsing when dealing with informal sentences in online reviews. Compared with the methods using alignment model, PSWAM can capture opinion relations more precisely by using partial alignment links as constrains when performing alignment. Moreover, when estimating candidate confidence using our graph-based algorithm, we make penalties on higher-degree vertices in order to decrease the probability of the random walk running into the unrelated regions in the graph. As a result, the precision can be improved. The experimental results on three data sets with different sizes and languages show that our approach can achieve superior performance on the state-of-the-art methods.", "title": "Opinion Target Extraction Using Partial-Supervised Word Alignment Model", "authors": [{"name": "Kang Liu"}, {"name": " Liheng Xu"}, {"name": " Jun Zhao"}]}, "934": {"session": "13:30 - 15:15", "abstract": "We focus on solving two-player zero-sum extensive-form games with perfect information and simultaneous moves. In these games, both players fully observe the current state of the game where they simultaneously make a move determining the next state of the game. We solve these games by a novel algorithm that relies on two factors: (1) it iteratively solves the games that correspond to a single simultaneous move by a double-oracle approach, and (2) it prunes the states of the game using bounds on the sub-game values obtained by classical Alpha-Beta search on a serialized variant of the game. We experimentally evaluate our algorithm on card game Goofspiel, a pursuit-evasion game, and randomly generated games. The results show that our novel algorithm provides significant running-time improvements and reduction in the number of evaluated nodes compared to the full search algorithm.", "title": "Using Double-oracle Method and Serialized Alpha-Beta Search for Pruning in Simultaneous Moves Games", "authors": [{"name": "Branislav Bosansky"}, {"name": " Jiri Cermak"}, {"name": " Viliam Lis&yacute;"}, {"name": " Roman Vitek"}, {"name": " Michal Pechoucek"}]}, "153": {"session": "15:45 - 17:30", "abstract": "Large-scale observational datasets are prevalent in many areas of research, including biomedical informatics, computational social science, and finance. However, our ability to use these data for decision-making lags behind our ability to collect and mine them. One reason for this is the lack of methods for inferring the causal impact of rare events. In cases such as the monitoring of continuous data streams from intensive care patients, social media, or finance, though, rare events may in fact be the most important ones. They may, for example, signal critical changes in a patient's status or trading volume. While prior data mining approaches can identify or predict rare events, they cannot determine their impact, and probabilistic causal inference methods fail to handle inference with infrequent events. Instead, we develop a new approach to finding the causal impact of rare events that leverages the large amount of data available to infer a model of a system's functioning and evaluates how rare events explain deviations from usual behavior. Using simulated data, we evaluate the approach and compare it against others, demonstrating that it can accurately and precisely infer the effects of rare events.", "title": "Causal Inference with Rare Events in Large-Scale Time-Series Data", "authors": [{"name": "Kleinberg Samantha"}]}, "601": {"session": "15:45 - 17:30", "abstract": "As collaborative tagging systems become increasingly popular in recent years, accurate and scalable tag recommending methods are more and more required. In practice, only several basic recommendation strategies are applied, most known as collaborative filtering methods. In reality, however, CF tends to suffer from the sparsity problem. In this paper, we propose a novel model based on Collaborative Topic Regression model(CTR) and seamlessly exploit the related information among items. Our evaluation shows that our model achieves significantly better recall than classical collaborative filtering approaches and CTR model even in extremely sparse condition.", "title": "Relation Regularized Collaborative Topic Regression for Tag Recommendation", "authors": [{"name": "Hao Wang"}, {"name": " Binyi Chen"}, {"name": " Wu-Jun Li"}]}, "884": {"session": "15:45 - 17:30", "abstract": "We continue our initial study regarding basic level of concepts in conceptual \n\ncategorization. Basic level of concepts is an important phenomenon studied \n\nin the psychology of concepts. We propose to utilize this phenomenon in \n\nformal concept analysis (FCA) to select important formal concepts. This is \n\na well-known critical problem because, as a rule, the number of all \n\nconcepts extracted from data is large. For this purpose, we review and \n\nformalize the main existing psychological approaches to basic level which \n\nare presented only informally and are not related to any particular formal \n\nmodel of concepts in the psychological literature. We argue and demonstrate \n\nby examples that basic level concepts may be regarded as  the most interesting \n\nformal concepts from user viewpoint. Interestingly, our formalization and \n\nexperiments reveal some previously unknown relationships between the\n\nexisting approaches to basic level. Thus, we  argue that a formalization of basic \n\nlevel in the framework  of FCA is beneficial for the psychologal investigations \n\nthemselves because it puts them on solid ground.", "title": "Basic Level in Formal Concept Analysis: Interesting Concepts and Psychological Ramifications", "authors": [{"name": "Radim Belohlavek"}, {"name": " Martin Trnecka"}]}, "158": {"session": "13:30 - 15:15", "abstract": "In social choice settings with strict preferences, random dictatorship rules were characterized by Gibbard [1977] as the only randomized social choice functions that satisfy strategyproofness and ex post efficiency. In the more general domain with indifferences, RSD (random serial dictatorship) rules are the well-known and perhaps only known generalization of random dictatorship. We present a new generalization of random dictatorship for indifferences called Maximal Recursive (MR) rule as an alternative to RSD. We show that MR is polynomial-time computable, weakly strategyproof with respect to stochastic dominance, and, in some respects, outperforms RSD on efficiency.", "title": "Maximal Recursive Rule: A New Social Decision Scheme", "authors": [{"name": "Haris Aziz"}]}, "888": {"session": "15:45 - 17:30", "abstract": "Diffusion processes in networks are increasingly used to model the spread of wildlife, information or social influence. Our work addresses the crucial problem of learning the underlying parameters that govern such a diffusion process by observing the time at which nodes become active. A key advantage of our approach is that, unlike previous work, it can tolerate missing observations for some nodes in the diffusion process. Having incomplete observations is characteristic of offline networks used to model the spread of wildlife. We develop the EM algorithm to address parameter learning in such settings. Since both the E and M steps are computationally challenging, we employ a number of optimization methods such as nonlinear and DC programming to address these challenges. Evaluation of the approach on the Red-cockaded Wood- pecker conservation problem shows that it is highly robust and accurately learns parameters in various settings, even with more than 80% missing data.", "title": "Parameter Learning for Latent Network Diffusion", "authors": [{"name": "Xiaojian Wu"}, {"name": " Akshat Kumar"}, {"name": " Daniel Sheldon"}, {"name": " Shlomo Zilberstein"}]}, "1200": {"session": "15:45 - 17:30", "abstract": "Nonnegative Matrix Tri-factorization (NMTF) and its graph regularized\n\nextensions have been widely used for co-clustering task to group data\n\npoints and features simultaneously. However existing methods are sensitive\n\nto noises and outliers which is due to the squared loss function is used\n\nto measure the quality of data reconstruction and graph regularization. In\n\nthis paper, we extend GNMTF by introducing a sparse outlier matrix into\n\nthe data reconstruction function and applying the $\\ell_1$ norm to measure\n\ngraph dual regularization errors, which leads to a novel Robust\n\nCo-Clustering (RCC) method. Accordingly, RCC is expected to obtain a more\n\nfaithful approximation to the data recovered from sparse outliers, and\n\nachieve robust regularization by reducing the regularization errors of\n\nunreliable graphs via $\\ell_1$ norm. To solve the optimization problem of\n\nRCC, an alternating iterative algorithm is provided and its convergence is\n\nalso proofed. We also show the connection between the sparse outlier\n\nmatrix in data reconstruction function and the robust Huber M-estimator.\n\nExperimental results on real-world data sets show that our RCC\n\nconsistently outperforms the other algorithms in terms of clustering\n\nperformance, which validates the effectiveness and robustness of the\n\nproposed approach.", "title": "Towards Robust Co-Clustering", "authors": [{"name": "Liang Du"}, {"name": " Yidong Shen"}]}, "285": {"session": "15:45 - 17:30", "abstract": "Top-k voting is an especially natural form of partial vote elicitation in which only length k prefixes of rankings are elicited. We analyze the ability of top-k vote elicitation to correctly determine true winners, with high probability, given probabilistic models of voter preferences and can- didate availability. We provide bounds on the minimal value of k required to determine the correct winner un- der the plurality and Borda voting rules in both the worst- case preference profiles and under impartial culture and Mallows models; and we derive conditions under which the special case of zero-elicitation (i.e., k = 0) produces the correct winner. Empirical results confirm the value of top-k voting.", "title": "Efficient Vote Elicitation under Candidate Uncertainty", "authors": [{"name": "Joel Oren"}, {"name": " Yuval Filmus"}, {"name": " Craig Boutilier"}]}, "237": {"session": "13:30 - 15:15", "abstract": "This paper introduces Monte-Carlo *-Minimax Search (MCMS), a new Monte-Carlo search algorithm for finite, turned-based, stochastic, two-player, zero-sum games of perfect information. Through a combination of sparse sampling and pruning techniques in the expectimax framework, MCMS allows deep plans to be constructed in the challenging setting of densely stochastic games, i.e., games where one would rarely ever expect to see the same successor state multiple times at any particular chance node. The performance of the algorithm is evaluated on four games: Pig, EinStein W&uuml;rfelt Nicht!, Can\u0089\u00db\u00aat Stop, and Ra.", "title": "Monte Carlo *-Minimax Search", "authors": [{"name": "Marc Lanctot"}, {"name": " Abdallah Saffidine"}, {"name": " Joel Veness"}, {"name": " Chris Archibald"}, {"name": " Mark Winands"}]}, "958": {"session": "13:30 - 15:15", "abstract": "This paper addresses the target valued search (TVS) problem, which is the\n\nproblem of finding a path between two nodes in a graph whose cost is as close as\n\npossible to a given target value, T. This problem has been previously addressed \n\nonly in directed acyclic graphs. In this work we develop the theory required to \n\nsolve this problem optimally for any type of graph. We adapt traditional heuristic \n\nsearch algorithms for this setting, and propose a novel bidirectional search \n\nalgorithm that is specifically suited for TVS. The benefits of this bidirectional \n\nsearch algorithm are discussed both theoretically and experimentally on several \n\ndomains. ", "title": "Target-Value Search Revisited", "authors": [{"name": "Carlos Linares L&oacute;pez"}, {"name": " Roni Stern"}, {"name": " Ariel Felner"}]}, "496": {"session": "15:45 - 17:30", "abstract": "RCC5 is an important and well-known calculus for representing and reasoning about mereological relations. Among many other applications, it is pivotal in the formalization of commonsense reasoning about natural categories. More in particular, it allows for a qualitative representation of conceptual spaces in the sense of G&auml;rdenfors. To further the role of RCC5 as a vehicle for conceptual reasoning, in this paper we combine RCC5 relations with information about betweenness of regions. The resulting calculus allows us to express, for instance, that some part (but not all) of region B is between regions A and C. We show how consistency for such networks can be decided in polynomial time for atomic networks, even when regions are required to be convex. From an application perspective, the ability to express betweenness information allows us to use RCC5 as a basis for interpolative reasoning, while the restriction to convex regions ensures that all consistent networks can be faithfully represented as a conceptual space.\n\n", "title": "Combining RCC5 relations with betweenness information", "authors": [{"name": "Steven Schockaert"}, {"name": " Sanjiang Li"}]}, "1": {"session": "15:45 - 17:30", "abstract": "", "title": "A Case-Based Solution to the Cold-Start Problem in Group Recommenders", "authors": [{"name": "Lara Quijano-Sanchez"}, {"name": " Derek Bridge"}, {"name": " Belen Diaz-Agudo and Juan Recio-Garcia"}]}, "323": {"session": "15:45 - 17:30", "abstract": "Semi-Structured data contains both text data and document metadata, such as papers with authors and web pages with different labels(tags).\n\nIn this paper we propose a novel method to model the semi-structured data, called the Semi-Structured LDA (SSLDA). \n\nThe SSLDA is a framework that leverages all the metadata(labels or tags) which appears in one document to infer the topic components for each document.\n\nThis allows SSLDA to directly learn document-topic distributions, but also infers the labels(tags) topic distributions for the purpose of classifications, clusters, recommendations and so on. \n\nBesides, the SSLDA automatically infers the probabilistic importance of different labels(tags). \n\nEfficient variational inference and EM algorithm is presented for model parameter estimation. \n\nWe demonstrate SSLDA's improved expressiveness over traditional LDA or other labeled topic models with visualizations of three corpus of semi-structured data, resulting in document modeling, text classification and label prediction.", "title": "Tag-Weighted Topic Model for Mining  Documents  with Labels", "authors": [{"name": "Shuangyin Li"}]}, "325": {"session": "15:45 - 17:30", "abstract": "We study map matching, the problem of estimating the route that is traveled by a driver, where the observation points with the Global Positioning System (GPS) are available.  The state-of-the-art approach for this problem is a Hidden Markov Model (HMM).  We propose a particular transition probability between latent road segments by the use of the number of turns in addition to the travel distance between the latent road segments.  We show, through numerical experiments, that the error with the map matching with the state-of-the-art HMM can be reduced substantially with the proposed transition probability.", "title": "Map Matching with Multiple Route Metrics", "authors": [{"name": "Takayuki Osogami"}, {"name": " Rudy Raymond"}]}, "1562": {"session": "13:30 - 15:15", "abstract": "Description Logic (DL) programs are a prominent approach for loose\n\ncoupling of a Description Logic knowledge base (alias, ontology) with\n\nnonmonotonic logic programming, in order to support rule-based reasoning\n\non top of ontologies, using a well-defined query interface. They in\n\nparticular allow to combine inferences from the ontology, and a\n\nbidirectional information flow between the rules and the ontology makes\n\nit possible to have adaptive combinations and solve advanced reasoning\n\nproblems. However, the interaction may cause inconsistency in that no\n\nanswer set, i.e., no model, of a DL-program exists. To remedy this\n\nproblem, i.e., in order to restore consistency, we consider the repair\n\nof DL-programs, taking the view that the data in the underlying ontology\n\nmight not be fully adequate. Therefore, repair answer sets are defined\n\nin terms of changes to (repair of) the data part of the ontology. We\n\nanalyze the computational complexity of the notion, and present results\n\nthat allow current algorithms for computing answer sets of DL-programs\n\nto be extended to compute repair answer sets. In the course of this, we\n\nencounter a novel ontology repair problem as a subtask, in which the\n\nentailment and non-entailment of a set of queries to the ontology has to\n\nbe guaranteed. While this problem is unsurprisingly intractable in\n\ngeneral, we identify useful classes of repairs in practice that are\n\ntractable for the well-known Description Logic DL-Lite, and which can be\n\ncomputed by the extended algorithm.", "title": "Data Repair of Inconsistent DL-programs", "authors": [{"name": "Thomas Eiter"}, {"name": " Michael Fink"}, {"name": " Daria Stepanova"}]}, "777": {"session": "13:30 - 15:15", "abstract": "We present an embedding of stochastic optimal control problems, of the so called path integral form, into reproducing kernel Hilbert spaces. Using consistent, sample based estimates of the embedding leads to a model-free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e.g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided.", "title": "Path Integral Control by Reproducing Kernel Hilbert Space Embedding", "authors": [{"name": "Konrad Rawlik"}, {"name": " Marc Toussaint"}, {"name": " Sethu Vijayakumar"}]}, "1170": {"session": "13:30 - 15:15", "abstract": "Most studies were devoted to the design of efficient algorithms and the evaluation and application on diverse ranking problems, whereas few work has been paid to the theoretical studies on ranking learnability. In this paper, we study the relation between uniform convergence, stability and learnability of ranking. In contrast to supervised learning where the learnability is equivalent to uniform convergence, we show that the ranking uniform convergence is sufficient but not necessary for ranking learnability with AERM, and we further present a sufficient condition for ranking uniform convergence with respect to bipartite ranking loss. Considering the ranking uniform convergence being unnecessary for ranking learnability, we prove that the ranking average stability is a necessary and sufficient condition for ranking learnability.", "title": "Uniform Convergence, Stability and Learnability for Ranking Problems", "authors": [{"name": "Wei Gao"}, {"name": " Zhi-Hua Zhou"}]}, "208": {"session": "13:30 - 15:15", "abstract": "  This paper proposes a simple linear Bayesian approach to reinforcement learning. We show that with an appropriate basis, a Bayesian linear Gaussian model is sufficient for accurately estimating the system dynamics, and in particular when we allow for correlated noise. Policies are estimated by first sampling a transition model from the current posterior, and then performing approximate dynamic programming on the sampled model. This form of approximate Thompson sampling results in good exploration in unknown environments. When the approximate dynamic programming is least-squares, the approach can be seen as an online, Bayesian generalisation of LSPI, where the empirical transition matrix is replaced with a sample from the posterior.\n\n", "title": "Linear Bayesian Reinforcement Learning", "authors": [{"name": "Nikolaos Tziortziotis"}, {"name": " Christos Dimitrakakis"}, {"name": " Konstantinos D.  Blekas"}]}, "1275": {"session": "15:45 - 17:30", "abstract": "Labeling training data is quite timeconsuming\n\nbut essential for supervised learning\n\nmodels. To solve this problem, the active\n\nlearning has been studied and applied to select\n\nthe informative and representative data\n\npoints for labeling. However, during the early\n\nstage of experiments, only a small number\n\n(or none) of labeled data points exist, thus\n\nthe most representative samples should be\n\nselected first. In this paper, we propose a\n\nnovel robust active learning method to handle\n\nthe early stage experimental design problem\n\nand select the most representative data\n\npoints. The robust sparse representation loss\n\nfunction is utilized to reduce the effect of\n\noutliers and the structured sparse regularization\n\nis adopted to find the most representative\n\nsamples during the sparse representations.\n\nA new efficient optimization algorithm\n\nis introduced to solve our non-smooth\n\nobjective with low computational cost and\n\nproved global convergence. Empirical results\n\non both single-label and multi-label classification\n\nbenchmark data sets show the promising\n\nresults of our method.", "title": "Early Active Learning via Robust Representation and Structured Sparsity", "authors": [{"name": "Feiping Nie"}, {"name": " Hua Wang"}, {"name": " Heng Huang"}]}, "486": {"session": "15:45 - 17:30", "abstract": "Abduction is among the most fundamental reasoning methods, with many important applications. Unfortunately, the computational complexity of this problem is very high (\\SigmaTwo-complete). This complexity barrier rules out the existence of a polynomial reduction to SAT. In this work we use structural properties (backdoor sets) to break this complexity barrier. We present efficient (fixed-parameter tractable) transformations from Abduction to SAT, which allow us to utilize the power of SAT solvers directly. Furthermore, we explain how to extend these transformations to enumerate all (subset-minimal) solutions and filter by constraints.", "title": "Backdoors to Abduction", "authors": [{"name": "Andreas Pfandler"}, {"name": " Stefan R&uuml;mmele"}, {"name": " Stefan Szeider"}]}, "1041": {"session": "15:45 - 17:30", "abstract": "We study the complexity of conjunctive query answering under guarded-based disjunctive existential rules, i.e., existential rules extended with disjunction in rule-heads. We focus our attention on (weakly-)(frontier-)guarded rules, and their main subclasses, i.e., linear rules and inclusion dependencies (IDs). Our main result states that conjunctive query answering under a fixed set of disjunctive IDs is 2EXPTIME-hard. This surprising result together with a 2EXPTIME upper bound for weakly-frontier-guarded disjunctive rules, obtained by exploiting results on the satisfiability of guarded negation first-order logic, gives us a complete picture of the computational complexity of our problem. We also study two natural subclasses of disjunctive IDs, namely frontier-one (i.e., only one variable is propagated), and full-identity (i.e., we are only allowed to copy a tuple), for which the combined complexity decreases to EXPTIME and coNP, respectively.", "title": "The Impact of Disjunction on Query Answering Under Guarded-based Existential Rules", "authors": [{"name": "Pierre Bourhis"}, {"name": " Michael Morak"}, {"name": " Andreas Pieris"}]}, "512": {"session": "15:45 - 17:30", "abstract": "Probabilistic abstract argumentation is an interesting extension of Dung's abstract argumentation framework, combining probability theory with argumentation. \n\nIn this setting, we address the fundamental problem of computing the \n\nprobability that a set of  arguments is an extension according to a given semantics. \n\nWe focus on the most popular semantics\n\n(i.e., admissible, stable, complete, grounded, preferred, ideal), \n\nand show the following dichotomy result:\n\ncomputing the probability that a set of arguments is an extension is\n\neither PTIME or FP^#P-complete depending on the semantics adopted.\n\nOur PTIME results are particularly interesting, \n\nas they hold for some semantics for which no polynomial-time \n\ntechnique was known so far.", "title": "On the Complexity of Probabilistic Abstract Argumentation", "authors": [{"name": "Bettina Fazzinga"}, {"name": " Sergio Flesca"}, {"name": " Francesco Parisi"}]}, "1289": {"session": "15:45 - 17:30", "abstract": "In recent computer vision research, multiple visual de-\n\nscriptors have been designed for image understanding,\n\nwhere different visual descriptors describe distinct perspec-\n\ntives of the images. Although each type of visual descrip-\n\ntor could be individually used for unsupervised image cat-\n\negorizations, the categorization results will be more ac-\n\ncurate by exploring rich information among multi-modal\n\ndescriptors. Several multi-modal clustering methods have\n\nbeen proposed to unsupervised integrate multi-modal im-\n\nages. However, they are graph based approaches, e.g.\n\nbased on spectral clustering, such that they cannot han-\n\ndle the big visual data. How to combine these multi-modal\n\nvisual features for unsupervised large-scale image catego-\n\nrizations has become a challenging problem. In this paper,\n\nwe propose a new robust large-scale clustering method to\n\nintegrate heterogeneous visual features. We evaluate the\n\nproposed new methods by six computer vision benchmark\n\ndata sets and compared the performance with several com-\n\nmonly used clustering approaches as well as the baseline\n\nmulti-modal clustering methods. In all experimental results,\n\nour proposed methods consistently achieve superiors clus-\n\ntering performances.", "title": "Multi-View K-Means Clusteriing for Heterogeneous Data Integration", "authors": [{"name": "Xiao Cai"}, {"name": " Feiping Nie"}, {"name": " Heng Huang"}]}, "1366": {"session": "15:45 - 17:30", "abstract": "Cross-domain collaborative filtering (CDCF) is be-coming an emerging research topic in recent years, which aims to leverage data from multiple do-mains to relieve the data sparsity issue. However, current CDCF methods only consider user and item factors but totally neglect heterogeneous domain factors, which may lead to improper knowledge transfer issues. To address this problem, we pro-pose a novel CDCF model, namely Bi-Linear Mul-tilevel Analysis (BLMA), which seamlessly bridges multilevel analysis theory to the most successful CF method, matrix factorization. Specially, BLMA uses a hierarchical structure that jointly consider the domain, community and user effects over items, so it can always generate domain adaptive factors using such multilevel effects. Moreover, a parallel Gibbs sampler is also devised to learn these effects. Experiments conducted on the real-world dataset prove the superiority of BLMA against other comparative state-of-the-art methods.", "title": "Cross-Domain Collaborative Filtering via Bilinear Multilevel Analysis", "authors": [{"name": "Liang Hu"}, {"name": " Jian Cao"}, {"name": " Guandong Xu"}, {"name": " Jie Wang"}, {"name": " Zhiping Gu"}]}, "356": {"session": "15:45 - 17:30", "abstract": "Algorithms for stable marriage and related matching problems typically assume that full preference information is available. While the Gale-Shapley algorithm can be viewed as a means of eliciting preferences incrementally, it does not prescribe a general means for matching with incomplete information, nor is it designed to minimize elicitation. We propose the use of maximum regret to measure the (inverse) degree of stability of a matching with partial preferences; minimax regret to find matchings that are maximally stable in the presence of partial preferences; and heuristic elicitation schemes that use max regret to determine relevant preference queries. We show that several of our schemes find stable matchings while eliciting considerably less preference information than Gale-Shapley and are much more appropriate in settings where approximate stability is viable.", "title": "Elicitation and Approximately Stable Matching with Partial Preferences", "authors": [{"name": "Joanna Drummond"}, {"name": " Craig Boutilier"}]}, "1579": {"session": "15:45 - 17:30", "abstract": "Convolution tree kernels have been success- fully applied to many language processing tasks for achieving state-of-the-art accuracy. Unfortunately, kernels require working in the dual space, which renders large-scale learning impractical. In this paper, we study the lat- est approaches to solve such problem ranging from feature hashing to reverse kernel engi- neering and approximate cutting plane train- ing with model compression. We derive a method that relies on reverse-kernel engineer- ing together with an efficient kernel learning method. The approach gives the advantage of using tree kernels to automatically gener- ate rich structured feature spaces and working in the linear space where learning and testing is fast. We experimented with training sets up to 4 million examples from Semantic Role Labeling. The results show that (i) structural features are essential and (ii) we can speed-up training from weeks to less than 20 minutes.", "title": "Fast Linearization of Tree Kernels over Large-Scale Data", "authors": [{"name": "Aliaksei Severyn"}, {"name": " Alessandro Moschitti"}]}, "471": {"session": "15:45 - 17:30", "abstract": "Cross-lingual links in Wikipedia have become an important resource for sharing knowledge across different languages. In order to enrich this resource, several knowledge linking approaches have been proposed for finding missing cross-lingual links between wikis. In these approaches, seed cross-lingual links and the inner link structures in wikis are two important factors for finding new cross-lingual links. When there are not enough seed cross-lingual links and inner links, discovering new cross-lingual links becomes a challenging problem. In order to solve this problem, we propose an approach that uses concept annotation to boost cross-lingual knowledge linking. Our approach first annotates each wiki article with concepts corresponding to the articles in the same wiki, so that missing inner links can be identified. Then, a pair-wise regression model is trained to predict new cross-lingual links based on six link-based similarity features.  The concept annotation and cross-lingual link prediction are two mutually enforced steps, therefore can be excuted iteratively to incrmentally find new cross-lingual links. Evaluation on the English and Chinese Wikipedia data shows that both the concept annotation method and cross-lingual link prediction method perform effectively. In the iterative running of our approach, more cross-lingual links can be found when concept annotation is performed. ", "title": "Boosting Cross-lingual Knowledge Linking via Concept Annotation", "authors": [{"name": "Wang Zhichun"}, {"name": " Juanzi Li"}, {"name": " Jie Tang"}]}, "689": {"session": "15:45 - 17:30", "abstract": "Since wind has an intrinsically complex and stochastic nature, accurate wind power forecasts are necessary for the safety and economics of wind energy utilization. In this paper, we investigated a combination of numeric and probabilistic models: Gaussian Process (GP) models were applied to one-day-ahead wind power forecasting, by processing data from Numerical Weather Prediction (NWP) model. Firstly the wind speed data from NWP was corrected by a GP, then as there is always a defined limit on power generated in a wind turbine due the turbine controlling strategy, wind power forecasts were realized by modeling the relationship between the corrected wind speed and power output using a Censored GP. To verify the proposed approach, two real world datasets were used for model construction and testing. The simulation results were compared with persistence method and Artificial Neural Network (ANN) models, and as calculated by Mean Absolute Error (MAE), the proposed model presents around 11% improvement in forecasting accuracy comparing to ANN model on one dataset, and nearly 5% improvement on another.", "title": "Short-term Wind Power Forecasting using Gaussian Processes", "authors": [{"name": "Niya Chen"}, {"name": " Zheng  Qian"}, {"name": " Ian Nabney"}, {"name": " Xiaofeng Meng"}]}, "1571": {"session": "13:30 - 15:15", "abstract": "Coreference resolution is the problem of clustering mentions into entities and is very critical for natural language understanding. This paper studies the problem of coreference resolution in the context of the newly emerging domain of Electronic Health Records (EHRs). We used a variant of best-link decoding strategy in our coreference model. Our decoding strategy allows us to use several constraints while selecting the best antecedent. Most of our constraints are based on the context in which the mentions appear. In this paper, we also show that different pronouns behave quite differently while doing coreference resolution. So, we developed different modules for resolving different pronouns. Our method for pronominal resolution and our use of constraints gave us significant performance improvements. To the best of our knowledge, we report the best results on the corpora used by us. Finally, we also report the results for end-to-end coreference resolution. As far as we know, end-to-end results have not been reported previously on the corpora used by us.\n\n", "title": "End-to-End Coreference Resolution for Clinical Narratives", "authors": [{"name": "Prateek Jindal"}, {"name": " Dan Roth"}]}, "1574": {"session": "13:30 - 15:15", "abstract": "Control and manipulation are two of the most studied types of attacks on elections.  In this paper, we study the complexity of control attacks on elections in which there are manipulators.  We study both the case where the \"chair\" who is seeking to control the election is allied with the manipulators, and the case where the manipulators seek to thwart the chair.  In the latter case, we see that the order of play matters substantially regarding the complexity.  We prove upper bounds, that hold over every election system with a polynomial-time winner problem, for all standard control cases, and some of these bounds are at the second or third level of the polynomial hierarchy.  Nonetheless, for important natural systems the complexity can be much lower.  For example, we prove that for plurality, Condorcet, and approval elections, the complexity of even competitive clashes between a controller and manipulators falls far below those high bounds, even as low as polynomial time.", "title": "Control in the Presence of Manipulators: Cooperative and Competitive Cases", "authors": [{"name": "Zack Fitzsimmons"}, {"name": " Edith Hemaspaandra"}, {"name": " Lane Hemaspaandra"}]}, "804": {"session": "13:30 - 15:15", "abstract": "There are many problems that neither A* nor IDA* can solve, because A* runs\n\nout of memory and IDA* runs out of time. \\emph{Forward Perimeter Search\n\n(FPS)} is a compromise between the two extremes. It builds a perimeter\n\naround the start node and tests for each perimeter node whether it lies on\n\na shortest path to the goal. The testing is done in parallel with\n\nbreadth-first iterative-deepening A* (BF-IDA). Its information is used to\n\nrefine the perimeter, so that the goal will be found early in the last\n\niteration.\n\n\n\nOur empirical results on two application domains, 24-puzzle and 17-pancake,\n\nindicate that FPS expands fewer nodes than BF-IDA* while requiring several\n\norders of magnitude less memory space.\n\n\n\nAdditionally, we present a hard problem instance of the 24-puzzle that needs\n\nat least 26 more moves to solve than the currently hardest instance. We prove\n\nwith FPS that its solution requires either 140 or 142 moves.", "title": "Forward Perimeter Search with Controlled Use of Memory", "authors": [{"name": "Thorsten Schuett"}, {"name": " Robert Doebbelin"}, {"name": " Alexander Reinefeld"}]}}