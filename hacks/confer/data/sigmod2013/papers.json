{"pods041": {"session": "Query Languages", "abstract": "We perform a fundamental investigation of the complexity of conjunctive query evaluation from the perspective of parameterized complexity.  \n\nWe classify sets of boolean conjunctive queries according to the complexity of\n\nthis problem.  Previous work showed that a set of conjunctive queries\n\nis fixed-parameter tractable precisely when the set is equivalent to a\n\nset of queries having bounded treewidth.  We present a fine\n\nclassification of query sets up to parameterized logarithmic space\n\nreduction.  We show that, in the bounded treewidth regime, there are\n\nthree complexity degrees and that the properties that determine the\n\ndegree of a query set are bounded pathwidth and bounded tree depth.\n\nWe also engage in a study of the two higher degrees via logarithmic\n\nspace machine characterizations and complete problems.  Our work\n\nyields a significantly richer perspective on the complexity of\n\nconjunctive queries and, at the same time, \n\nsuggests new avenues of research in parameterized complexity.", "title": "The Fine Classification of Conjunctive Queries and Parameterized Logarithmic Space Complexity", "authors": [{"affiliation": "Universidad del Pa\u00eds Vasco and IKERBASQUE", "location": "San Sebastian  Spain ", "name": "Hubie Chen", "email": "Chen"}, {"affiliation": "Kurt Godel Research Center", "location": "Vienna  Austria ", "name": "Moritz M\u00fcller", "email": "M\u00fcller"}]}, "sig633": {"session": "Complex Event Processing", "abstract": "Regular expression matching over sequences in real time is a crucial task in complex event processing on data streams. Given that such data sequences are often noisy and errors have temporal and spatial correlations, performing regular expression matching effectively and efficiently is a challenging task. Instead of the traditional approach of learning a distribution of the stream first and then processing queries, we propose a new approach that efficiently does the matching based on an error model. In particular, our algorithms are based on the realistic Markov chain error model, and report all matching paths to trace relevant basic events that trigger the matching. This is much more informative than a single matching path. We also devise algorithms to efficiently return only top-k matching paths, and to handle negations in an extended regular expression. Finally, we conduct a comprehensive experimental study to evaluate our algorithms using real datasets.", "title": "E-Matching: Event Processing over Noisy Sequences in Real Time", "authors": [{"affiliation": "University of Massachusetts, Lowell", "location": "Lowell MA USA Computer Science", "name": "Zheng Li", "email": "Li"}, {"affiliation": "University of Massachusetts, Lowell", "location": "Lowell MA USA Computer Science", "name": "Tingjian Ge", "email": "Ge"}, {"affiliation": "University of Massachusetts, Lowell", "location": "Lowell MA USA Computer Science", "name": "Cindy Chen", "email": "Chen"}]}, "pods047": {"session": "Query Languages", "abstract": "The extensional aspect of expressive power---i.e., what queries can or cannot be expressed---has been the subject of many studies of query languages. Paradoxically, although efficiency is of primary concern in computer science, the intensional aspect of expressive power---i.e., what queries can or cannot be implemented efficiently---has been much neglected.  Here, we discuss the intensional expressive power of SQLP, a nested relational calculus augmented with aggregate functions and a powerset operation. We show that queries on structures such as long chains, deep trees, etc. have a dichotomous behaviour: Either they are already expressible in the calculus without using the powerset operation or they require at least exponential space. This result generalizes in three significant ways several old dichotomy-like results, such as that of Suciu and Paredaens that the complex object algebra of Abiteboul and Beeri needs exponential space to implement the transitive closure of a long chain.  Firstly, a more expressive query language---in particular, one that captures SQL---is considered here.  Secondly, queries on a more general class of structures than a long chain are considered here. \n\nLastly, our proof is more general and holds for all query languages exhibiting a certain normal form and possessing a locality property.", "title": "A Dichotomy in the Intensional Expressive Power of Nested Relational Calculi augmented with Aggregate Functions and a Powerset Operator", "authors": [{"affiliation": "National University of Singapore", "location": "Singapore  Singapore School of Computing", "name": "Limsoon Wong", "email": "Wong"}]}, "pods044": {"session": "Data Mining/Information Retrieval", "abstract": "To help a user specify and verify quantified queries --- a class of database queries known to be very \n\nchallenging for all but the most expert users --- one can question the user on whether certain data objects are\n\nanswers or non-answers to her intended query. In this paper, we analyze the number of questions \n\nneeded to learn or verify qhorn queries, a special class of Boolean quantified queries whose underlying form is conjunctions of quantified Horn expressions. We provide optimal polynomial-question and polynomial-time learning and verification algorithms for two subclasses of the class qhorn with upper constant limits on a query's causal density.", "title": "Learning and Verifying Quantified Boolean Queries by Example", "authors": [{"affiliation": "Yale University", "location": "New Haven CT USA Computer Science", "name": "Azza Abouzied", "email": "Abouzied"}, {"affiliation": "Yale University", "location": "New Haven CT USA Computer Science", "name": "Dana Angluin", "email": "Angluin"}, {"affiliation": "University of California, Berkeley", "location": "Berkeley CA USA Computer Science", "name": "Christos Papadimitriou", "email": "Papadimitriou"}, {"affiliation": "University of California, Berkeley", "location": "Berkeley CA USA Computer Science", "name": "Joseph Hellerstein", "email": "Hellerstein"}, {"affiliation": "Yale University", "location": "New Haven CT USA Computer Science", "name": "Avi Silberschatz", "email": "Silberschatz"}]}, "sig670": {"session": "Schema Matching and Spatial Databases I", "abstract": "Moving object databases arise in numerous applications such as traffic monitoring, crowd tracking, and games.  They all require keeping track of objects that move and thus the database of objects must be constantly updated.  The cover fieldtree (more commonly known as the loose quadtree and the loose octree, depending on the dimension of the underlying space) is designed to overcome the drawback of spatial data structures that associate objects with their minimum enclosing quadtree (octree) cells which is that the size of these cells depends more on the position of the objects and less on their size.  In fact, the size of these cells may be as large as the entire space from which the objects are drawn.  The loose quadtree (octree) overcomes this drawback by expanding the size of the space that is spanned by each quadtree (octree) cell c of width w by a cell expansion factor p (p>0) so that the expanded cell is of width (1+p)*w and an object is associated with its minimum enclosing expanded quadtree (octree) cell.  It is shown that for an object o with minimum bounding hypercube box b of radius r (i.e., half the length of a side of the hypercube), the maximum possible width w of the minimum enclosing expanded quadtree cell c is just a function of r and p, and is independent of the position of o.  Normalizing w via division by 2r enables calculating the range of possible expanded quadtree cell sizes as a function of p.  For p >= 0.5 the range consists of just two values and usually just one value for p >= 1.  This makes updating very simple and fast as for p >= 0.5, there are at most two possible new cells associated with the moved object and thus the update can be done in O(1) time.  Experiments with random data showed that the update time to support motion in such an environment is minimized when p is infinitesimally less than 1, with as much as a one order of magnitude increase in the number of updates that can be handled vis-a-vis the p=0 case in a given unit of time.  Similar results for updates were obtained for an N-body simulation where improved query performance and scalability were also observed.\n\n\n\nFinally, in order amplify the paper, a video tiled \"Crates and Barrels\" was produced which is an N-body simulation of 14,000 objects.  The video is available from the following URL: http://www.youtube.com/watch?v=Sokq3FRGc0s.  An applet to illustrate the behavior of the loose quadtree was developed and is available from http://donar.umiacs.umd. edu/quadtree/rectangles/loosequad.html.\n\n", "title": "Indexing Methods for Moving Object Databases: Games and Other Applications", "authors": [{"affiliation": "University of Maryland", "location": "College Park CA USA Department of Computer Science", "name": "Hanan Samet", "email": "Samet"}, {"affiliation": "NEC Labs America", "location": "Cupertino CA USA ", "name": "jagan Sankaranarayanan", "email": "Sankaranarayanan"}, {"affiliation": "University of Maryland", "location": "College Park MD USA Department of Computer Science", "name": "Michael Auerbach", "email": "Auerbach"}]}, "pods048": {"session": "Graph and XML Querying", "abstract": "It is known that unions of acyclic conjunctive\n\nqueries (CQs) can be evaluated in linear time, as opposed to arbitrary\n\nCQs, for which the evaluation problem is NP-complete. It follows from \n\ntechniques in the area of constraint-satisfaction problems that \n\n\"semantically acyclic\" unions of CQs -- i.e., unions of CQs that are \n\nequivalent to a union of acyclic ones -- can be evaluated in polynomial \n\ntime, though testing membership in the class of semantically acyclic CQs \n\nis NP-complete. \n\n\n\nWe study here the fundamental notion of semantic acyclicity in the context\n\nof graph databases and unions of conjunctive regular path queries with \n\ninverse (UC2RPQs). It is known that unions of acyclic C2RPQs can be \n\nevaluated efficiently, but it is by no means obvious whether the same \n\nholds for the class of UC2RPQs that are semantically acyclic. We prove that\n\nchecking whether a UC2RPQ is semantically acyclic is decidable in\n\n2EXPSPACE, and that it is EXPSPACE-hard even in the absence of\n\ninverses. Furthermore, we show that evaluation of semantically acyclic \n\nUC2RPQs is fixed-parameter tractable.  In addition, our tools yield a \n\nstrong theory of approximations for UC2RPQs when no equivalent acyclic \n\nUC2RPQ exists. ", "title": "Semantic Acyclicity on Graph Databases", "authors": [{"affiliation": "University of Chile", "location": "Santiago  Chile ", "name": "Pablo Barcel\u00f3 Baeza", "email": "Barcel\u00f3 Baeza"}, {"affiliation": "University of Chile", "location": "Santiago  Chile ", "name": "Miguel Romero", "email": "Romero"}, {"affiliation": "Rice University", "location": "Houston TX USA ", "name": "Moshe Vardi", "email": "Vardi"}]}, "sig635": {"session": "XML", "abstract": "Given a tree $Q$ and a large set of trees ${\\mathcal T} = \\{T_1,\\ldots,T_n\\}$, the {\\em subtree similarity-search problem\\/}  is that of finding the subtrees of trees among\n\n${\\mathcal T}$ that are most similar to $Q$, using the tree edit distance metric. Determining similarity using tree edit distance has been proven useful in a variety of application areas. While subtree similarity-search has been studied in the past, solutions required traversal of all of $\\T$, which poses a severe bottleneck in processing time, as $\\T$ grows larger. This paper proposes the first index structure for\n\nsubtree similarity-search, provided that the unit cost function is used. Extensive experimentation and comparison to previous work shows the huge improvement gained when using the proposed index structure and processing algorithm. ", "title": "Indexing for Subtree Similarity-Search using Edit Distance", "authors": [{"affiliation": "The Hebrew University of Jerusalem", "location": "Jerusalem  Israel ", "name": "Sara Cohen", "email": "Cohen"}]}, "sig652": {"session": "Systems, Performance II", "abstract": "The fact that multi-core CPUs have become so common and that the number of CPU cores in one chip has continued to rise means that a server machine can easily contain an extremely high number of CPU cores. The CPU scalability of IT systems is thus attracting a considerable amount of research attention. Some systems, such as ACID-compliant DBMSs, are said to be difficult to scale, probably due to the mutual exclusion required to ensure data consistency. Possible countermeasures include latch-free (LF) data structures, an elemental technology to improve the CPU scalability by eliminating the need for mutual exclusion. This paper investigates these LF data structures with a particular focus on their applicability and effectiveness. Some existing LF data structures (such as LF hash tables) have been adapted to PostgreSQL, one of the most popular open-source DBMSs. The performance improvement was evaluated with a benchmark program simulating real-world transactions. Measurement results obtained from state-of-the-art 80-core machines demonstrated that the LF data structures were effective for performance improvement in a many-core situation in which DBT-1 throughput increased by about 2.5 times. Although the poor performance of the original DBMS was due to a severe latch-related bottleneck and can be improved by parameter tuning, it is of practical importance that LF data structures provided performance improvement without deep understanding of the target system behavior that is necessary for the parameter tuning.", "title": "Latch-Free Data Structures for DBMS", "authors": [{"affiliation": "NEC Corporation", "location": "Kawasaki  Japan Knowledge Discovery Research Laboratories", "name": "Takashi Horikawa", "email": "Horikawa"}]}, "sig650": {"session": "Road Networks and Trajectories", "abstract": "Bichromatic reverse nearest neighbor (BRNN) queries have been studied extensively in the literature of spatial databases. Given a set P of service-providers and a set O of customers, a BRNN query is to find which customers in O are \"interested\" in a given service-provider in P. Recently, it has been found that this kind of queries lacks the consideration of the capacities of service-providers and the demands of customers. In order to address this issue, some spatial matching problems have been proposed, which, however, cannot be used for some real-life applications like emergency facility allocation where the maximum matching cost (or distance) should be minimized.  In this paper, we propose a new problem called Spatial Matching for Minimizing Maximum matching distance (SPM-MM). Then, we design two algorithms for SPM-MM, Threshold-Adapt and Swap-Chain. Threshold-Adapt is simple and easy to understand but not scalable to large datasets due to its relatively high time/space complexity. Swap-Chain, which follows a fundamentally different idea from Threshold-Adapt, runs faster than Threshold-Adapt by orders of magnitude and uses significantly less memory.  We conducted extensive empirical studies which verified the efficiency and scalability of Swap-Chain.", "title": "On Optimal Worst-Case Matching", "authors": [{"affiliation": "The Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Cheng Long", "email": "Long"}, {"affiliation": "The Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Raymond Chi-Wing Wong", "email": "Wong"}, {"affiliation": "University of Illinois at Chicago", "location": "Chicago IL USA Department of Computer Science", "name": "Philip S. Yu", "email": "Yu"}, {"affiliation": "The Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Minhao Jiang", "email": "Jiang"}]}, "sig651": {"session": "Systems, Performance I", "abstract": "This paper focuses on running scans in a main memory data processing system at \"bare metal\" speed. Essentially, this means that the system must aim to process data at or near the speed of the processor (the fastest component in most system configurations). Scans are common in main memory data processing environments, and with the state-of-the-art techniques it still takes many cycles per input tuple to apply simple predicates on a single column of a table. In this paper, we propose a technique called BitWeaving that exploits the parallelism available at the bit level in modern processors. BitWeaving operates on multiple bits of data in a single cycle, processing bits from different columns in each cycle. Thus, bits from a batch of tuples are processed in each cycle, allowing BitWeaving to drop the cycles per column to below one in some case. BitWeaving comes in two flavors: BitWeaving/V which looks like a columnar organization but at the bit level, and BitWeaving/H which packs bits horizontally. In this paper we also develop the arithmetic framework that is needed to evaluate predicates using these BitWeaving organizations. Our experimental results show that both these methods produce significant performance benefits over the existing state-of-the-art methods, and in some cases produce over an order of magnitude in performance improvement.", "title": "BitWeaving: Fast Scans for Main Memory Data Processing", "authors": [{"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Yinan Li", "email": "Li"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Jignesh Patel", "email": "Patel"}]}, "sig657": {"session": "Transactions", "abstract": "In databases with a large buffer pool, a transaction may run in less time than it takes to log the transaction\u0092s commit record on stable storage. Such cases motivate a technique called early lock release: immediately after appending its commit record to the log buffer in memory, a transaction may release its locks. Thus, it cuts overall lock duration to a fraction and reduces lock contention accordingly.\n\nEarly lock release also has its problems. The initial mention of early lock release was incomplete, the first detailed description and implementation was incorrect with respect to read-only transactions, and the most recent design initially had errors and still does not cover unusual lock modes such as \u0091increment\u0092 locks. Thus, we set out to achieve the same goals as early lock release but with a different, simpler, and more robust approach.\n\nThe resulting technique, controlled lock violation, requires no new theory, applies to any lock mode, promises less implementation effort and slightly less run-time effort, and also optimizes distributed transactions, e.g., in systems that rely on multiple replicas for high availability and high reliability. In essence, controlled lock violation retains locks until the transaction is durable but permits other transactions to violate its locks while flushing its commit log record to stable storage.", "title": "Controlled Lock Violation", "authors": [{"affiliation": "HP Labs", "location": "Madison  USA ", "name": "Goetz Graefe", "email": "Graefe"}, {"affiliation": "HP Labs", "location": "Palo Alto  USA ", "name": "Mark Lillibridge", "email": "Lillibridge"}, {"affiliation": "HP Labs", "location": "Palo Alto  USA ", "name": "Harumi Kuno", "email": "Kuno"}, {"affiliation": "HP Labs", "location": "Palo Alto  USA ", "name": "Joseph Tucek", "email": "Tucek"}, {"affiliation": "HP Labs", "location": "Palo Alto  USA ", "name": "Alistair Veitch", "email": "Veitch"}]}, "sig342": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "As enterprises become more automated, real-time, and data-driven, they need to integrate new data sources and specialized processing engines. The traditional business intelligence architecture of Extract-Transform-Load (ETL) flows, followed by querying, reporting, and analytic operations, is being generalized to analytic data flows that utilize a variety of data types and operations. These complicated flows are difficult to design, implement and maintain since they span a variety of systems. Additionally, new design requirements may be imposed such as design for fault-tolerance, freshness, maintainability, sampling, etc. To reduce development time and maintenance costs, automation is needed. We present xPAD, our platform to manage analytic data flows. xPAD enables flow design. We show how these designs can be optimized, not just for performance, but for other objectives as well. xPAD is engine-agnostic. We show how it can generate executable code for a number of execution engines. It can also import existing flows from other engines and optimize those flows. In that way, it can transform a flow written for one engine into an optimized flow for a different engine. In our demonstration, we will also use various example flows to show optimization for different objectives and comparison of flow execution on different engines.", "title": "xPAD: A Platform for Analytic Data Flows", "authors": [{"affiliation": "HP Labs", "location": "Palo Alto CA USA ", "name": "Alkis Simitsis", "email": "Simitsis"}, {"affiliation": "HP Labs", "location": "Palo Alto CA USA ", "name": "Kevin Wilkinson", "email": "Wilkinson"}, {"affiliation": "Univercity Politechnica de Catalunya", "location": "Barcelona  Spain ", "name": "Petar Jovanovic", "email": "Jovanovic"}]}, "sig655": {"session": "Text Databases", "abstract": "There is a wide range of applications that require to query a large database of texts to search for similar strings or substrings. Traditional approximate substring matching requests a user to specify a similarity threshold. Without top-k approximate substring matching, users have to try repeatedly different maximum distance threshold values when the proper threshold is unknown in advance.\n\n\n\nIn our paper, we first propose the efficient algorithms for finding the top-k approximate substring matches with a given query string in a set of data strings. To reduce the number of expensive distance computations, the proposed algorithms utilize our novel filtering techniques which take advantages of q-grams and inverted q-gram indexes available. We conduct extensive experiments with real-life data sets. Our experimental results confirm the effectiveness and scalability of our proposed algorithms.", "title": "Efficient Top-k Algorithms for Approximate Substring Matching", "authors": [{"affiliation": "Seoul National University", "location": "Seoul  South Korea ", "name": "Younghoon Kim", "email": "Kim"}, {"affiliation": "Seoul National University", "location": "Seoul  South Korea ", "name": "Kyuseok Shim", "email": "Shim"}]}, "sig658": {"session": "Privacy", "abstract": "In statistical privacy, utility refers to two concepts: information preservation \u0096 how much statistical information is retained by a sanitizing algorithm, and usability \u0096 how (and with how much difficulty) does one extract this information to build statistical models, answer queries, etc. Some scenarios incentivize a separation between information preservation and usability, so that the data owner first chooses a sanitizing algorithm to maximize a measure of information preservation and, afterward, the data consumers process the sanitized output according to their needs [22, 46].\n\nWe analyze a variety of utility measures and show that the average (over possible outputs of the sanitizer) error of Bayesian decision makers forms the unique class of utility measures that satisfy three axioms related to information preservation. The axioms are agnostic to Bayesian concepts such as subjective probabilities and hence strengthen support for Bayesian views in privacy research. In particular, this result connects information preservation to aspects of usability \u0096 if the information preservation of a sanitizing algorithm should be measured as the average error of a Bayesian decision maker, shouldn\u0092t Bayesian decision theory be a good choice when it comes to using the sanitized outputs for various purposes? We put this idea to the test in the unattributed histogram problem where our decision- theoretic post-processing algorithm empirically outperforms previously proposed approaches.", "title": "Information Preservation in Statistical Privacy and Bayesian Estimation of Unattributed Histograms", "authors": [{"affiliation": "Penn State University", "location": "University Park PA USA Computer Science & Engineering", "name": "Bing-Rong Lin", "email": "Lin"}, {"affiliation": "Penn State University", "location": "University Park PA USA Computer Science & Engineering", "name": "Daniel Kifer", "email": "Kifer"}]}, "sig659": {"session": "Transactions", "abstract": "In the era of smartphones and mobile computing, many popular applications such as Facebook, twitter, Gmail, and even Angry birds game manage their data using SQLite. This is mainly due to the development productivity and solid transactional support. For transactional atomicity, however, SQLite relies on less sophisticated but costlier page-oriented journaling mechanisms. Hence, this is often cited as the main cause of tardy responses in mobile applications.\n\nFlash memory does not allow data to be updated in place, and the copy-on-write strategy is adopted by most flash storage devices. In this paper, we propose X-FTL, a transactional flash translation layer(FTL) for SQLite databases. By offloading the burden of guaranteeing the transactional atomicity from a host system to flash storage and by taking advantage of the copy-on-write strategy used in modern FTLs, X-FTL drastically improves the transactional throughput almost for free without resorting to costly journaling schemes. We have implemented X-FTL on an SSD development board called OpenSSD, and modified SQLite and ext4 file system minimally to make them compatible with the extended abstractions provided by X-FTL. We demonstrate the effectiveness of X-FTL using real and synthetic SQLite workloads for smartphone applications, TPC-C benchmark for OLTP databases, and FIO benchmark for file systems.", "title": "X-FTL: Transactional FTL for SQLite Databases", "authors": [{"affiliation": "Sungkyunkwan University", "location": "Suwon  South Korea College of Info. and Comm. Engr.", "name": "Woon-Hak Kang", "email": "Kang"}, {"affiliation": "Sungkyunkwan University", "location": "Suwon  South Korea College of Info. and Comm. Engr.", "name": "Sang-Won Lee", "email": "Lee"}, {"affiliation": "Seoul National University", "location": "Seoul  South Korea School of Computer Science and Engineering", "name": "Bongki Moon", "email": "Moon"}, {"affiliation": "Sungkyunkwan University", "location": "Suwon  South Korea College of Info. and Comm. Engr.", "name": "Gi-Hwan Oh", "email": "Oh"}, {"affiliation": "Sungkyunkwan University", "location": "Suwon  South Korea College of Info. and Comm. Engr.", "name": "Changwoo Min", "email": "Min"}]}, "sig349": {"session": "Demo 3: Database Optimization; Performance", "abstract": "As the data collected by enterprises grows in scale, there is a growing trend of performing data analytics on large datasets. Batch processing systems that can handle petabyte scale of data, such as Hadoop, have flourished and gained traction in the industry. As the results of batch analytics have been used to continuously improve front-facing user experience, there is a growing interest in pushing the processing latency down. This trend has fueled a resurgence in the development and usage of execution engines \n\nthat can process {\\em continuous queries}.\n\n\n\nAn important class of continuous queries is windowed aggregation queries. Such queries arise in a wide range of applications such as generating personalized content and results. Today, considerable manual effort goes into finding\n\nthe most suitable execution engine for these queries and on tuning query performance on these engines. An ecosystem composed of multiple execution engines may be needed in order to run the overall query workload efficiently given the diverse set of requirements that arise in practice.\n\n\n\n\\textit{Cyclops} is a continuous query processing platform that manages and orchestrates windowed aggregation queries in an ecosystem composed of multiple continuous query  execution engines. Cyclops employs a cost-based approach for picking the most suitable engine and plan for executing a given query. This demonstration first presents an interactive visualization of the rich execution plan space of windowed aggregation queries, which allows users to analyze and understand the differences among plans. The next part of the demonstration will drill down into the design of Cyclops. For a given query, we show the cost spectrum of query execution plans across three different execution engines---Esper, Storm, and Hadoop---as estimated by Cyclops.\n\n", "title": "Execution and Optimization of Continuous Queries with Cyclops", "authors": [{"affiliation": "Duke University", "location": "Durham NC USA ", "name": "Harold Lim", "email": "Lim"}, {"affiliation": "Duke University", "location": "Durham NC USA ", "name": "Shivnath Babu", "email": "Babu"}]}, "sig268": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "We present a novel cloud system based on DBMS technology,\n\nwhere data mining algorithms are offered as a service. A local DBMS connects to the cloud and the cloud system returns computed data mining models as small relational tables that are archived and which can be easily transferred, queried and integrated with the client database. Unlike other analytic systems, our solution is not based on MapReduce. Our system avoids exporting large tables outside the local DBMS and thus it avoids transmitting large volumes of data to the cloud. The system offers three processing modes: local, cloud and hybrid, where a linear cost model is used to choose processing mode. In hybrid mode processing is split between the local DBMS and the cloud DBMS. Our system has a job scheduler with FIFO, SJF and RR policies to enhance response time and get partial results early. The cloud DBMS\n\nperforms dynamic job scheduling, model computation and model archive management. Our system incorporates several optimizations: local data set summarization with sufficient statistics, sampling, caching matrices in RAM and selectively transmitting small matrices, back and forth. We show that in general the most efficient computing mechanism is hybrid processing: summarizing or sampling the data set in the local DBMS, transferring small matrices back and forth, leaving mathematically complex methods as a task for the cloud DBMS.", "title": "Data Mining Algorithms as a Service in the Cloud Exploiting Relational Database Systems", "authors": [{"affiliation": "University of Houston", "location": "Houston  USA ", "name": "Carlos Ordonez", "email": "Ordonez"}, {"affiliation": "Instituto Polit\u00e9cnico Nacional", "location": "Mexico TX Mexico ", "name": "Javier Garc\u00eda-Garc\u00eda", "email": "Garc\u00eda-Garc\u00eda"}, {"affiliation": "Greenplum/EMC", "location": "San Mateo CA USA ", "name": "Carlos Garcia-Alvarado", "email": "Garcia-Alvarado"}, {"affiliation": "University of Houston", "location": "Houston TX USA ", "name": "Wellington Cabrera", "email": "Cabrera"}, {"affiliation": "UT MD Anderson C.C.", "location": "Houston TX USA ", "name": "Veerabhadran Baladandayuthapani", "email": "Baladandayuthapani"}, {"affiliation": "University of Houston", "location": "Houston TX USA ", "name": "Mohammed Quraishi", "email": "Quraishi"}]}, "sig264": {"session": "Industrial 5: Big Data III & More", "abstract": "In an object-to-relational mapping system (ORM), mapping expressions explain how to expose relational data as objects and how to store objects in tables. If mappings are sufficiently expressive, then it is possible to define lossy mappings. If a user updates an object, stores it in the database based on a lossy mapping, and then retrieves the object from the database, the user might get a different result than the updated state of the object; that is, the mapping might not ``roundtrip.\" To avoid this, the ORM should validate that user-defined mappings roundtrip the data. However, this problem is NP-hard, so mapping validation can be very slow for large or complex mappings.\n\n\n\nWe circumvent this problem by developing an incremental compiler for OR mappings. Given a validated mapping, a modification to the object schema is compiled into incremental modifications of the mapping. We define the problem formally, present algorithms to solve it for Microsoft's Entity Framework, and report on an implementation. For some mappings, incremental compilation is over 100 times faster than a full mapping compilation, in one case dropping from 8 hours to 50 seconds.", "title": "Incremental Mapping Compilation in an Object-to-Relational Mapping System", "authors": [{"affiliation": "Microsoft Corporation", "location": "Redmond WA USA ", "name": "Philip Bernstein", "email": "Bernstein"}, {"affiliation": "University of Pennsylvania", "location": "Philadelphia PA USA ", "name": "Marie Jacob", "email": "Jacob"}, {"affiliation": "Universidad de Chile", "location": "Santiago  Chile Department of Computer Science", "name": "Jorge P\u00e9rez", "email": "P\u00e9rez"}, {"affiliation": "Universitat Polit\u00e8cnica de Catalunya", "location": "Barcelona  Spain ", "name": "Guillem Rull", "email": "Rull"}, {"affiliation": "Microsoft Corporation", "location": "Redmond WA USA ", "name": "James Terwilliger", "email": "Terwilliger"}]}, "sig495": {"session": "Systems, Performance III", "abstract": "This paper describes the SimSQL system, which allows for SQLbased\n\nspecification, simulation, and querying of database-valued\n\nMarkov chains, i.e., chains whose value at any time step comprises\n\nthe contents of an entire database. SimSQL extends the earlier\n\nMonte Carlo database system (MCDB), which permitted Monte\n\nCarlo simulation of static database-valued random variables. Like\n\nMCDB, SimSQL uses user-specified \u0093VG functions\u0094 to generate\n\nthe simulated data values that are the building blocks of a simulated\n\ndatabase. The enhanced functionality of SimSQL is enabled by the\n\nability to parametrize VG functions using stochastic tables, so that\n\none stochastic database can be used to parametrize the generation\n\nof another stochastic database, which can parametrize another, and\n\nso on. Other key extensions include the ability to explicitly define\n\nrecursive versions of a stochastic table and the ability to execute\n\nthe simulation in a MapReduce environment. We focus on applying\n\nSimSQL to Bayesian machine learning.", "title": "Simulation of Database-Valued Markov Chains Using SimSQL", "authors": [{"affiliation": "Rice University", "location": "Houston TX USA ", "name": "Zhuhua Cai", "email": "Cai"}, {"affiliation": "LogicBlox, Inc.", "location": "Atlanta GA USA ", "name": "Zografoula Vagena", "email": "Vagena"}, {"affiliation": "Rice University", "location": "Houston TX USA ", "name": "Luis Perez", "email": "Perez"}, {"affiliation": "Rice University", "location": "Houston TX USA ", "name": "Subramanian Arumugam", "email": "Arumugam"}, {"affiliation": "IBM Almaden", "location": "San Jose CA USA ", "name": "Peter Haas", "email": "Haas"}, {"affiliation": "Rice University", "location": "Houston TX USA ", "name": "Christopher Jermaine", "email": "Jermaine"}]}, "sig260": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "Exploratory analysis on big data requires us to rethink data management across the entire stack \u0096 from the underlying data processing techniques to the user experience. We demonstrate Stat! \u0096 a visualization and analytics environment that allows users to rapidly experiment with exploratory queries over big data. Data scientists can use Stat! to quickly refine to the correct query, while getting immediate feedback after processing a fraction of the data. Stat! can work with multiple processing engines in the backend; in this demo, we use Stat! with the Microsoft StreamInsight streaming engine. StreamInsight is used to generate incremental early results to queries and refine these results as more data is processed. Stat! allows data scientists to explore data, dynamically compose multiple queries to generate streams of partial results, and display partial results in both textual and visual form.", "title": "Stat! - An Interactive Analytics Environment for Big Data", "authors": [{"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Mike Barnett", "email": "Barnett"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Badrish Chandramouli", "email": "Chandramouli"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Robert DeLine", "email": "DeLine"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Steven Drucker", "email": "Drucker"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Danyel Fisher", "email": "Fisher"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Jonathan Goldstein", "email": "Goldstein"}, {"affiliation": "North Carolina State University", "location": "Raleigh NC USA ", "name": "Patrick Morrison", "email": "Morrison"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "John Platt", "email": "Platt"}]}, "sig411": {"session": "PTAbstract", "abstract": "Our field is drowning in a sea of conference submissions.  We assert that the sheer number of papers has begun to seriously hurt the quality of the work that the field is doing and that the field is going to implode unless we take action to remedy the situation. In order to improve the quality of the papers being published we must reduce the number being submitted. This will require a change in the culture of our field where \u0093more\u0094 is being equated to \u0093better\u0094 by both hiring and promotion committees. In this panel we will explore some ideas for correcting the situation.    ", "title": "We are Drowning in a Sea of Least Publishable Units (LPUs)", "authors": [{"affiliation": "Microsoft Corporation", "location": "Madison WI USA ", "name": "David DeWitt", "email": "DeWitt"}, {"affiliation": "QCRI", "location": "Qatar  Qatar ", "name": "Ihab Ilyas", "email": "Ilyas"}, {"affiliation": "University of Wisconsin", "location": "Madison WI USA Computer Sciences Department", "name": "Jeffrey Naughton", "email": "Naughton"}, {"affiliation": "MIT", "location": "Cambridge MA USA CSAIL", "name": "Michael Stonebraker", "email": "Stonebraker"}]}, "sig551": {"session": "Graph Connectivity", "abstract": "Reachability querying is a basic graph operation with numerous important applications in databases, network analysis, computational biology, software engineering, etc. Although  many indexes have been proposed to answer reachability queries, most of them are only efficient for handling relatively small graphs. We propose TF-label, an efficient and scalable labeling scheme for processing reachability queries. TF-label is constructed based on a novel topological folding (TF) that recursively folds an input graph into half so as to reduce the label size, thus improving query efficiency. We show that TF-label is efficient to construct and propose efficient algorithms and optimization schemes. Our experiments verify that TF-label is significantly more scalable and efficient than the state-of-the-art methods in both index construction and query processing.\n\n", "title": "TF-Label: a Topological-Folding Labeling Scheme for Reachability Querying in a Large Graph", "authors": [{"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "James Cheng", "email": "Cheng"}, {"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Silu Huang", "email": "Huang"}, {"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Huanhuan Wu", "email": "Wu"}, {"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Ada Fu", "email": "Fu"}]}, "sig222": {"session": "Spatial Databases II", "abstract": "Recently, spatial keyword queries become a hot topic in the literature. One example of these queries is the collective spatial keyword query (CoSKQ) which is to find a set of objects in the database such that it covers a set of given keywords collectively and has the smallest cost. Unfortunately, existing exact algorithms have severe scalability problems and existing approximate algorithms, though scalable, cannot guarantee near-to-optimal solutions. In this paper, we study the CoSKQ problem and address the above issues.\n\n\n\nFirstly, we consider the CoSKQ problem using an existing cost measurement called the maximum sum cost. This problem is called MaxSum-CoSKQ and is known to be NP-hard. We observe that the maximum sum cost of a set of objects is dominated by at most three objects which we call the distance owners of the set. Motivated by this, we propose a distance owner-driven approach which involves two algorithms: one is an exact algorithm which runs faster than the best-known existing algorithm by several orders of magnitude and the other is an approximate algorithm which improves the best-known constant approximation factor from 2 to 1.375. \n\n\n\nSecondly, we propose a new cost measurement called diameter cost and CoSKQ with this measurement is called Dia-CoSKQ. We prove that Dia-CoSKQ is NP-hard. With the same distance owner-driven approach, we design two algorithms for Dia-CoSKQ: one is an exact algorithm which is efficient and scalable and the other is an approximate algorithm  which gives a sqrt{3}-factor approximation. \n\n\n\nWe conducted extensive experiments on real datasets which verified that the proposed exact algorithms are scalable and the proposed approximate algorithms return near-to-optimal solutions.", "title": "Collective Spatial Keyword Queries: A Distance Owner-Driven Approach", "authors": [{"affiliation": "The Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Cheng Long", "email": "Long"}, {"affiliation": "The Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Raymond Chi-Wing Wong", "email": "Wong"}, {"affiliation": "Simon Fraser University", "location": "Burnaby BC Canada Department of Computing Science", "name": "Ke Wang", "email": "Wang"}, {"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Ada Wai-Chee Fu", "email": "Fu"}]}, "sig555": {"session": "Spatial Databases II", "abstract": "The rise of GPS-equipped mobile devices has led to the emergence of big trajectory data. In this paper, we study a new path finding query which finds the most frequent path (MFP) during user-specified time periods in large-scale historical trajectory data. We refer to this query as time period-based MFP (TPMFP). Specifically, given a time period T, a source v_s and a destination v_d, TPMFP searches the MFP from v_s to v_d during T. Though there exist several proposals on defining MFP, they only consider a fixed time period. Most importantly, we find that none of them can well reflect people\u0092s common sense notion which can be described by three key properties, namely suffix-optimal (i.e., any suffix of an MFP is also an MFP), length-insensitive (i.e., MFP should not favor shorter or longer paths), and bottleneck-free (i.e., MFP should not contain infrequent edges). The TPMFP with the above properties will reveal not only common routing preferences of the past travelers, but also take the time effectiveness into consideration. Therefore, our first task is to give a TPMFP definition that satisfies the above three properties. Then, given the comprehensive TPMFP definition, our next task is to find TPMFP over huge amount of trajectory data efficiently. Particularly, we propose efficient search algorithms together with novel indexes to speed up the processing of TPMFP. To demonstrate both the effectiveness and the efficiency of our approach, we conduct extensive experiments using a real dataset containing over 11 million trajectories.", "title": "Finding Time Period-Based Most Frequent Path in Big Trajectory Data", "authors": [{"affiliation": "Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong ", "name": "Wuman Luo", "email": "Luo"}, {"affiliation": "Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong ", "name": "Haoyu Tan", "email": "Tan"}, {"affiliation": "Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong ", "name": "Lei Chen", "email": "Chen"}, {"affiliation": "Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong ", "name": "Lionel M. Ni", "email": "Ni"}]}, "pods050": {"session": "Query processing/Verification", "abstract": "Data-centric dynamic systems are systems where both the process controlling the dynamics and the manipulation of data are equally central. We study verification of (first-order) mu-calculus variants over relational data-centric dynamic systems, where data are maintained in a relational database, and the process is described in terms of atomic actions that evolve the database. Action execution may involve calls to external services, thus inserting fresh data into the system. As a result such systems are infinite-state. We show that verification is undecidable in general, and we isolate notable cases where decidability is achieved. Specifically we start by considering service calls that return values deterministically (depending only on passed parameters). We show that in a mu-calculus variant that preserves knowledge of objects appeared along a run we get decidability under the assumption that the fresh data introduced along a run are bounded, though they might not be bounded in the overall system. In fact we tie such a result to a notion related to weak acyclicity studied in data exchange. Then, we move to nondeterministic services and we investigate decidability under the assumption that knowledge of objects is preserved only if they are continuously present. We show that if infinitely many values occur in a run but do not accumulate in the same state, then we get again decidability. We give syntactic conditions to avoid this accumulation through the novel notion of \"generate-recall acyclicity\", which ensures that every service call activation generates new values that cannot be accumulated indefinitely.", "title": "Verification of Relational Data-Centric Dynamic Systems with External Services", "authors": [{"affiliation": "Free University of Bozen-Bolzano", "location": "Bolzano  Italy KRDB Research Centre for Knowledge and Data", "name": "Babak Bagheri Hariri", "email": "Bagheri Hariri"}, {"affiliation": "Free University of Bozen-Bolzano", "location": "Bolzano  Italy KRDB Research Centre for Knowledge and Data", "name": "Diego Calvanese", "email": "Calvanese"}, {"affiliation": "Sapienza Universit\u00e0 di Roma", "location": "Rome  Italy ", "name": "Giuseppe De Giacomo", "email": "De Giacomo"}, {"affiliation": "University of California San Diego", "location": "La Jolla CA USA ", "name": "Alin Deutsch", "email": "Deutsch"}, {"affiliation": "Free University of Bozen-Bolzano", "location": "Bolzano  Italy KRDB Research Centre for Knowledge and Data", "name": "Marco Montali", "email": "Montali"}]}, "pods051": {"session": "Graph and XML Querying", "abstract": "Regular path queries (RPQs) select vertices connected by\n\nsome path in a graph. The edge labels of such a path have\n\nto form a word that matches a given regular expression. We\n\ninvestigate the evaluation of RPQs with an additional con-\n\nstraint that prevents multiple traversals of the same vertices.\n\nThose regular simple path queries (RSPQs) quickly become\n\nintractable, even for basic languages such as (aa)? or a? ba? .\n\nIn this paper, we establish a comprehensive classification\n\nof regular languages with respect to the complexity of the\n\ncorresponding regular simple path query problem. More\n\nprecisely, we identify for which languages RSPQs can be\n\nevaluated in polynomial time, and show that evaluation is\n\nNP-complete for languages outside this fragment. We thus\n\nfully characterize the frontier between tractability and in-\n\ntractability for RSPQs, and we refine our results to show the\n\nfollowing trichotomy: evaluation of RSPQs is either AC0 ,\n\nNL-complete or NP-complete in data complexity, depend-\n\ning on the language L. The fragment identified also admits\n\na simple characterization in terms of regular expressions.\n\nFinally, we also discuss the complexity of deciding whether\n\na language L belongs to the fragment above. We consider\n\nseveral alternative representations of L: DFAs, NFAs or\n\nregular expressions, and prove that this problem is NL-\n\ncomplete for the first representation and PSPACE-complete\n\nfor the other two. As a conclusion we extend our results from\n\nedge-labeled graphs to vertex-labeled graphs.", "title": "A Trichotomy for Regular Simple Path Queries on Graphs", "authors": [{"affiliation": "Inria", "location": "Lille  France ", "name": "Guillaume Bagan", "email": "Bagan"}, {"affiliation": "Lille 1 Univerity & Inria", "location": "Lille  France ", "name": "Angela Bonifati", "email": "Bonifati"}, {"affiliation": "Tel-Aviv University", "location": "Tel-Aviv  Israel ", "name": "Benoit Groz", "email": "Groz"}]}, "sig626": {"session": "Crowdsourcing", "abstract": "Crowdsourcing has created a variety of opportunities for many challenging problems by leveraging human intelligence.  \n\nFor example, applications such as image tagging, natural language processing, and semantic-based information retrieval \n\ncan exploit crowd-based human computation to supplement existing computational algorithms.  Naturally, human workers \n\nin crowdsourcing solve problems based on their knowledge, experience, and perception.  It is therefore not clear which \n\nproblems can be better solved by crowdsourcing than solving solely using traditional machine-based methods. Therefore, \n\na cost sensitive quantitative analysis method is needed.\n\n\n\nIn this paper, we design and implement a cost sensitive method for crowdsourcing. We online estimate the profit of the \n\ncrowdsourcing job so that those questions with no future profit from crowdsourcing can be terminated. Two models are \n\nproposed to estimate the profit of crowdsourcing job, namely the linear value model and the generalized non-linear model.\n\nUsing these models, the expected profit of obtaining new answers for a specific question is computed based on the answers\n\nalready received.  A question is terminated in real time if the marginal expected profit of obtaining more answers is not \n\npositive. We extends the method to publish a batch of questions in a HIT. We evaluate the effectiveness of our proposed \n\nmethod using two real world jobs on AMT.  The experimental results show that our proposed method outperforms all the \n\nstate-of-art methods.", "title": "An Online Cost Sensitive Decision-Making Method in Crowdsourcing Systems", "authors": [{"affiliation": "National University of Singapore", "location": "Singapore  Singapore School of Computing", "name": "Jinyang Gao", "email": "Gao"}, {"affiliation": "National University of Singapore", "location": "Singapore  Singapore School of Computing", "name": "Xuan Liu", "email": "Liu"}, {"affiliation": "National University of Singapore", "location": "Singapore  Singapore School of Computing", "name": "Beng Chin Ooi", "email": "Ooi"}, {"affiliation": "Microsoft Research Asia", "location": "Beijing  China ", "name": "Haixun Wang", "email": "Wang"}, {"affiliation": "Zhejiang University", "location": "Hangzhou  China College of Computer Science", "name": "Gang Chen", "email": "Chen"}]}, "pods054": {"session": "Data Mining/Information Retrieval", "abstract": "A frequent subgraph of a given collection of graphs is a graph that is isomorphic to a subgraph of at least as many graphs in the collection as a given threshold.  Frequent subgraphs generalize frequent itemsets and arise in various contexts, from bioinformatics to the Web.  Since the space of frequent subgraphs is typically extremely large, research in graph mining has focused on special types of frequent subgraphs that can be orders of magnitude smaller in number, yet encapsulate the space of all frequent subgraphs. Maximal frequent subgraphs (i.e., the ones not properly contained in any frequent subgraph) constitute the most useful such type.\n\n\n\nIn this paper, we embark on a comprehensive investigation of the computational complexity of mining maximal frequent subgraphs.  Our study is carried out by considering the effect of three different parameters: possible restrictions on the class of graphs; a fixed bound on the threshold; and a fixed bound on the number of desired answers.  We focus on specific classes of connected graphs: general graphs, planar graphs, graphs of bounded degree, and graphs of bounded tree-width (trees being a special case). Moreover, each class has two variants: the one in which the nodes are unlabeled, and the one in which they are uniquely labeled.  We delineate the complexity of the enumeration problem for each of these variants by determining when it is solvable in (total or incremental) polynomial time and when it is NP-hard.  Specifically, for the labeled classes, we show that bounding the threshold yields tractability but, in most cases, bounding the number of answers does not, unless P=NP; an exception is the case of labeled trees, where bounding either of these two parameters yields tractability.  The state of affairs turns out to be quite different for the unlabeled classes.  The main (and most challenging to prove) result concerns unlabeled trees: we show NP-hardness, even if the input consists of two trees, and both the threshold and the number of desired answers are equal to just two. In other words, we establish that the following problem is NP-complete: given two unlabeled trees, do they have more than one maximal subtree in common?", "title": "The Complexity of Mining Maximal Frequent Subgraphs", "authors": [{"affiliation": "IBM Research - Almaden", "location": "San Jose CA USA ", "name": "Benny Kimelfeld", "email": "Kimelfeld"}, {"affiliation": "UC Santa Cruz and IBM Research - Almaden", "location": "Santa Cruz CA USA ", "name": "Phokion Kolaitis", "email": "Kolaitis"}]}, "pods055": {"session": "PODS Tutorial 1", "abstract": "We introduce and study a model of collaborative data-driven workflows. In a local-as-view style, each peer has a partial view of a global instance that remains purely virtual. Local updates have side effects on other peers' data, defined via the global instance. We also assume that the peers provide (an abstraction of) their specifications, so that each peer can actually see and reason on the specification of the entire system.\n\n\n\nWe study the ability of a peer to carry out runtime reasoning about the global run of the system, and in particular about actions of other peers, based on its own local observations.  A main contribution is to show that, under a reasonable restriction (namely, \"key-visibility\"), one can construct a finite symbolic representation of the infinite set of global runs consistent with given local observations.  Using the symbolic representation, we show\n\nthat we can evaluate in PSPACE a large class of properties over global runs, expressed in an extension of first-order logic with past linear-time temporal operators, PLTL-FO.  We also provide a variant of the algorithm allowing to incrementally monitor a statically defined property, and then develop an extension allowing to monitor an infinite class of properties sharing the same temporal structure, defined dynamically as the run unfolds. Finally, we consider an extension of the language, that permits workflow control with PLTL-FO formulas.  We prove that this does not increase the power of the workflow specification language, thereby showing that the language is closed under such introspective reasoning.\n\n", "title": "Collaborative Data-Driven Workflows: Think Global,  Act Local", "authors": [{"affiliation": "INRIA Saclay and ENS Cachan", "location": "Cachan  France ", "name": "Serge Abiteboul", "email": "Abiteboul"}, {"affiliation": "UC San Diego and INRIA Saclay", "location": "La Jolla CA USA ", "name": "Victor Vianu", "email": "Vianu"}]}, "pods057": {"session": "RDF and Ontologies", "abstract": "Ontology-based data access is concerned with querying incomplete data\n\nsources in the presence of domain-specific knowledge provided by an\n\nontology.  A central notion in this setting is that of an\n\nontology-mediated query, which is a database query coupled with an\n\nontology.  In this paper, we study several classes of\n\nontology-mediated queries, where the database queries are given as\n\nsome form of conjunctive query and the ontologies are formulated in\n\ndescription logics or other relevant fragments of first-order logic,\n\nsuch as the guarded fragment and the unary-negation fragment.  The\n\ncontributions of the paper are three-fold. First, we characterize the\n\nexpressive power of ontology-mediated queries in terms of fragments of\n\ndisjunctive datalog.  Second, we establish intimate connections\n\nbetween ontology-mediated queries and constraint satisfaction problems\n\n(CSPs) and their logical generalization, MMSNP formulas. Third, we\n\nexploit these connections to obtain new results regarding (i)\n\nfirst-order rewritability and datalog-rewritability of\n\nontology-mediated queries, (ii) P/NP dichotomies for ontology-mediated\n\nqueries, and (iii) the query containment problem for ontology-mediated\n\nqueries.", "title": "Ontology-based Data Access: A Study through Disjunctive Datalog, CSP, and MMSNP", "authors": [{"affiliation": "CNRS and Universite Paris Sud", "location": "Paris  France ", "name": "Meghyn Bienvenu", "email": "Bienvenu"}, {"affiliation": "University of California at Santa Cruz", "location": "Santa Cruz CA USA ", "name": "Balder ten Cate", "email": "ten Cate"}, {"affiliation": "University of Bremen", "location": "Bremen  Germany ", "name": "Carsten Lutz", "email": "Lutz"}, {"affiliation": "University of Liverpool", "location": "Liverpool  United Kingdom ", "name": "Frank Wolter", "email": "Wolter"}]}, "pods058": {"session": "Query Languages", "abstract": "We consider the problem of computing a relational query $q$ on a large input database of size $n$, using a large number $p$ of servers. The computation is performed in {\\em rounds}, and each server can receive only $O(n/p^{1-\\varepsilon})$ bits of data, where $\\varepsilon \\in [0,1]$ is a parameter that controls replication. We examine how many global communication steps are needed to compute $q$. We establish both lower and upper bounds, in two settings. \n\nFor a single round of communication, we give lower bounds in the strongest possible model, where arbitrary bits may be exchanged; we show that any algorithm requires $\\varepsilon \\geq 1-1/\\tau^*$,\n\nwhere $\\tau^*$ is the fractional vertex cover of the hypergraph of $q$. We also give an algorithm that\n\nmatches the lower bound for a specific class of databases.\n\nFor multiple rounds of communication, we present lower bounds in a model where routing decisions for a tuple are tuple-based. We show that for the class of {\\em tree-like} queries there exists a tradeoff between the number of rounds and the space exponent $\\varepsilon$.\n\nThe lower bounds for multiple rounds are the first of their kind. Our results also imply that transitive\n\n closure cannot be computed in $O(1)$ rounds of communication.", "title": "Communication Steps for Parallel Query Processing", "authors": [{"affiliation": "University of Washington", "location": "Seattle  USA ", "name": "Paul Beame", "email": "Beame"}, {"affiliation": "University of Washington", "location": "Seattle  USA ", "name": "Paraschos Koutris", "email": "Koutris"}, {"affiliation": "University of Washington", "location": "Seattle  USA ", "name": "Dan Suciu", "email": "Suciu"}]}, "sig298": {"session": "Data Cleaning", "abstract": "Various computational procedures or constraint-based methods for data repairing have been proposed over the last decades to identify errors and, when possible, correct them.\n\nHowever, these approaches have several limitations including the scalability and quality of the values to be used in replacement of the errors.\n\nIn this paper, we propose a new data repairing approach that is based on maximizing the likelihood of replacement data\n\ngiven the data distribution, which can be modeled using statistical machine learning techniques.\n\nThis is a novel approach combining machine learning and likelihood methods for cleaning dirty databases by value modification.\n\nWe develop a quality measure of the repairing updates based on the likelihood benefit and the amount of changes applied to the database.\n\nWe propose SCARE (SCalable Automatic REpairing), a systematic scalable framework\n\nthat follows our approach.\n\nSCARE relies on a robust mechanism for horizontal data partitioning and\n\na combination of machine learning techniques to predict the set of possible updates.\n\nDue to data partitioning, several updates can be predicted for a single record\n\nbased on local views on each data partition.\n\nTherefore, we propose a mechanism to combine the local predictions and obtain accurate final predictions.\n\nFinally, we experimentally\n\ndemonstrate the effectiveness, efficiency, and scalability of our approach on real-world datasets\n\nin comparison to recent data cleaning approaches.\n\n", "title": "Don't be SCAREd: Use SCalable Automatic REpairing with Maximal Likelihood and Bounded Changes", "authors": [{"affiliation": "Microsoft Corp.", "location": "Bellevue WA USA ", "name": "Mohamed Yakout", "email": "Yakout"}, {"affiliation": "Institut de Recherche pour le D\u00e9veloppement", "location": "Aix-en-Provence  France ", "name": "Laure Berti-\u00c9quille", "email": "Berti-\u00c9quille"}, {"affiliation": "Qatar Computing Research Institute", "location": "Doha  Qatar ", "name": "Ahmed Elmagarmid", "email": "Elmagarmid"}]}, "sig353": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "A database contains rich, inter-related, multi-typed data and information, forming one or a set of gigantic, intercon- nected, heterogeneous information networks. Much knowl- edge can be derived from such information networks if we systematically develop an effective and scalable database-oriented information network analysis technology. In this system demo, we take a computer science research publica- tion network as an example, which is an information net- work derived from an integration of DBLP, other web-based information about researchers, and partially available cita- tion data, and construct a Research-Insight system in order to demonstrate the power of database-oriented information network analysis. We show that nontrivial research insight can be obtained from such analysis, including (1) ranking, clustering, classification and similarity search of researchers, terms and venues for research subfields and themes, (2) recommending good researchers and good research papers to read or cite when conducting research on certain topics (3) predicting potential collaborators for certain theme-oriented research, and (4) predicting advisor-advisee rela- tionships and affiliation history based on historical research publications. Although some of these functions have been studied in recent research, effective and scalable realization of such functions in large networks still poses challenging research problems. Moreover, some function are our on- going research tasks. By integrating these functionalities, Research-Insight may not only provide with us insightful rec- ommendations in CS research but also help us gain insight on how to perform effective data mining in large databases.", "title": "Research-Insight: Providing Insight on Research by Publication Network Analysis", "authors": [{"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Fangbo Tao", "email": "Tao"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Xiao Yu", "email": "Yu"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Kin Hou Lei", "email": "Lei"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "George Brova", "email": "Brova"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Xiao Cheng", "email": "Cheng"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Jiawei Han", "email": "Han"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Rucha Kanade", "email": "Kanade"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Yizhou Sun", "email": "Sun"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Chi Wang", "email": "Wang"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Lidan Wang", "email": "Wang"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Champaign IL USA Department of Computer Science", "name": "Tim Weninger", "email": "Weninger"}]}, "sig640": {"session": "Data Storage", "abstract": "Efficient storage and querying of RDF data is of increasing importance, due to\n\nthe increased popularity and widespread acceptance of RDF on the web and in the\n\nenterprise. In this paper, we describe a novel storage and query mechanism for\n\nRDF which works on top of existing relational representations.  Reliance on\n\nrelational representations of RDF means that one can take advantage of 35+\n\nyears of research on efficient storage and querying, industrial-strength\n\ntransaction support, locking, security, etc.  However, there are significant\n\nchallenges in storing RDF in relational, which include data sparsity and schema\n\nvariability.  We describe novel mechanisms to shred RDF into relational, and\n\nnovel query translation techniques to maximize the advantages of this shredded\n\nrepresentation.  We show that these mechanisms result in consistently good\n\nperformance across multiple RDF benchmarks, even when compared with current\n\nstate-of-the-art stores. This work provides the basis for RDF support in DB2\n\nv.10.1.\n\n", "title": "Building an Efficient RDF Store Over a Relational Database", "authors": [{"affiliation": "IBM Research", "location": "Yorktown Heights NY USA ", "name": "Mihaela Bornea", "email": "Bornea"}, {"affiliation": "IBM Research", "location": "Yorktown Heights NY USA ", "name": "Julian Dolby", "email": "Dolby"}, {"affiliation": "IBM Research", "location": "Yorktown Heights NY USA ", "name": "Anastasios Kementsietsidis", "email": "Kementsietsidis"}, {"affiliation": "IBM Research", "location": "Yorktown Heights NY USA ", "name": "Kavitha Srinivas", "email": "Srinivas"}, {"affiliation": "IBM Software Group", "location": "HURSLEY  United Kingdom ", "name": "Patrick Dantressangle", "email": "Dantressangle"}, {"affiliation": "IBM Research", "location": "Yorktown Heights NY USA ", "name": "Octavian Udrea", "email": "Udrea"}, {"affiliation": "IBM Research", "location": "Yorktown Heights NY USA ", "name": "Bishwaranjan Bhattacharjee", "email": "Bhattacharjee"}]}, "sig643": {"session": "Cloud computing", "abstract": "MapReduce has become a dominant parallel computing paradigm for {\\em big data}, i.e., colossal datasets at the scale of tera-bytes or higher. Ideally, a MapReduce system should achieve a high degree of load balancing among the participating machines, and minimize the space usage, CPU and I/O time, and network transfer at each machine. Although these principles have guided the development of MapReduce algorithms, limited emphasis has been placed on enforcing serious constraints on the aforementioned metrics simultaneously. This paper presents the notion of {\\em minimal algorithm}, that is, an algorithm that guarantees the best parallelization in multiple aspects at the same time, up to  a small constant factor. We show the existence of elegant minimal algorithms for a set of fundamental database problems, and demonstrate their excellent performance with extensive experiments.", "title": "Minimal MapReduce Algorithms", "authors": [{"affiliation": "Chinese University of Hong Kong", "location": "Hong Kong  Hong Kong ", "name": "Yufei Tao", "email": "Tao"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore ", "name": "Wenqing Lin", "email": "Lin"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore ", "name": "Xiaokui Xiao", "email": "Xiao"}]}, "sig644": {"session": "Social Media", "abstract": "Analyzing sentiments of demographic groups is becoming important for the Social Web, where millions of users provide opinions on a wide variety of content.  While several approaches exist for mining sentiments from product reviews or micro-blogs, little attention has been devoted to aggregating and comparing extracted sentiments for different demographic groups over time, such as `Students in Italy' or `Teenagers in Europe'. This problem demands efficient and scalable methods for sentiment aggregation and correlation, which account for the evolution of sentiment values, sentiment bias, and other factors associated with the special characteristics of web data. We propose a scalable approach for sentiment indexing and aggregation that works on multiple time granularities and uses incrementally updateable data structures for online operation. Furthermore, we describe efficient methods for computing meaningful sentiment correlations, which exploit pruning based on demographics and use top-k correlations compression techniques. We present an extensive experimental evaluation with both synthetic and real datasets, demonstrating the effectiveness of our pruning techniques and the efficiency of our solution.", "title": "Efficient Sentiment Correlation for Large-scale Demographics", "authors": [{"affiliation": "University of Trento", "location": "Trento  Italy ", "name": "Mikalai Tsytsarau", "email": "Tsytsarau"}, {"affiliation": "Laboratoire d'Informatique de Grenoble", "location": "Grenoble  France ", "name": "Sihem Amer-Yahia", "email": "Amer-Yahia"}, {"affiliation": "University of Trento", "location": "Trento  Italy ", "name": "Themis Palpanas", "email": "Palpanas"}]}, "sig647": {"session": "Crowdsourcing", "abstract": "Harnessing a crowd of Web users for data collection has recently become a wide-spread phenomenon. A key challenge is that the human knowledge forms an open world and it is thus difficult to know what kind of information we should be looking for. Classic databases have addressed this problem by data mining techniques that identify interesting data patterns. These techniques, however, are not suitable for the crowd. This is mainly due to properties of the human memory, such as the tendency to remember simple trends and summaries rather than exact details.\n\n\n\nFollowing these observations, we develop here for the first time the foundations of crowd mining. We first define the formal settings. Based on these, we design a framework of generic components, used for choosing the best questions to ask the crowd and mining significant patterns from the answers. We suggest general implementations for these components, and test the resulting algorithm's performance on benchmarks that we designed for this purpose. Our algorithm consistently outperforms alternative baseline algorithms.", "title": "Crowd Mining", "authors": [{"affiliation": "Tel Aviv University", "location": "Tel Aviv  Israel ", "name": "Yael Amsterdamer", "email": "Amsterdamer"}, {"affiliation": "Tel Aviv University", "location": "Tel Aviv  Israel ", "name": "Yael Grossman", "email": "Grossman"}, {"affiliation": "Tel Aviv University", "location": "Tel Aviv  Israel ", "name": "Tova Milo", "email": "Milo"}, {"affiliation": "T\u00e9l\u00e9com ParisTech", "location": "Paris  France ", "name": "Pierre Senellart", "email": "Senellart"}]}, "sig480": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "We propose to demonstrate an open-source tool, LinkIT, for privacy preserving record Linkage and Integration via data Transformations. LinkIT implements novel algorithms\n\nthat support data transformations for linking sensitive attributes, and is designed to work with our previously developed tool, FRIL (Fine-grained Record Integration and Linkage), to provide a complete record linkage solution.  LinkIT can be also used as a stand-alone secure transformation tool to link string records.  The system uses a novel embedding technique based on frequent variable length grams mined from original records with differential privacy, and utilizes a personalized threshold for performing linkage in the embedded space. Compared to the state-of-the-art secure transformation method \\cite{Scannapieco:2007}, LinkIT guarantees stronger privacy with better scalability while achieving comparable utility results.", "title": "LinkIT: Privacy Preserving Record Linkage and Integration via Transformations", "authors": [{"affiliation": "Emory University", "location": "Atlanta  USA ", "name": "Luca Bonomi", "email": "Bonomi"}, {"affiliation": "Emory University", "location": "Atlanta  USA ", "name": "Li Xiong", "email": "Xiong"}, {"affiliation": "Emory Uniersity", "location": "Atlanta  USA ", "name": "James Lu", "email": "Lu"}]}, "sig649": {"session": "Data Mining", "abstract": "We study the following problem: Given a database $\\D$ with schema $\\G$ and an output table $Out$, compute a join query $Q$ that generates $Out$ from $\\D$. A simpler variant allows $Q$ to return a superset of $Out$. This problem has numerous applications, both by itself, and as a building block for other problems. Related prior work imposes conditions on the structure of $Q$ which are not always consistent with the application, but simplify computation. We discuss several natural SQL queries that do not satisfy these conditions and cannot be discovered by prior work.\n\n\n\nIn this paper, we propose an efficient algorithm that discovers queries with arbitrary join graphs. A crucial insight is that any graph can be characterized by the \n\ncombination of a simple structure, called a star, and a series of merge steps over the star. The merge steps define a lattice over graphs derived from the same star. This allows us to explore the set of candidate solutions in a principled way and quickly prune out a large number of infeasible graphs. We also design several optimizations that significantly reduce the running time. Finally, we conduct an extensive experimental study over a benchmark database and show that our approach is scalable and accurately discovers complex join queries.\n\n\n\n\n\n", "title": "Reverse Engineering Complex Join Queries", "authors": [{"affiliation": "National University of Singapore", "location": "Singapore  Singapore ", "name": "Meihui Zhang", "email": "Zhang"}, {"affiliation": "Turn, Inc.", "location": "Redwood City CA USA ", "name": "Hazem Elmeleegy", "email": "Elmeleegy"}, {"affiliation": "AT&T Labs - Research", "location": "Florham Park NJ USA ", "name": "Cecilia Procopiuc", "email": "Procopiuc"}, {"affiliation": "AT&T Labs - Research", "location": "Florham Park NJ USA ", "name": "Divesh Srivastava", "email": "Srivastava"}]}, "sig648": {"session": "Information Extraction", "abstract": "Kernel density estimates are important for a broad variety of applications. Their construction has been well-studied, but existing techniques are expensive on massive datasets and/or only provide heuristic approximations without theoretical guarantees.  We propose randomized and deterministic algorithms with quality guarantees which are orders of magnitude more efficient than previous algorithms.  Our algorithms do not require knowledge of the kernel or its bandwidth parameter and are easily parallelizable.  We demonstrate how to implement our ideas in a centralized setting and in \\emph{MapReduce}, although our algorithms are applicable to any large-scale data processing framework.  Extensive experiments on large real datasets demonstrate the quality, efficiency, and scalability of our techniques.", "title": "Quality and Efficiency for Kernel Density Estimates in Large Data", "authors": [{"affiliation": "University of Utah", "location": "Salt Lake City UT USA School of Computing", "name": "Yan Zheng", "email": "Zheng"}, {"affiliation": "University of Utah", "location": "Salt Lake City UT USA School of Computing", "name": "Jeffrey Jestes", "email": "Jestes"}, {"affiliation": "University of Utah", "location": "Salt Lake City UT USA School of Computing", "name": "Jeff Phillips", "email": "Phillips"}, {"affiliation": "University of Utah", "location": "Salt Lake City UT USA School of Computing", "name": "Feifei Li", "email": "Li"}]}, "sig359": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "Even though scripting languages like Pig allow for simpler coding, performing analytics over Big Data using Map-Reduce engines remains challenging. To further assist developers, and support novice users, we offer \"The Farm\", a catalog of scriptable services supporting creation, discovery, composition, and optimized execution. Each Pig script added to The Farm becomes an executable service, with inputs and outputs defined by relation schemas. Those services are discoverable using natural language search, and composable using a drag-and-drop interface. To support efficient execution, composed services are automatically merged to a single executable script, which can then be run by a growing selection of platform-specific optimizers and interpreters.", "title": "The Farm - where Pig Scripts are bred and raised", "authors": [{"affiliation": "HP Labs", "location": "Palo Alto CA USA ", "name": "Craig Sayers", "email": "Sayers"}, {"affiliation": "HP Labs", "location": "Palo Alto CA USA ", "name": "Alkis Simitsis", "email": "Simitsis"}, {"affiliation": "HP Labs", "location": "Palo Alto CA USA ", "name": "Georgia Koutrika", "email": "Koutrika"}, {"affiliation": "HP Labs", "location": "Palo Alto CA USA ", "name": "Alejandro Gonzalez", "email": "Gonzalez"}, {"affiliation": "HP Labs", "location": "Palo Alto CA USA ", "name": "David Cantu", "email": "Cantu"}, {"affiliation": "HP Labs", "location": "Palo Alto CA USA ", "name": "Meichun Hsu", "email": "Hsu"}]}, "sig219": {"session": "Industrial 2: Enterprise Data management", "abstract": "Managing temporal data is becoming increasingly important for many applications. Several database systems already support the time dimension, but provide only few temporal operators, which also often exhibit poor performance characteristics. On the academic side, a large number of algorithms and data structures have been proposed, but they often address a subset of these temporal operators only. In this paper, we develop the Timeline Index as a novel, unified data structure that efficiently supports temporal\n\noperators such as temporal aggregation, time travel, and temporal joins. As the Timeline Index is independent of the physical order of the data, it provides flexibility in physical design; e.g., it supports any kind of compression scheme, which is crucial for main memory column stores. Our experiments show that the Timeline Index has predictable performance and beats state-of-the-art approaches significantly, sometimes by orders of magnitude.", "title": "Timeline Index: A Unified Data Structure for Processing Queries on Temporal Data in SAP HANA", "authors": [{"affiliation": "ETH Zurich", "location": "Zurich  Switzerland Department of Computer Science", "name": "Martin Kaufmann", "email": "Kaufmann"}, {"affiliation": "ETH Zurich", "location": "Zurich  Switzerland Department of Computer Science", "name": "Amin Manjili", "email": "Manjili"}, {"affiliation": "ETH Zurich", "location": "Zurich  Switzerland Department of Computer Science", "name": "Panagiotis Vagenas", "email": "Vagenas"}, {"affiliation": "Uni Freiburg", "location": "Freiburg  Germany ", "name": "Peter Fischer", "email": "Fischer"}, {"affiliation": "ETH Zurich", "location": "Zurich  Switzerland ", "name": "Donald Kossmann", "email": "Kossmann"}, {"affiliation": "SAP AG", "location": "Walldorf  Germany ", "name": "Franz F\u00e4rber", "email": "F\u00e4rber"}, {"affiliation": "SAP AG", "location": "Walldorf  Germany ", "name": "Norman May", "email": "May"}]}, "sig087": {"session": "XML", "abstract": "Keyword search for smallest lowest common ancestors (SLCAs) in XML data has been widely accepted as a meaningful way to identify matching nodes where their subtrees contain an input set of keywords. Although SLCA and its variants (e.g.,MLCA) perform admirably in identifying matching nodes, surprisingly, they perform poorly for searches on irregular schemas that have missing elements, that is, (sub)elements that are optional, or appear in some instances of an element type but not all (e.g., a <population> subelement in a <city> element might be optional, appearing when the population is known and absent when the population is unknown). In this paper, we generalize the SLCA search paradigm to support queries involving missing elements. Specifically, we propose a novel property called optionality resilience that specifies the desired behaviors of an XML keyword search (XKS) approach for queries involving missing elements. We present two variants of a novel algorithm called MESSIAH (Missing Element-conSciouS hIgh-quality SLCA searcH), which are optionality resilient to irregular documents. MESSIAH logically transforms an XML document to a minimal full document where all missing elements are represented as empty elements, i.e., the irregular schema is made \u0093regular\u0094, and then employs efficient strategies to identify partial and complete full SLCA nodes (SLCA nodes in the full document) from it. Specifically, it generates the same SLCA nodes as any state-of-the-art approach when the query does not involve missing elements but avoids irrelevant results when missing elements are involved. Our experimental study demonstrates the ability of MESSIAH to produce superior quality search results.", "title": "MESSIAH: Missing Element-Conscious SLCA Nodes Search in XML Data", "authors": [{"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engg", "name": "Ba Quan Truong", "email": "Truong"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engg", "name": "Sourav S Bhowmick", "email": "S Bhowmick"}, {"affiliation": "Utah State University", "location": "Logan UT USA Department of Computer Science", "name": "Curtis Dyreson", "email": "Dyreson"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engg", "name": "Aixin Sun", "email": "Sun"}]}, "sig563": {"session": "Demo 1: Data Intensive Applications", "abstract": "Reproducibility is a core component of the scientific process. Revisiting and reusing past results allow science to move forward - \"standing on the shoulders of giants\", as Newton once said. An impediment to the adoption of computational reproducibility is that authors find it difficult to generate a compendium that encompasses all the required components to correctly reproduce their experiments. Even when a compendium is available, reviewers and readers may have difficulties in verifying the results on platforms different from the ones where the experiments were originally run. As a step towards simplifying the process of creating reproducible experiments, we have developed ReproZip, a tool that automatically captures the provenance of experiments and packs all the necessary files, library dependencies and variables to reproduce the results. Reviewers can then unpack and run the experiments without having to install any additional software. We will demonstrate real use cases for ReproZip, how packages are created, and how reviewers can validate and explore experiments.", "title": "Packing Experiments for Sharing and Publication", "authors": [{"affiliation": "Polytechnic Institute of New York University", "location": "Brooklyn NY USA ", "name": "Fernando Chirigati", "email": "Chirigati"}, {"affiliation": "New York University", "location": "New York City NY USA ", "name": "Dennis Shasha", "email": "Shasha"}, {"affiliation": "Polytechnic Institute of New York University", "location": "Brooklyn NY USA ", "name": "Juliana Freire", "email": "Freire"}]}, "sig162": {"session": "Industrial 4: Systems & New Hardware Trends", "abstract": "Performance of query processing functions in a DBMS can be affected by many factors, including the hardware platform, data distributions, predicate parameters, compilation method, algorithmic variations and the interactions between these.\n\nGiven that there are often different function implementations possible, there is a latent performance diversity which represents both a threat to performance robustness if ignored (as is usual now) and an opportunity to increase the performance if one would be able to use the best performing implementation in each situation.\n\nMicro Adaptivity, proposed here, is a framework that keeps many alternative function implementations (flavors) in a system.\n\nIt uses a learning algorithm to choose the most promising flavor potentially at each function call, guided by the actual costs observed so far.\n\nWe argue that Micro Adaptivity both increases performance robustness, and saves development time spent in finding and tuning heuristics and cost model thresholds in query optimization.\n\nIn this paper, we (i) characterize a number of factors that cause performance diversity between primitive flavors, (ii) describe an e-greedy learning algorithm that casts the flavor selection into a multi-armed bandit problem, and (iii) describe the software framework for Micro Adaptivity that we implemented in the Vectorwise system.\n\nWe provide micro-benchmarks, and an overall evaluation on TPC-H, showing consistent improvements.", "title": "Micro Adaptivity in Vectorwise", "authors": [{"affiliation": "Actian", "location": "Amsterdam  Netherlands ", "name": "Bogdan Raducanu", "email": "Raducanu"}, {"affiliation": "CWI", "location": "Amsterdam  Netherlands ", "name": "Peter Boncz", "email": "Boncz"}, {"affiliation": "Snowflake Computing", "location": "San Mateo CA USA ", "name": "Marcin Zukowski", "email": "Zukowski"}]}, "sig163": {"session": "Graph Connectivity", "abstract": "A strongly connected component (\\sscc) is a maximal subgraph of a\n\ndirected graph $G$ in which every pair of nodes are reachable from\n\neach other in the \\sscc. With such a property, a general directed\n\ngraph can be represented by a directed acyclic graph (\\DAG) by\n\ncontracting an \\sscc of $G$ to a node in \\DAG. In many real\n\napplications that need graph pattern matching, topological sorting, or\n\nreachability query processing, the best way to deal with a general\n\ndirected graph is to deal with its \\DAG representation. Therefore,\n\nfinding all \\ssccs\n\nin a directed graph $G$ is a critical operation.  The existing\n\nin-memory algorithms based on depth first search (\\dfs) can find all\n\n\\ssccs in linear time w.r.t. the size of a graph. However, when a\n\ngraph cannot resident entirely in the main\n\nmemory, the existing external or semi-external algorithms to find all\n\n\\ssccs have limitation to achieve high I/O efficiency.\n\nIn this paper, we study new I/O efficient semi-external algorithms to\n\nfind all \\ssccs for a massive directed graph $G$ that cannot reside in\n\nmain memory entirely. To overcome the deficiency of the existing \\dfs\n\nbased semi-external algorithm that heavily relies on a total order, we\n\nexplore a weak order based on which we investigate new algorithms.  We\n\npropose a new two phase algorithm, namely, tree\n\nconstruction and tree search. In the tree construction phase, a\n\nspanning tree of $G$ can be constructed in bounded sequential scans of\n\n$G$.  In the tree\n\nsearch phase, it needs to sequentially scan the graph once to find all\n\n\\ssccs.  In addition, we propose a new single phase algorithm, which\n\ncombines the tree construction and tree search phases into a single\n\nphase, with three new optimization techniques. They are early\n\nacceptance, early rejection, and batch processing.  By the single\n\nphase algorithm with the new optimization techniques, we can\n\nsignificantly reduce the number of I/Os and CPU cost.  We conduct\n\nextensive experimental studies using 4 real datasets including a\n\nmassive real dataset, and several synthetic datasets to confirm the\n\nI/O efficiency of our approaches.", "title": "I/O Efficient: Computing SCCs in Massive Graphs", "authors": [{"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  China ", "name": "Zhiwei Zhang", "email": "Zhang"}, {"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  China ", "name": "Jeffrey Xu Yu", "email": "Yu"}, {"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  China ", "name": "Lu Qin", "email": "Qin"}, {"affiliation": "The University of New South Wales", "location": "Sydney  Australia ", "name": "Lijun Chang", "email": "Chang"}, {"affiliation": "The University of New South Wales", "location": "Sydney  Australia ", "name": "Xuemin Lin", "email": "Lin"}]}, "sig216": {"session": "Distributed Systems", "abstract": "In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data.\n\nIt is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users' demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query's results. To enable large-scale inter-media retrieval, we propose a novel \\emph{inter-media hashing} (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.", "title": "Inter-Media Hashing for Large-scale Retrieval from Heterogeneous Data Sources", "authors": [{"affiliation": "The University of Queensland", "location": "Brisbane  Australia ", "name": "jingkuan song", "email": "song"}, {"affiliation": "The University of Queensland", "location": "Brisbane  Australia ", "name": "Yang Yang", "email": "Yang"}, {"affiliation": "Carnegie Mellon University", "location": "Pittsburgh  USA ", "name": "Yi Yang", "email": "Yang"}, {"affiliation": "The University of Queensland", "location": "Brisbane  Australia ", "name": "Zi Huang", "email": "Huang"}, {"affiliation": "The University of Queensland", "location": "Brisbane  Australia ", "name": "Heng Tao Shen", "email": "Shen"}]}, "sig214": {"session": "Demo 3: Database Optimization; Performance", "abstract": "This demonstration presents SharedDB, an implementation of a relational database system capable of executing all SQL operators by sharing computation and resources across all running queries. SharedDB sidesteps the traditional query-at-a-time approach and executes queries in batches. Unlike proposed multi-query optimization ideas, in SharedDB queries do not have to contain common subexpressions in order to be part of the same batch, which allows for a higher degree of sharing. By sharing as much as possible, SharedDB avoids repeating parts of computation that is common across all running queries. The goal of this demonstration is to show the ability of shared query execution to a) answer complex and diverse workloads, and b) reduce the interaction among concurrently executed queries that is observed in traditional systems and leads to performance deterioration and instabilities.", "title": "Workload Optimization using SharedDB", "authors": [{"affiliation": "ETH Zurich", "location": "Zurich  Switzerland Computer Science", "name": "Georgios Giannikis", "email": "Giannikis"}, {"affiliation": "ETH Zurich", "location": "Zurich  Switzerland Computer Science", "name": "Darko Makreshanski", "email": "Makreshanski"}, {"affiliation": "ETH Zurich", "location": "Zurich  Switzerland Computer Science", "name": "Gustavo Alonso", "email": "Alonso"}, {"affiliation": "ETH Zurich", "location": "Zurich  Switzerland Computer Science", "name": "Donald Kossmann", "email": "Kossmann"}]}, "sig401": {"session": "Crowdsourcing", "abstract": "The development of crowdsourced query processing systems has recently attracted a significant attention in the database community. A variety of crowdsourced queries have been investigated. In this paper, we focus on the crowdsourced join query which aims to utilize humans to find all pairs of matching objects from two collections. As a human-only solution is expensive, we adopt a hybrid human-machine approach which first uses machines to generate a candidate set of matching pairs, and then asks humans to label the pairs in the candidate set as either matching or non-matching. Given the candidate pairs, existing approaches will publish all pairs for verification to a crowdsourcing platform. However, they neglect the fact that the pairs satisfy transitive relations. As an example, if o1 matches with o2, and o2 matches with o3, then we can deduce that o1 matches with o3 without needing to crowdsource (o1, o3). To this end, we study how to leverage transitive relations for crowdsourced joins. We propose a hybrid transitive-relations and crowdsourcing labeling framework which aims to crowdsource the minimum number of pairs to label all the candidate pairs. We prove the optimal labeling order and devise a parallel labeling algorithm to efficiently crowdsource the pairs following the order. We evaluate our approaches in both simulated environment and a real crowdsourcing platform. Experimental results show that our approaches with transitive relations can save much more money and time than existing methods, with a little loss in the result quality.", "title": "Leveraging Transitive Relations for Crowdsourced Joins", "authors": [{"affiliation": "Department of Computer Science, Tsinghua University", "location": "Beijing  China ", "name": "Jiannan Wang", "email": "Wang"}, {"affiliation": "Department of Computer Science, Tsinghua University", "location": "Beijing  China ", "name": "Guoliang Li", "email": "Li"}, {"affiliation": "Brown University", "location": "Providence  USA ", "name": "Tim Kraska", "email": "Kraska"}, {"affiliation": "AMPLab, UC Berkeley", "location": "Berkeley  USA ", "name": "Michael Franklin", "email": "Franklin"}, {"affiliation": "Department of Computer Science, Tsinghua University", "location": "Beijing  China ", "name": "Jianhua Feng", "email": "Feng"}]}, "sig654": {"session": "Graph Connectivity", "abstract": "Efficiently computing k-edge connected components in a large graph, G = (V, E), where V is the vertex set and E is the edge set, is a long standing research problem. It is not only fundamental in graph analysis but also crucial in graph search optimization algorithms. Consider existing techniques for computing k-edge connected components are quite time consuming and are unlikely to be scalable for large scale graphs, in this paper we firstly propose a novel graph decomposition paradigm to iteratively decompose a graph G for computing its k-edge connected components such that the number of drilling-down iterations h is bounded by the \u0093depth\u0094 of the k-edge connected components nested together to form G, where h usually is a small integer in practice. Secondly, we devise a novel, efficient threshold-based graph decomposition algorithm, with time complexity O(l \u00d7 |E|), to decompose a graph G at each iteration, where l usually is a small integer with l ? |V|. As a result, our algorithm for computing k-edge connected components significantly improves the time complexity of an existing state-of-the-art technique from O(|V|^2|E| + |V|^3 log |V|) to O(h \u00d7 l \u00d7 |E|). Finally, we conduct extensive performance studies on large real and synthetic graphs. The performance studies demonstrate that our techniques significantly outperform the state-of-the-art solution by several orders of magnitude.", "title": "Efficiently Computing k-Edge Connected Components via Graph Decomposition", "authors": [{"affiliation": "East China Normal University, University of New South Wales", "location": "Sydney  Australia ", "name": "Lijun Chang", "email": "Chang"}, {"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  China ", "name": "Jeffrey Xu Yu", "email": "Yu"}, {"affiliation": "The Chinese University of Hong Kong", "location": "Hong Kong  China ", "name": "Lu Qin", "email": "Qin"}, {"affiliation": "East China Normal University, University of New South Wales", "location": "Sydney  Australia ", "name": "Xuemin Lin", "email": "Lin"}, {"affiliation": "Swinburne University of Technology", "location": "Melbourne  Australia ", "name": "Chengfei Liu", "email": "Liu"}, {"affiliation": "Australian National University", "location": "Canberra  Australia ", "name": "Weifa Liang", "email": "Liang"}]}, "pods100": {"session": "RDF and Ontologies", "abstract": "The Datalog +/- family of expressive extensions of Datalog has recently been introduced as a new paradigm for query answering over ontologies, which captures and extends several common description logics. It extends plain Datalog by features such as existentially quantified rule heads and, at the same time, restricts the rule syntax so as to achieve decidability and tractability. In this paper, we continue the research on Datalog +/-. More precisely, we generalize the well-founded semantics (WFS), as the standard semantics for nonmonotonic normal programs in the database context, to Datalog +/- programs with negation under the unique name assumption (UNA). We prove that for guarded Datalog +/- with negation under the standard WFS,  answering normal Boolean conjunctive queries is decidable, and we provide precise complexity results for this problem, namely, in particular, completeness for PTIME (resp., 2-EXPTIME) in the data (resp., combined) complexity.", "title": "Well-Founded Semantics for Extended Datalog and Ontological Reasoning", "authors": [{"affiliation": "UC Santa Cruz", "location": "Santa Cruz CA USA ", "name": "Andr\u00e9 Hernich", "email": "Hernich"}, {"affiliation": "University of Strathclyde", "location": "Glasgow  Scotland Uk ", "name": "Clemens Kupke", "email": "Kupke"}, {"affiliation": "University of Oxford", "location": "Oxford  United Kingdom ", "name": "Thomas Lukasiewicz", "email": "Lukasiewicz"}, {"affiliation": "University of Oxford", "location": "Oxford  United Kingdom ", "name": "Georg Gottlob", "email": "Gottlob"}]}, "pods101": {"session": "PODS Keynote & Welcome", "abstract": "In this work we survey the research on foundations of data-aware (business) processes that has been carried out in the database theory community. We show that this community has indeed developed over the years a multi-faceted culture of merging data and processes.  We argue that it is this community that should lay the foundations to solve, at least from the point of view of formal analysis, the dichotomy between data and processes still persisting in business process management.", "title": "Foundations of Data-Aware Process Analysis: A Database Theory Perspective", "authors": [{"affiliation": "Free University of Bozen-Bolzano", "location": "Bolzano  Italy KRDB Research Centre for Knowledge and Data", "name": "Diego Calvanese", "email": "Calvanese"}, {"affiliation": "Sapienza Universit\u00e0 di Roma", "location": "Rome  Italy ", "name": "Giuseppe De Giacomo", "email": "De Giacomo"}, {"affiliation": "Free University of Bozen-Bolzano", "location": "Bolzano  Italy KRDB Research Centre for Knowledge and Data", "name": "Marco Montali", "email": "Montali"}]}, "sig289": {"session": "Demo 3: Database Optimization; Performance", "abstract": "Scientific discoveries increasingly rely on the ability to efficiently grind massive amounts of experimental data using database technologies. To bridge the gap between the needs of the Data-Intensive Research fields and the current DBMS technologies, we have introduced SciQL (pronounced as\n\n `cycle'). SciQL is the first SQL-based declarative query language for scientific applications with both tables and arrays as first class citizens. It provides a seamless symbiosis of array-, set- and sequence- interpretations.\n\nA key innovation is the extension of value-based grouping of SQL:2003 with structural grouping, i.e., group array elements based on their positions. This leads to a generalisation of window-based query processing with wide applicability in science domains.\n\n\n\nIn this demo, we showcase a proof of concept implementation of SciQL in the relational database system MonetDB. First, with the Conway's Game of Life application implemented purely in SciQL queries, we demonstrate the storage of arrays in the MonetDB as first class citizens, and the execution of a comprehensive set of basic operations on arrays. Then, to show the usefulness of SciQL for real-world array data processing use cases, we demonstrate how various common image processing and remote sensing operations are executed as SciQL queries. The audience is invited to challenge SciQL with their use cases.", "title": "SciQL: Array Data Processing Inside an RDBMS", "authors": [{"affiliation": "Centrum Wiskunde & Informatica", "location": "Amsterdam  Netherlands ", "name": "Ying Zhang", "email": "Zhang"}, {"affiliation": "Centrum Wiskunde & Informatica", "location": "Amsterdam  Netherlands ", "name": "Martin Kersten", "email": "Kersten"}, {"affiliation": "Centrum Wiskunde & Informatica", "location": "Amsterdam  Netherlands ", "name": "Stefan Manegold", "email": "Manegold"}]}, "sig568": {"session": "Systems, Performance I", "abstract": "Database administrators of Online Transaction Processing (OLTP) systems constantly face difficult questions. For example, \u0093What is the maximum throughput I can sustain with my current hardware?\u0094, \u0093How much disk I/O will my system perform if the requests per second double?\u0094, or \u0093What will happen if the ratio of transactions in my system changes?\u0094. Resource prediction and performance analysis are both vital and difficult in this setting. Here the challenge is due to high degrees of concurrency, competition for resources, and complex interactions between transactions, all of which non-linearly impact performance.\n\nAlthough difficult, such analysis is a key component in enabling database administrators to understand which queries are eating up the resources, and how their system would scale under load. In this paper, we introduce our framework, called DBSeer, that addresses this problem by employing statistical models that provide resource and performance analysis and prediction for highly concurrent OLTP workloads. Our models are built on a small amount of training data from standard log information collected during normal system operation. These models are capable of accurately measuring several performance metrics, including resource consumption on a per-transaction-type basis, resource bottlenecks, and throughput at different load levels. We have validated these models on MySQL/Linux with numerous experiments on standard benchmarks (TPC-C) and real workloads (Wikipedia), observing high accuracy (within a few percent error) when predicting all of the above metrics.\n\n", "title": "Performance and Resource Modeling in Highly-Concurrent OLTP Workloads", "authors": [{"affiliation": "MIT", "location": "Cambridge MA USA CSAIL", "name": "Barzan Mozafari", "email": "Mozafari"}, {"affiliation": "Microsoft", "location": "Mountain View CA USA ", "name": "Carlo Curino", "email": "Curino"}, {"affiliation": "MIT", "location": "Cambridge MA USA CSAIL", "name": "Alekh Jindal", "email": "Jindal"}, {"affiliation": "MIT", "location": "Cambridge MA USA CSAIL", "name": "Samuel Madden", "email": "Madden"}]}, "pods067": {"session": "Indexing/Query Answering", "abstract": "\\begin{abstract}\n\nWe study the static and dynamic \\emph{planar range skyline reporting problem} in the external memory model with block size $B$, under a linear space budget.  The problem asks for an $O(n/B)$ space data structure that stores $n$ points in the plane, and supports reporting the $k$ maximal input points (a.k.a.\\\n\n\\emph{skyline}) among the points that lie within a given query rectangle $Q =\n\n[\\alpha_1, \\alpha_2] \\times [\\beta_1, \\beta_2]$. When $Q$ is \\emph{3-sided},\n\ni.e. one of its edges is grounded, two variants arise: \\emph{top-open} for\n\n$\\beta_2 = \\infty$ and \\emph{left-open} for $\\alpha_1 = -\\infty$ (symmetrically \\emph{bottom-open} and \\emph{right-open}) queries.\n\n\n\nWe present optimal static data structures for \\emph{top-open} queries, for the\n\ncases where the universe is $\\mathbb{R}^2$, a $U \\times U$ grid, and rank space $[\\bigO(n)]^2$. We also show that \\emph{left-open} queries are harder, as they require $\\Omega((n/B)^\\epsilon + k/B)$ I/Os for $\\epsilon > 0$, when only linear space is allowed. We show that the lower bound is tight, by a structure that\n\nsupports 4-sided queries in matching complexities.  Interestingly, these lower\n\nand upper bounds coincide with those of the {\\em planar orthogonal range\n\nreporting problem}, i.e., the skyline requirement does not alter the problem\n\ndifficulty at all!\n\n\n\nFinally, we present the first dynamic linear space data structure that supports\n\ntop-open queries in $O(\\log_{2B^\\epsilon} n + k/B^{1-\\epsilon})$ and updates in $O(\\log_{2B^\\epsilon} n )$ worst case I/Os, for $\\epsilon \\in [0,1]$.  This also\n\nyields a linear space data structure for 4-sided queries with optimal query I/Os\n\nand $\\bigO(\\log (n/B))$ amortized update I/Os.  We consider of independent\n\ninterest the main component of our dynamic structures, a new real-time\n\nI/O-efficient and catenable variant of the fundamental structure {\\em priority\n\nqueue with attrition} by Sundar.\n\n\\end{abstract}", "title": "I/O-Efficient Planar Range Skyline and Attrition Priority Queues", "authors": [{"affiliation": "MADALGO, Aarhus University", "location": "Aarhus  Denmark Computer Science", "name": "Casper Kejlberg-Rasmussen", "email": "Kejlberg-Rasmussen"}, {"affiliation": "Chinese University of Hong Kong, CUHK", "location": "Hong Kong  Hong Kong Computer Science and Engineering", "name": "Yufei Tao", "email": "Tao"}, {"affiliation": "Hong Kong University of Science and Technology, HKUST", "location": "Hong Kong  Hong Kong Computer Science and Engineering", "name": "Konstantinos Tsakalidis", "email": "Tsakalidis"}, {"affiliation": "Aristotle University of Thessaloniki", "location": "Thessaloniki  Greece Computer Science Department", "name": "Kostas Tsichlas", "email": "Tsichlas"}, {"affiliation": "Korea Advanced Institute of Science and Technology, KAIST", "location": "Daejeon  South Korea Division of Web Science and Technology", "name": "Jeonghun Yoon", "email": "Yoon"}]}, "PODS01gc": {"session": "Welcome", "abstract": "", "title": "PODS'13 General Chair's and Program Chair's Welcome Message", "authors": [{"affiliation": "IBM T.J. Watson Research Center", "location": "New York  USA ", "name": "Richard Hull", "email": "Hull"}, {"affiliation": "University of Edinburgh", "location": "Edinburgh  United Kingdom ", "name": "Wenfei Fan", "email": "Fan"}]}, "sig174": {"session": "Data Analytics", "abstract": "We present Cumulon, a system designed to help users rapidly develop and intelligently deploy matrix-based big-data analysis programs in the cloud.  Cumulon features a flexible execution model and new operators especially suited for such workloads. We show how to implement Cumulon on top of Hadoop/HDFS while avoiding limitations of MapReduce, and demonstrate Cumulon's performance advantages over existing Hadoop-based systems for statistical data analysis. To support intelligent deployment in the cloud according to time/budget constraints, Cumulon goes beyond database-style optimization to make choices automatically on not only physical operators and their parameters, but also hardware provisioning and configuration settings. We apply a suite of benchmarking, simulation, modeling, and search techniques to support effective cost-based optimization over this rich space of deployment plans.", "title": "Cumulon: Optimizing Statistical Data Analysis in the Cloud", "authors": [{"affiliation": "Duke University", "location": "Durham NC USA ", "name": "Botong Huang", "email": "Huang"}, {"affiliation": "Duke University", "location": "Durham NC USA ", "name": "Shivnath Babu", "email": "Babu"}, {"affiliation": "Duke University", "location": "Durham NC USA ", "name": "Jun Yang", "email": "Yang"}]}, "sig311": {"session": "Industrial 4: Systems & New Hardware Trends", "abstract": "Data storage devices are getting \u0093smarter.\u0094 Smart Flash storage devices (a.k.a. \u0093Smart SSD\u0094) are on the horizon and will package CPU processing and DRAM storage inside a Smart SSD, and make that available to run user programs inside a Smart SSD. The focus of this paper is on exploring the opportunities and challenges associated with exploiting this functionality of Smart SSDs for relational analytic query processing. We have implemented an initial prototype of Microsoft SQL Server running on a Samsung Smart SSD. Our results demonstrate that significant performance and energy gains can be achieved by pushing selected query processing components inside the Smart SSDs. We also identify various changes that SSD device manufacturers can make to increase the benefits of using Smart SSDs for data processing applications, and also suggest possible research opportunities for the database community.", "title": "Query Processing on Smart SSDs: Opportunities and Challenges", "authors": [{"affiliation": "Microsoft", "location": "Seattle WA USA ", "name": "Jaeyoung Do", "email": "Do"}, {"affiliation": "Samsung", "location": "San Jose CA USA ", "name": "Yang-Suk Kee", "email": "Kee"}, {"affiliation": "University of Wisconsin - Madison", "location": "Madison WI USA Computer Sciences Department", "name": "Jignesh Patel", "email": "Patel"}, {"affiliation": "Samsung", "location": "Banwol-Ri  South Korea ", "name": "Chanik Park", "email": "Park"}, {"affiliation": "University of Wisconsin - Madison", "location": "Madison WI USA Computer Sciences Department", "name": "Kwanghyun Park", "email": "Park"}, {"affiliation": "Microsoft", "location": "Madison WI USA ", "name": "David DeWitt", "email": "DeWitt"}]}, "sig677": {"session": "PTAbstract", "abstract": "We present a novel adaptive log compression scheme. Results show 30% improvement on compression ratios over existing approaches.", "title": "Adaptive Log Compression for Massive Log Data", "authors": [{"affiliation": "University of Utah", "location": "Salt Lake City UT USA School of Computing", "name": "Robert Christensen", "email": "Christensen"}, {"affiliation": "University of Utah", "location": "Salt Lake City UT USA School of Computing", "name": "Feifei Li", "email": "Li"}]}, "phd32wm": {"session": "Welcome", "abstract": "", "title": "SIGMOD'13 PhD Symposium Chairs' Welcome Message", "authors": [{"affiliation": "Hong Kong University of Science and Technology ", "location": "Hong Kong  China ", "name": "Lei Chen", "email": "Chen"}, {"affiliation": "Google Inc.", "location": "Mountain View CA USA ", "name": "Xin Luna Dong", "email": "Dong"}, {"affiliation": "", "location": "   ", "name": "Luna Dong", "email": "Dong"}]}, "sig671": {"session": "Systems, Performance III", "abstract": "Developers of rapidly growing applications must be able to anticipate potential scalability problems before they cause performance issues in production environments. A new type of data independence, called scale independence, seeks to address this challenge by guaranteeing a bounded amount of work is required to execute all queries in an application, independent of the size of the underlying data. While optimization strategies have been developed to provide these guarantees for the class of queries that are scale-independent when executed using simple indexes, there are important queries for which such techniques are insufficient.\n\n\n\nExecuting these more complex queries scale-independently requires precomputation using incrementally-maintained materialized views. However, since this precomputation effectively shifts some of the query processing burden from execution time to insertion time, a scale-independent system must be careful to ensure that storage and maintenance costs do not threaten scalability. In this paper, we describe a scale-independent view selection and maintenance system, which uses novel static analysis techniques that ensure that created views do not themselves become scaling bottlenecks. Finally, we present an empirical analysis that includes all the queries from the TPC-W benchmark and validates our implementation's ability to maintain nearly constant high-quantile query and update latency even as an application scales to hundreds of machines.", "title": "Generalized Scale Independence Through Incremental Precomputation", "authors": [{"affiliation": "Google, Inc", "location": "Mountain View CA USA ", "name": "Michael Armbrust", "email": "Armbrust"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA EECS", "name": "Eric Liang", "email": "Liang"}, {"affiliation": "Brown University", "location": "Providence CA USA ", "name": "Tim Kraska", "email": "Kraska"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA ", "name": "Armando Fox", "email": "Fox"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA EECS", "name": "Michael Franklin", "email": "Franklin"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA EECS", "name": "David Patterson", "email": "Patterson"}]}, "sig672": {"session": "Data Storage", "abstract": "We present a system for the automatic synthesis of efficient algorithms specialized for a particular memory hierarchy and a set of storage devices. The developer provides two independent inputs: 1) an algorithm that ignores memory hierarchy and external storage aspects; and 2) a description of the target memory hierarchy, including its topology and parameters. Our system is able to automatically synthesize memory-hierarchy and storage-device-aware algorithms out of those specifications, for tasks such as joins and sorting. The framework is extensible and allows developers to quickly synthesize custom out-of-core algorithms as new storage technologies become available.", "title": "Automatic Synthesis of Out-of-Core Algorithms", "authors": [{"affiliation": "EPFL", "location": "Lausanne, Vaud  Switzerland School of Computer and Communications Sciences", "name": "Yannis Klonatos", "email": "Klonatos"}, {"affiliation": "EPFL", "location": "Lausanne, Vaud  Switzerland School of Computer and Communications Sciences", "name": "Andres N\u00f6tzli", "email": "N\u00f6tzli"}, {"affiliation": "EPFL", "location": "Lausanne, Vaud  Switzerland School of Computer and Communications Sciences", "name": "Andrej Spielmann", "email": "Spielmann"}, {"affiliation": "EPFL", "location": "Lausanne, Vaud  Switzerland School of Computer and Communications Sciences", "name": "Christoph Koch", "email": "Koch"}, {"affiliation": "EPFL", "location": "Lausanne, Vaud  Switzerland School of Computer and Communications Sciences", "name": "Victor Kuncak", "email": "Kuncak"}]}, "sig673": {"session": "Systems, Performance III", "abstract": " We develop a new pricing system, QueryMarket, for flexible\n\n    query pricing in a data market based on an earlier theoretical\n\n    framework (Koutris et al., PODS 2012). To build such a system, we show how to\n\n    use an Integer Linear Programming formulation of the pricing\n\n    problem for a large class of queries, even when pricing is\n\n    computationally hard. Further, we leverage query\n\n  history to avoid double charging when queries purchased over time have\n\n  overlapping information, or when the database is updated.  We then\n\n  present a technique that fairly shares revenue when multiple\n\n  sellers are involved. Finally, we implement our approach in\n\n  a prototype and evaluate its performance on several query\n\n    workloads.", "title": "Toward Practical Query Pricing with QueryMarket", "authors": [{"affiliation": "University of Washington", "location": "Seattle WA USA ", "name": "Paraschos Koutris", "email": "Koutris"}, {"affiliation": "University of Washington", "location": "Seattle  USA ", "name": "Prasang Upadhyaya", "email": "Upadhyaya"}, {"affiliation": "University of Washington", "location": "Seattle  USA ", "name": "Magdalena Balazinska", "email": "Balazinska"}, {"affiliation": "University of Washington", "location": "Seattle  USA ", "name": "Bill Howe", "email": "Howe"}, {"affiliation": "University of Washington", "location": "Seattle  USA ", "name": "Dan Suciu", "email": "Suciu"}]}, "sig572": {"session": "Demo 1: Data Intensive Applications", "abstract": "Current recommender systems are focused largely on recommending items based on similarity. For instance, Netflix can recommend movies similar to previously viewed movies, and Amazon can recommend items based on ratings of similar users. Although similarity-based recommendation works well for books and movies, it provides an incomplete solution for items such as clothing or furniture which are inherently used in combination with other items of the same type, e.g., shirt with pants, and desk with a chair. As a result, the decision to buy a clothing or furniture item depends not only on the item itself, but also on how well it works with other items of that type. Recommending such items therefore requires a {\\it combination-based} recommendation system that given an item, can suggest interesting and diverse combinations containing that item. This problem is challenging because features affecting combination quality are often difficult to identify; quality, being a function of all items in the combination, cannot be computed independently; and there are an exponential number of combinations to explore. In this demonstration, we present CHIC, a first-of-its-kind, combination-based recommendation system for clothing. The audience will interact with our system through the CHIC mobile app which allows the user to take a picture of a clothing item and search for interesting combinations containing the item instantly. The audience can also compete with CHIC to create alternate ensembles and compare quality. Finally, we highlight via visualizations the core modules of CHIC including model building and our\n\nnovel search and classification algorithm, C-Search.", "title": "CHIC: A Combination-based Recommendation System", "authors": [{"affiliation": "CSAIL, Massachusetts Institute of Technology", "location": "Cambridge MA USA ", "name": "Manasi Vartak", "email": "Vartak"}, {"affiliation": "CSAIL, Massachusetts Institute of Technology", "location": "Cambridge MA USA ", "name": "Samuel Madden", "email": "Madden"}]}, "sig228": {"session": "Graph Management", "abstract": "This paper studies I/O-efficient algorithms for settling the classic {\\em triangle listing} problem, whose solution is a basic operator in dealing with many other graph problems. Specifically, given an undirected graph  $G$, the objective of triangle listing is to find all the cliques involving 3 vertices in $G$. The problem has been well studied in internal memory, but remains an urgent difficult challenge when $G$ does not fit in memory, rendering any algorithm to entail frequent I/O accesses. Although previous research has attempted to tackle the challenge, the state-of-the-art solutions rely on a set of crippling assumptions to guarantee good performance. Motivated by this, we develop a new algorithm that is provably I/O and CPU efficient at the same time, without making any assumption on the input $G$ at all. The algorithm uses ideas drastically different from all the previous approaches, and outperformed the existing competitors by a factor over an order of magnitude in our extensive experimentation.", "title": "Massive Graph Triangulation", "authors": [{"affiliation": "Chinese University of Hong Kong", "location": "Hong Kong  Hong Kong ", "name": "Xiaocheng Hu", "email": "Hu"}, {"affiliation": "Chinese University of Hong Kong", "location": "Hong Kong  Hong Kong ", "name": "Yufei Tao", "email": "Tao"}, {"affiliation": "Korea Advanced Institute of Science and Technology", "location": "Daejeon  South Korea ", "name": "Chin-Wan Chung", "email": "Chung"}]}, "sig576": {"session": "Data Analytics", "abstract": "Shark is a new data analysis system that marries query processing with complex analytics on large clusters. It leverages a novel distributed memory abstraction to provide a unified engine that can run SQL queries and sophisticated analytics functions (e.g. iterative machine learning) at scale, and efficiently recovers from failures mid-query. This allows Shark to run SQL queries up to 100X faster than Apache Hive, and machine learning programs more than 100X faster than Hadoop. Unlike previous systems, Shark shows that it is possible to achieve these speedups while retaining a MapReduce-like execution engine, and the fine-grained fault tolerance properties that such engine provides. It extends such an engine in several ways, including column-oriented in-memory storage and dynamic mid-query replanning, to effectively execute SQL. The result is a system that matches the speedups reported for MPP analytic databases over MapReduce, while offering fault tolerance properties and complex analytics capabilities that they lack.", "title": "Shark: SQL and Rich Analytics at Scale", "authors": [{"affiliation": "UC Berkeley", "location": "Berkeley CA USA AMPLab", "name": "Reynold Xin", "email": "Xin"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA AMPLab", "name": "Josh Rosen", "email": "Rosen"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA AMPLab", "name": "Matei Zaharia", "email": "Zaharia"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA AMPLab", "name": "Michael Franklin", "email": "Franklin"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA AMPLab", "name": "Scott Shenker", "email": "Shenker"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA AMPLab", "name": "Ion Stoica", "email": "Stoica"}]}, "sig321": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "We demonstrate our PARAS technology for supporting interactive association mining at near real-time speeds. Key technical innovations of PARAS, in particular, stable region abstractions and rule redundancy management supporting novel parameter space-centric exploratory queries will be showcased. The audience will be able to interactively explore the parameter space view of rules. They will experience near real-time speeds achieved by PARAS for operations, such as comparing rule sets mined using different parameter values, that would otherwise take hours of computation and much manual investigation. Overall, we will demonstrate that the PARAS system provides a rich experience to data analysts through parameter tuning recommendations while significantly reducing the trial-and-error interactions.", "title": "PARAS: Interactive Parameter Space Exploration for Association Rule Mining", "authors": [{"affiliation": "Worcester Polytechnic Institute", "location": "Worcester MA USA Computer Science", "name": "Abhishek Mukherji", "email": "Mukherji"}, {"affiliation": "Worcester Polytechnic Institute", "location": "Worcester MA USA Computer Science", "name": "Xika Lin", "email": "Lin"}, {"affiliation": "Worcester Polytechnic Institute", "location": "Worcester MA USA Computer Science", "name": "Christopher Botaish", "email": "Botaish"}, {"affiliation": "Worcester Polytechnic Institute", "location": "Worcester MA USA Computer Science", "name": "Jason Whitehouse", "email": "Whitehouse"}, {"affiliation": "Worcester Polytechnic Institute", "location": "Worcester MA USA Computer Science", "name": "Elke Rundensteiner", "email": "Rundensteiner"}, {"affiliation": "Worcester Polytechnic Institute", "location": "Worcester MA USA Computer Science", "name": "Matthew Ward", "email": "Ward"}, {"affiliation": "Worcester Polytechnic Institute", "location": "Worcester MA USA Computer Science", "name": "Carolina Ruiz", "email": "Ruiz"}]}, "sig200": {"session": "Demo 3: Database Optimization; Performance", "abstract": "Recently, a large number of  pay-as-you-go data services are offered over cloud infrastructures. Data service providers need appropriate and flexible query charging mechanisms and query optimization that take into consideration cloud operational expenses, pricing strategies and user preferences. Yet, existing solutions are static and non-configurable. We demonstrate COCCUS, a modular system for cost-aware query execution, adaptive query charge and optimization of cloud data services. The audience can set their queries along with their execution preferences and budget constraints, while COCCUS adaptively determines query charge and manages secondary data structures according to various economic policies. We demonstrate COCCUS 's operation over centralized and shared nothing CloudDBMS architectures on top of public and private IaaS clouds. The audience is enabled to set economic policies and execute various workloads through a comprehensive GUI. COCCUS 's adaptability is showcased using real-time graphs depicting a number of key performance metrics.", "title": "COCCUS: Self-Configured Cost-Based Query Services in the Cloud", "authors": [{"affiliation": "CSLAB, National Technical University of Athens", "location": "Athens  Greece ", "name": "Ioannis Konstantinou", "email": "Konstantinou"}, {"affiliation": "Institute of Services Science University of Geneva", "location": "Geneva  Switzerland ", "name": "Verena Kantere", "email": "Kantere"}, {"affiliation": "Department of Informatics Ionian University", "location": "Corfu  Greece ", "name": "Dimitrios Tsoumakos", "email": "Tsoumakos"}, {"affiliation": "CSLAB, National Technical University of Athens", "location": "Athens  Greece ", "name": "Nectarios Koziris", "email": "Koziris"}]}, "sig073": {"session": "Complex Event Processing", "abstract": "Web-based enterprises process events generated by millions of users interacting with their websites.  Rich statistical data distilled from combining such interactions in near real-time generates enormous business value.  In this paper, we describe the architecture of Photon, a geographically distributed system for joining multiple continuously flowing streams of data in real-time with high scalability and low latency, where the streams may be unordered or delayed.  The system fully tolerates infrastructure degradation and datacenter-level outages without any manual intervention.  Photon guarantees that there will be no duplicates in the joined output (at-most-once semantics) at any point in time, that most joinable events will be present in the output in real-time (near-exact semantics), and exactly-once semantics eventually.\n\n\n\nPhoton is deployed within Google Advertising System to join data streams such as web search queries and user clicks on advertisements.  It produces joined logs that are used to derive key business metrics, including billing for advertisers.  Our production deployment processes millions of events per minute at peak with an average end-to-end latency of less than 10 seconds.  We also present challenges and solutions in maintaining large persistent state across geographically distant locations, and highlight the design principles that emerged from our experience.\n\n", "title": "Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams", "authors": [{"affiliation": "Google Inc.", "location": "Mountain View CA USA ", "name": "Rajagopal Ananthanarayanan", "email": "Ananthanarayanan"}, {"affiliation": "Google Inc.", "location": "Mountain View CA USA ", "name": "Venkatesh Basker", "email": "Basker"}, {"affiliation": "Google Inc.", "location": "Mountain View CA USA ", "name": "Sumit Das", "email": "Das"}, {"affiliation": "Google Inc.", "location": "Mountain View CA USA ", "name": "Ashish Gupta", "email": "Gupta"}, {"affiliation": "Google Inc.", "location": "Mountain View CA USA ", "name": "Haifeng Jiang", "email": "Jiang"}, {"affiliation": "Google Inc.", "location": "Mountain View CA USA ", "name": "Tianhao Qiu", "email": "Qiu"}, {"affiliation": "Max Planck Institute for Software Systems", "location": "Kaiserslautern  Germany ", "name": "Alexey Reznichenko", "email": "Reznichenko"}, {"affiliation": "Google Inc.", "location": "Dublin  Ireland ", "name": "Deomid Ryabkov", "email": "Ryabkov"}, {"affiliation": "Google Inc.", "location": "Mountain View CA USA ", "name": "Manpreet Singh", "email": "Singh"}, {"affiliation": "Google Inc.", "location": "Mountain View CA USA ", "name": "Shivakumar Venkataraman", "email": "Venkataraman"}]}, "sig206": {"session": "Industrial 2: Enterprise Data management", "abstract": "We describe a software architecture we have developed for a constructive containment checker of Entity SQL queries defined over extended ER schemas expressed in Microsoft's Entity Data Model. Our application of interest is compilation of object-to-relational mappings for Microsoft's ADO.NET Entity Framework, which has been shipping since 2007. The supported language includes several features which have been individually addressed in the past but, to the best of our knowledge, they have not been addressed all at once before. Moreover, when embarking on an implementation, we found no guidance in the literature on how to modularize the software or apply published algorithms to a commercially-supported language. This paper reports on our experience in addressing these real-world challenges.", "title": "Query Containment in Entity SQL (Extended Abstract)", "authors": [{"affiliation": "Universitat Polit\u00e8cnica de Catalunya", "location": "Barcelona  Spain Department of Service and Information System Engineering", "name": "Guillem Rull", "email": "Rull"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Philip Bernstein", "email": "Bernstein"}, {"affiliation": "European Microsoft Innovation Center", "location": "Aachen  Germany ", "name": "Ivo dos Santos", "email": "dos Santos"}, {"affiliation": "University of California, San Diego", "location": "La Jolla CA USA Department of Computer Science and Engineering", "name": "Yannis Katsis", "email": "Katsis"}, {"affiliation": "Google", "location": "Kirkland WA USA ", "name": "Sergey Melnik", "email": "Melnik"}, {"affiliation": "Universitat Polit\u00e8cnica de Catalunya", "location": "Barcelona  Spain Department of Service and Information System Engineering", "name": "Ernest Teniente", "email": "Teniente"}]}, "sig478": {"session": "Industrial 3: Big Data II & Web", "abstract": " Database benchmarks are an important tool for database researchers and practitioners that ease the process of making informed comparisons between different database hardware, software and configurations. Large scale web services such as social networks are a major and growing database application area, but currently there are few benchmarks that accurately model web service workloads.  In this paper we present a new synthetic benchmark called LinkBench. LinkBench is based on traces from production databases that store \"social graph\" data at Facebook, a major social network. We characterize the data and query workload in many dimensions, and use the insights gained to construct a realistic synthetic benchmark. LinkBench provides a realistic and challenging test for persistent storage of social and web service data, filling a gap in the available tools for researchers, developers and administrators. \n\n", "title": "LinkBench: a Database Benchmark Based on the Facebook Social Graph", "authors": [{"affiliation": "University of Chicago", "location": "Chicago IL USA Department of Computer Science", "name": "Timothy Armstrong", "email": "Armstrong"}, {"affiliation": "Facebook, Inc.", "location": "Menlo Park CA USA ", "name": "Vamsi Ponnekanti", "email": "Ponnekanti"}, {"affiliation": "Facebook, Inc.", "location": "Menlo Park CA USA ", "name": "Dhruba Borthakur", "email": "Borthakur"}, {"affiliation": "Facebook, Inc.", "location": "Menlo Park CA USA ", "name": "Mark Callaghan", "email": "Callaghan"}]}, "sig479": {"session": "Demo 3: Database Optimization; Performance", "abstract": "Modern enterprises have to deal with a variety of analytical queries over very large datasets. In this respect, Hadoop has gained much popularity since it scales to thousand of nodes and terabytes of data. However, Hadoop suffers from poor performance, especially in I/O performance. Several works have proposed alternate data storage for Hadoop in order to improve the query performance. However, many of these works end up making deep changes in Hadoop or HDFS. As a result, they are (i) difficult to adopt by several users, and (ii) not compatible with future Hadoop releases. In this paper, we present CARTILAGE, a comprehensive data storage framework built on top of HDFS. CARTILAGE allows users full control over their data storage, including data partitioning, data replication, data layouts, and data placement. Furthermore, CARTILAGE can be layered on top of an existing HDFS installation. This means that Hadoop, as well as other query engines, can readily make use of CARTILAGE. We describe several use-cases of CARTILAGE and propose to demonstrate the flexibility and efficiency of CARTILAGE through a set of novel scenarios.", "title": "CARTILAGE: Adding Flexibility to the Hadoop Skeleton", "authors": [{"affiliation": "MIT", "location": "Cambridge MA USA CSAIL", "name": "Alekh Jindal", "email": "Jindal"}, {"affiliation": "QCRI", "location": "Doha  Qatar ", "name": "Jorge Quian\u00e9-Ruiz", "email": "Quian\u00e9-Ruiz"}, {"affiliation": "MIT", "location": "Cambridge MA USA CSAIL", "name": "Samuel Madden", "email": "Madden"}]}, "phdg03": {"session": "PT", "abstract": "Online social networks play a major role in the spread of information at very large scale and it becomes essential to provide means to analyze this phenomenon. Analyzing information diffusion proves to be a challenging task since the raw data produced by users of these networks are a flood of ideas, recommendations, opinions, etc. The aim of this PhD work is to help in the understanding of this phenomenon. So far, our contributions are the following: (i) a survey of developments in the field; (ii) T-BaSIC, a graph-based model for information diffusion prediction; (iii) SONDY, an open source platform that helps understanding social network users' interests and activity by providing emerging topics and events detection as well as network analysis functionalities. ", "title": "Information Diffusion in Online Social Networks", "authors": [{"affiliation": "Universit\u00e9 Lumi\u00e8re Lyon 2", "location": "Bron  France ERIC Lab", "name": "Adrien Guille", "email": "Guille"}]}, "sig473": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "There is a growing interest in methods for analyzing data describing networks of all types, including biological, physical, social, and scientific collaboration networks. Typically the data describing these networks is observational, and thus noisy and incomplete; it is often at\n\nthe wrong level of fidelity and abstraction for meaningful data analysis. This demonstration presents GrDB, a system that enables data analysts to write declarative programs to specify and combine different network data cleaning tasks, visualize the output, and engage in the process of decision review and correction if necessary. The declarative interface of GrDB makes it very easy to quickly write analysis tasks and execute them over data, while the visual component facilitates debugging the program and performing fine grained corrections.\n\n", "title": "GrDB: A System for Declarative and Interactive Analysis of Noisy Information Networks", "authors": [{"affiliation": "University of Maryland", "location": "College Park MD USA Department of Computer Science", "name": "Walaa Eldin Moustafa", "email": "Moustafa"}, {"affiliation": "University of Maryland", "location": "College Park MD USA Department of Computer Science", "name": "Hui Miao", "email": "Miao"}, {"affiliation": "University of Maryland", "location": "College Park MD USA Department of Computer Science", "name": "Amol Deshpande", "email": "Deshpande"}, {"affiliation": "University of Maryland", "location": "College Park MD USA Department of Computer Science", "name": "Lise Getoor", "email": "Getoor"}]}, "pods072": {"session": "Data Mining/Information Retrieval", "abstract": "The monotone duality problem is defined as follows: Given two monotone formulas f and g in irredundant DNF, decide whether f and g are dual. This problem is the same as duality testing for hypergraphs, that is, checking whether a hypergraph H consists of precisely all minimal transversals of a hypergraph G. By exploiting a recent problem-decomposition method by Boros and Makino (ICALP 2009), we show that duality testing for hypergraphs, and thus for monotone DNFs,  is feasible in \n\nDSPACE(log^2 n), i.e., in quadratic logspace. As the monotone duality problem is equivalent to a number of problems in the areas of databases, data mining, and knowledge discovery, the results presented here yield new complexity results for those problems, too. For example, it follows from our results that whenever, for a Boolean-valued relation  (whose attributes represent items), a number of maximal frequent itemsets and a number of minimal infrequent itemsets are known, then it can be decided in quadratic logspace  whether there exist additional frequent or infrequent itemsets. ", "title": "Deciding Monotone Duality and Identifying Frequent Itemsets  in Quadratic Logspace", "authors": [{"affiliation": "University of Oxford", "location": "Oxford  United Kingdom Department of Computer Science", "name": "Georg Gottlob", "email": "Gottlob"}]}, "sig682": {"session": "PTAbstract", "abstract": "We propose Resa, a novel framework for robust, elastic and realtime stream processing in the cloud. In addition to traditional functionalities of streaming and cloud systems, Resa provides (i) a novel mechanism that handles dynamic additions and removals nodes in an operator, and (ii) a node re-assignment scheme that minimizes output latency using a queuing model. We have implemented Resa on top of Twitter Storm. Experiments using real data demonstrate the effectiveness and efficiency of Resa.", "title": "Resa: Realtime Elastic Streaming Analytics in the Cloud", "authors": [{"affiliation": "Shanghai Jiao Tong University", "location": "Shanghai  China ", "name": "Tian Tan", "email": "Tan"}, {"affiliation": "Advanced Digital Sciences Center", "location": "Singapore  Singapore ", "name": "Richard Ma", "email": "Ma"}, {"affiliation": "Advanced Digital Sciences Center", "location": "Singapore  Singapore ", "name": "Marianne Winslett", "email": "Winslett"}, {"affiliation": "Advanced Digital Sciences Center", "location": "Singapore  Singapore ", "name": "Yin Yang", "email": "Yang"}, {"affiliation": "Shanghai Jiao Tong University", "location": "Shanghai  China ", "name": "Yong Yu", "email": "Yu"}, {"affiliation": "Advanced Digital Sciences Center", "location": "Singapore  Singapore ", "name": "Zhenjie Zhang", "email": "Zhang"}]}, "pods077": {"session": "Graph and XML Querying", "abstract": "We study the satisfiability problem for XPath with data equality tests. XPath is a node selecting language for XML documents whose satisfiability problem is known to be undecidable, even for very simple fragments. However, we show that the satisfiability for XPath with the rightward, leftward  and downward reflexive-transitive axes (namely following-sibling-or-self, preceding-sibling-or-self, descendant-or-self) is decidable. Our algorithm yields a complexity of 3EXPSPACE, and we also identify an expressive-equivalent normal form for the logic for which the satisfiability problem is in 2EXPSPACE. These results are in contrast with the undecidability of the satisfiability problem as soon as we replace  the reflexive-transitive axes with just transitive (non-reflexive) ones.", "title": "On XPath with Transitive Axes and Data Tests", "authors": [{"affiliation": "University of Edinburgh", "location": "Edinburgh  United Kingdom ", "name": "Diego Figueira", "email": "Figueira"}]}, "pods074": {"session": "Awards Session", "abstract": "The term naive evaluation refers to evaluating queries over\n\nincomplete databases as if nulls were usual data values, i.e., to\n\nusing the standard database query evaluation engine. Since the\n\nsemantics of query answering over incomplete databases is that of\n\ncertain answers, we would like to know when naive evaluation\n\ncomputes them: i.e., when certain answers can be found without\n\ninventing new specialized algorithms. For relational databases it is\n\nwell known that unions of conjunctive queries possess this desirable\n\nproperty, and results on preservation of formulae under homomorphisms\n\ntell us that within relational calculus, this class cannot be\n\nextended under the open-world assumption.\n\n\n\nOur goal here is twofold. First, we develop a general framework that\n\nallows us to determine, for a given semantics of incompleteness,\n\nclasses of queries for which naive evaluation computes certain\n\nanswers. Second, we apply this approach to a variety of semantics,\n\nshowing that for many classes of queries beyond unions of conjunctive\n\nqueries, naive evaluation makes perfect sense under assumptions\n\ndifferent from open-world.  Our key observations are: (1)\n\n naive evaluation is equivalent to\n\nmonotonicity of queries with respect to a semantics-induced ordering,\n\nand (2) for most reasonable semantics, such monotonicity is captured\n\nby preservation under various types of homomorphisms. Using these\n\nresults we find classes of queries for which naive evaluation works,\n\ne.g., positive first-order formulae for the closed-world\n\nsemantics. Even more, we introduce a general relation-based\n\nframework for defining semantics of incompleteness, show how it can be\n\nused to capture many known semantics and to introduce new ones, and\n\ndescribe classes of first-order queries for which naive evaluation\n\nworks under such semantics.\n\n\n\n", "title": "When is Naive Evaluation Possible?", "authors": [{"affiliation": "University of Edinburgh", "location": "Edinburgh  United Kingdom ", "name": "Amelie Gheerbrant", "email": "Gheerbrant"}, {"affiliation": "University of Edinburgh", "location": "Edinburgh  United Kingdom ", "name": "Leonid Libkin", "email": "Libkin"}, {"affiliation": "LSV, ENS-Cachan, INRIA and CNRS", "location": "Cachan  France ", "name": "Cristina Sirangelo", "email": "Sirangelo"}]}, "pods075": {"session": "RDF and Ontologies", "abstract": "Querying RDF data is viewed as one of the main applications of graph\n\nquery languages, and yet the standard model of graph databases --\n\nessentially labeled graphs -- is different from the triples-based\n\nmodel of RDF. While encodings of RDF databases into graph data exist,\n\nwe show that even the most natural ones are bound to lose some\n\nfunctionality when used in conjunction with graph query languages. The\n\nsolution is to work directly with triples, but then many properties\n\ntaken for granted in the graph database context (e.g., reachability)\n\nlose their natural meaning.\n\n\n\nOur goal is to introduce languages that work directly over triples and\n\nare closed, i.e., they produce sets of triples, rather than\n\ngraphs. Our basic language is called $\\TA$, or Triple Algebra: it\n\nguarantees closure properties by replacing the product with a family\n\nof join operations.  We extend $\\TA$ with recursion, and explain\n\nwhy such an extension is more intricate for triples than for\n\ngraphs. We present a declarative language, namely a fragment of\n\ndatalog, capturing the recursive algebra. For both languages, the\n\ncombined complexity of query evaluation is given by low-degree\n\npolynomials. We compare our languages with relational languages, such\n\nas finite-variable logics, and previously studied graph query\n\nlanguages such as adaptations of XPath, regular path queries, and\n\nnested regular expressions; many of these languages are subsumed by\n\nthe recursive triple algebra. We also provide examples of the\n\nusefulness of $\\TA$ in querying graph and RDF data.", "title": "TriAL for RDF: Adapting Graph Query Languages for RDF Data", "authors": [{"affiliation": "University of Edinburgh", "location": "Edinburgh  United Kingdom ", "name": "Leonid Libkin", "email": "Libkin"}, {"affiliation": "University of Edinburgh and PUC Chile", "location": "Edinburgh  United Kingdom ", "name": "Juan Reutter", "email": "Reutter"}, {"affiliation": "University of Edinburgh", "location": "Edinburgh  United Kingdom ", "name": "Domagoj Vrgo?", "email": "Vrgo?"}]}, "sig668": {"session": "Systems, Performance II", "abstract": "It is surprisingly hard to obtain accurate and precise measurements of the time spent executing a query.  We review relevant process and overall\n\nmeasures obtainable from the Linux kernel and introduce a structural causal model relating these measures. A thorough correlational analysis provides strong support for this model. Using this model, we developed a timing protocol, which (1) performs sanity checks to ensure validity of the data, (2) drops some query  executions via clearly motivated predicates, (3) drops some entire queries at a cardinality, again via clearly motivated predicates, (4) for those that remain, for each computes a single measured time by a carefully justified formula over the underlying measures of the remaining query executions, and (5) performs post-analysis sanity checks. The resulting query time measurement procedure, termed the Tucson Protocol, applies to proprietary and open-source DBMSes.", "title": "DBMS Metrology: Measuring Query Time", "authors": [{"affiliation": "University of Arizona", "location": "Tucson AZ USA ", "name": "Sabah Currim", "email": "Currim"}, {"affiliation": "University of Arizona", "location": "Tucson AZ USA Department of Computer Science", "name": "Richard Snodgrass", "email": "Snodgrass"}, {"affiliation": "University of Arizona", "location": "Tucson AZ USA Department of Computer Science", "name": "Young-Kyoon Suh", "email": "Suh"}, {"affiliation": "Teradata Corporation", "location": "Torrence CA USA ", "name": "Rui Zhang", "email": "Zhang"}, {"affiliation": "USDS", "location": "San Diego CA USA ", "name": "Matthew Johnson", "email": "Johnson"}, {"affiliation": "University of Arizona", "location": "Tucson AZ USA Department of Computer Science", "name": "Cheng Yi", "email": "Yi"}]}, "sig231": {"session": "Security", "abstract": "The modern computing landscape contains an increasing number of app ecosystems, where users store personal data on platforms such as Facebook or smartphones. APIs enable third-party applications (apps) to utilize that data. A key concern associated with app ecosystems is the confidentiality of user data.\n\n\n\nIn this paper, we develop a new model of disclosure in app ecosystems. In contrast with previous solutions, our model is data-derived and semantically meaningful. Information disclosure is modeled in terms of a set of distinguished security views.  Each query is labeled with the precise set of security views that is needed to answer it, and these labels drive policy decisions.\n\n\n\nWe explain how our disclosure model can be used in practice and provide algorithms for labeling conjunctive queries for the case of single-atom security views. We show that our approach is useful by demonstrating the scalability of our algorithms and by applying it to the real-world disclosure control system used by Facebook.", "title": "Fine-Grained Disclosure Control for App Ecosystems", "authors": [{"affiliation": "Cornell University", "location": "Ithaca NY USA ", "name": "Gabriel Bender", "email": "Bender"}, {"affiliation": "Cornell University", "location": "Ithaca NY USA ", "name": "Lucja Kot", "email": "Kot"}, {"affiliation": "Cornell University", "location": "Ithaca NY USA ", "name": "Johannes Gehrke", "email": "Gehrke"}, {"affiliation": "EPFL", "location": "Lausanne  Switzerland ", "name": "Christoph Koch", "email": "Koch"}]}, "sig105": {"session": "Industrial 1: Big Data I", "abstract": "We present the architecture behind Twitter's real-time related query\n\nsuggestion and spelling correction service. Although these tasks have\n\nreceived much attention in the web search literature, the Twitter\n\ncontext introduces a real-time \"twist\": after significant breaking\n\nnews events, we aim to provide relevant results within minutes.  This\n\npaper provides a case study illustrating the challenges of real-time\n\ndata processing in the era of \"big data\". We tell the story of how\n\nour system was built twice: our first implementation was built on a\n\ntypical Hadoop-based analytics stack, but was later replaced because\n\nit did not meet the latency requirements necessary to generate\n\nmeaningful real-time results. The second implementation, which is the\n\nsystem deployed in production today, is a custom in-memory processing engine\n\nspecifically designed for the task. This experience taught us that the\n\ncurrent typical usage of Hadoop as a \"big data\" platform, while\n\ngreat for experimentation, is not well suited to low-latency\n\nprocessing, and points the way to future work on data analytics\n\nplatforms that can handle \"big\" as well as \"fast\" data.\n\n", "title": "Fast Data in the Era of Big Data: Twitter's Real-Time Related Query Suggestion Architecture", "authors": [{"affiliation": "Twitter", "location": "San Francisco CA USA ", "name": "Gilad Mishne", "email": "Mishne"}, {"affiliation": "Twitter", "location": "San Francisco CA USA ", "name": "Jeff Dalton", "email": "Dalton"}, {"affiliation": "Twitter", "location": "San Francisco CA USA ", "name": "Zhenghua Li", "email": "Li"}, {"affiliation": "Twitter", "location": "San Francisco CA USA ", "name": "Aneesh Sharma", "email": "Sharma"}, {"affiliation": "Twitter", "location": "San Francisco CA USA ", "name": "Jimmy Lin", "email": "Lin"}]}, "sig663": {"session": "Social Media", "abstract": " A great deal of research has been conducted on modeling and discovering communities in complex networks. In most real life networks, an object often participates in  multiple overlapping communities. In view of this, recent research has focused on mining overlapping communities in complex networks. The algorithms essentially materialize a snapshot of the overlapping communities in the network. This approach has three drawbacks, however. First, the mining algorithm uses the same global criterion to decide whether a subgraph qualifies as a community.  In other words, the criterion is fixed and predetermined. But in\n\nreality, communities for different vertices may have very\n\ndifferent characteristics. Second, it is costly, time consuming, and often unnecessary to find communities for an entire network. Third, the approach does not support dynamically evolving networks. In this paper, we focus on online search of overlapping communities, that is, given a query vertex, we find meaningful overlapping communities the vertex belongs to in an online manner. In doing so,\n\n  each search can use community criterion tailored for the vertex in the search. To support this approach, we introduce a novel model for overlapping communities, and we provide theoretical guidelines for tuning the model. We present several algorithms for online overlapping community search and we conduct comprehensive experiments to demonstrate the effectiveness of the model and the algorithms. We also suggest many potential applications of our model and algorithms.", "title": "Online Search of Overlapping Communities", "authors": [{"affiliation": "Fudan University", "location": "Shanghai  China ", "name": "Wanyun Cui", "email": "Cui"}, {"affiliation": "Fudan University", "location": "Shanghai  China ", "name": "Yanghua Xiao", "email": "Xiao"}, {"affiliation": "Microsoft Research Asia", "location": "Beijing  China ", "name": "Haixun Wang", "email": "Wang"}, {"affiliation": "Fudan University", "location": "Shanghai  China ", "name": "Yiqi Lu", "email": "Lu"}, {"affiliation": "Fudan University", "location": "Shanghai  China ", "name": "Wei Wang", "email": "Wang"}]}, "sig662": {"session": "Data Streams", "abstract": "Massive amount of data that are geo-tagged and associated with text information are being generated at an unprecedented scale. Users may want to be notified of interesting geo-textual objects during a period of time. For example, a user may want to be informed when tweets containing term \"garage sale\" are posted within 5 km of the user's home in the next 72 hours.\n\n\n\nIn this paper, for the first time we study the problem of matching a stream of incoming Boolean Range Continuous queries over a stream of incoming geo-textual objects in real time. We develop a new system for addressing the problem. In particular, we propose a hybrid index, called IQ-tree, and novel cost models for managing a stream of incoming Boolean Range Continuous queries. We also propose algorithms for matching the queries with incoming geo-textual objects based on the index.  Results of empirical studies with\n\nimplementations of the proposed techniques demonstrate that the paper's proposals offer scalability and are capable of excellent performance.", "title": "An Efficient Query Indexing Mechanism for Filtering Geo-Textual Data", "authors": [{"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engineering", "name": "Lisi Chen", "email": "Chen"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engineering", "name": "Gao Cong", "email": "Cong"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engineering", "name": "Xin Cao", "email": "Cao"}]}, "sig661": {"session": "Data Cleaning", "abstract": "Despite the increasing importance of data quality and the rich theoretical and practical contributions in all aspects of data cleaning, there is no single end-to-end off-the-shelf solution to (semi-)automate the detection and the repairing of violations w.r.t. a set of heterogeneous and ad-hoc quality constraints. In short, there is no commodity platform similar to general purpose DBMSs that can be easily customized and deployed to solve application-specific data quality problems. In this paper, we present NADEEF, an extensible, generalized and easy-to-deploy data cleaning platform. NADEEF distinguishes between a programming interface and a core to achieve generality and extensibility. The programming interface allows the users to specify multiple types of data quality rules, which uniformly define what is wrong with the data and (possibly) how to repair it through writing code that implements predefined classes. We show that the programming interface can be used to express many types of data quality rules beyond the well known CFDs (FDs), MDs and ETL rules. Treating user implemented interfaces as black-boxes, the core provides algorithms to detect errors and to clean data. The core is designed in a way to allow cleaning algorithms to cope with multiple rules holistically, i.e. detecting and repairing data errors without differentiating between various types of rules. We showcase two implementations for core repairing algorithms. These two implementations demonstrate the extensibility of our core, which can also be replaced by other user-provided algorithms. Using real-life data, we experimentally verify the generality, extensibility, and effectiveness of our system.", "title": "NADEEF: A Commodity Data Cleaning System", "authors": [{"affiliation": "University of Trento", "location": "Trento  Italy ", "name": "Michele Dallachiesa", "email": "Dallachiesa"}, {"affiliation": "Purdue University", "location": "Lafayette  USA ", "name": "Amr Ebaid", "email": "Ebaid"}, {"affiliation": "University of Minnesota", "location": "Minneapolis  USA ", "name": "Ahmed Eldawy", "email": "Eldawy"}, {"affiliation": "QCRI", "location": "Doha  Qatar ", "name": "Ahmed Elmagarmid", "email": "Elmagarmid"}, {"affiliation": "QCRI", "location": "Doha  Qatar ", "name": "Ihab Ilyas", "email": "Ilyas"}, {"affiliation": "QCRI", "location": "Doha  Qatar ", "name": "Mourad Ouzzani", "email": "Ouzzani"}, {"affiliation": "QCRI", "location": "Doha  Qatar ", "name": "Nan Tang", "email": "Tang"}]}, "phdg12": {"session": "PT", "abstract": "While originally proposed to provide fault-tolerance and scalability for data analysis queries on unstructured data over massive clusters, MapReduce systems today are being used for analysis of rich combinations of unstructured, semi-structured and structured data.  To achieve performance on these new workloads, MapReduce systems (and the distributed file systems on which they are built) can no longer rely on static data placement strategies.  In this thesis, we propose new physical data independence and adaptive data tuning solutions that can greatly improve the performance of analysis queries in systems where workloads are not static and where workloads may include complex queries with overlapping or related computations (subqueries). While profiting from the work on physical data independence in relational systems, we propose novel strategies that recognize the central role of data partitioning (and co-partitioning) in shared-nothing distributed file systems.", "title": "DeepSea: Self-Adaptive Data Partitioning and Replication in Scalable Distributed Data Systems", "authors": [{"affiliation": "University of Toronto", "location": "Toronto ON Canada Computer Science", "name": "Jiang Du", "email": "Du"}]}, "phdg13": {"session": "PT", "abstract": "Data processing systems face the task of efficiently storing and processing data at petabyte scale, with\n\nthe amount set to increase in the future. To meet such a requirement, highly scalable, shared-nothing\n\nsystems, e.g. Google's BigTable \\cite{chang2008bigtable} or Facebook's Cassandra \\cite{lakshman2010cassandra},\n\nare built to partition data and process it in parallel on distributed nodes in a cluster. This allows\n\nthe handling of data at scale but introduces new challenges due to the distribution of data. Running queries\n\ninvolves a high network overhead because data has to be exchanged between cluster nodes and hence, the network\n\nbecomes a critical part of the system. To avoid the network bottleneck, it is essential for distributed data\n\nprocessing systems (DDPS) to be aware of the network rather than treating it as a black box.\n\n\n\nWe propose in-network processing as a way of achieving network-awareness to decrease bandwidth usage by\n\ncustom routing, redundancy elimination, and on-path data reduction. Thereby, we can increase the query\n\nthroughput of a DDPS. The challenges of an in-network processing system range from design issues, such as\n\nperformance and transparency, to the integration with query optimisation and deployment in data centres. We\n\nformulate these challenges as possible research directions and provide a prototype implementation. Our \n\npreliminary results suggest that we can significantly improve query throughput in a DDPS by performing\n\npartial data reduction within the network.", "title": "Exploiting In-network Processing for Big Data Management", "authors": [{"affiliation": "Imperial College", "location": "London  United Kingdom Computing", "name": "Lukas Rupprecht", "email": "Rupprecht"}]}, "sig665": {"session": "Privacy", "abstract": "epsilon-differential privacy is rapidly emerging as the state-of-the-art scheme for protecting individuals' privacy in published analysis results over sensitive data. The main idea is to perform random perturbations on the analysis results, such that any individual's presence in the data has negligible impact on the randomized results. This paper focuses on analysis tasks that involve model fitting, i.e., finding the parameters of a statistical model that best fit the dataset. For such tasks, the quality of the differentially private results depends upon both the effectiveness of the model fitting algorithm, and the amount of perturbations required to satisfy the privacy guarantees. Most previous studies start from a state-of-the-art, non-private model fitting algorithm, and develop a differentially private version. Unfortunately, many model fitting algorithms require intensive perturbations to satisfy epsilon-differential privacy, leading to poor overall result quality.\n\n\n\nMotivated by this, we propose PrivGene, a general-purpose differentially private model fitting solution based on genetic algorithms (GA).  PrivGene needs significantly less perturbations than previous methods, and it achieves higher overall result quality, even for model fitting tasks where GA is not the first choice without privacy considerations. Further, PrivGene performs the random perturbations using a novel technique called the enhanced exponential mechanism, which improves over the exponential mechanism by exploiting the special properties of model fitting tasks. As case studies, we apply PrivGene to three common analysis tasks involving model fitting: logistic regression, SVM classification, and k-means clustering. Extensive experiments using real data confirm the high result quality of PrivGene, and its superiority over existing methods.", "title": "PrivGene: Differentially Private Model Fitting Using Genetic Algorithms", "authors": [{"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engineering", "name": "Jun Zhang", "email": "Zhang"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engineering", "name": "Xiaokui Xiao", "email": "Xiao"}, {"affiliation": "Advanced Digital Sciences Center", "location": "Singapore  Singapore ", "name": "Yin Yang", "email": "Yang"}, {"affiliation": "Advanced Digital Sciences Center", "location": "Singapore  Singapore ", "name": "Zhenjie Zhang", "email": "Zhang"}, {"affiliation": "University of Illinois at Urbana-Champaign", "location": "Urbana IL USA Department of Computer Science", "name": "Marianne Winslett", "email": "Winslett"}]}, "sig664": {"session": "Indexing", "abstract": "Main-memory database systems are emerging as the new backbone of business applications. Besides flat relational data representations also hierarchical ones are essential for these modern applications; therefore we devise a new indexing and versioning approach for hierarchies that is deeply integrated into the relational kernel.\n\n\n\nWe propose the \\textit{DeltaNI} index as a versioned pendant of the nested intervals (NI) labeling scheme. The index is space- and time-efficient and yields a gapless, fixed-size integer NI labeling for each version while also supporting branching histories. In contrast to a na\\\"ive NI labeling, it facilitates even complex updates of the tree structure. As many query processing techniques that work on top of the NI labeling have already been proposed, our index can be used as a building block for processing various kinds of queries. We evaluate the performance of the index on large inputs consisting of millions of nodes and thousands of versions. Thereby we show that DeltaNI scales well and can deliver satisfying performance for large business scenarios.", "title": "DeltaNI: An Ef?cient Labeling Scheme for Versioned Hierarchical Data", "authors": [{"affiliation": "Technische Universit\u00e4t M\u00fcnchen", "location": "Garching  Germany ", "name": "Jan Finis", "email": "Finis"}, {"affiliation": "Technische Universit\u00e4t M\u00fcnchen", "location": "Garching  Germany ", "name": "Robert Brunel", "email": "Brunel"}, {"affiliation": "Technische Universit\u00e4t M\u00fcnchen", "location": "Garching  Germany ", "name": "Alfons Kemper", "email": "Kemper"}, {"affiliation": "Technische Universit\u00e4t M\u00fcnchen", "location": "Garching  Germany ", "name": "Thomas Neumann", "email": "Neumann"}, {"affiliation": "SAP AG", "location": "Walldorf  Germany ", "name": "Franz F\u00e4rber", "email": "F\u00e4rber"}, {"affiliation": "SAP AG", "location": "Walldorf  Germany ", "name": "Norman May", "email": "May"}]}, "sig333": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "Burst identification has been extensively studied in the \n\ncontext of document streams, where a burst is generally exhibited when an unusually \n\nhigh frequency is observed for a term t. Previous works have focused exclusively on \n\neither temporal or spatial burstiness patterns. The former represents bursty timeframes \n\nwithin a single stream, while the latter characterizes sets of streams that simultaneously exhibited a \n\nbursty behavior for a user-specified timeframe. Our previous work was \n\nthe first to study the spatiotemporal burstiness of terms. In this context, a \n\nburstiness pattern consists of both a timeframe and a set of streams, both of which need \n\nto be identified automatically. In this paper we describe STEM (Spatio-TEmporal Miner), \n\na system for finding spatiotemporal burstiness patterns in a collection of spatially \n\ndistributed frequency streams. STEM implements the full \n\nfunctionality required to mine spatiotemporal burstiness patterns from virtually \n\nany collection of geostamped streams. Examples of such collections include document streams \n\n(e.g. online newspapers), geo-aware microblogging platforms (e.g. Twitter). \n\nThis paper describes the STEM system and discusses how its features can \n\nbe accessed via a user-friendly interface.", "title": "STEM: A Spatio-TEmporal Miner for Bursty Activity", "authors": [{"affiliation": "Boston University", "location": "Boston MA USA ", "name": "Theodoros Lappas", "email": "Lappas"}, {"affiliation": "IBM Research", "location": "Rio De Janeiro  Brazil ", "name": "Marcos Vieira", "email": "Vieira"}, {"affiliation": "University of Athens", "location": "Athens  Greece ", "name": "Dimitrios Gunopulos", "email": "Gunopulos"}, {"affiliation": "University of California, Riverside", "location": "Riverside  USA ", "name": "Vassilis Tsotras", "email": "Tsotras"}]}, "sig332": {"session": "Industrial 3: Big Data II & Web", "abstract": "There is a tremendous interest in big data by academia, industry and a large user base. Several commercial and open source providers unleashed a variety of products to support big data storage and processing. As these products mature, there is a need to evaluate and compare the performance of these systems. \n\nIn this paper, we present BigBench, an end-to-end big data benchmark proposal. The underlying business model of BigBench is a product retailer. The proposal covers a data model and synthetic data generator that addresses the variety, velocity and volume aspects of big data systems containing structured, semi-structured and unstructured data. The structured part of the BigBench data model is adopted from the TPC-DS benchmark, which is enriched with semi-structured and unstructured data components. The semi-structured part captures registered and guest user clicks on the retailer\u0092s website. The unstructured data captures product reviews submitted online. The data generator designed for BigBench provides scalable volumes of raw data based on a scale factor. The BigBench workload is designed around a set of queries against the data model. From a business prospective, the queries cover the different categories of  big data analytics proposed by McKinsey. From a technical prospective, the queries are designed to span three different dimensions based on data sources, query processing types and analytic techniques. \n\nWe illustrate the feasibility of BigBench by implementing it on the Teradata Aster Database. The test includes generating and loading a 200 Gigabyte BigBench data set and testing the workload by executing the BigBench queries (written using Teradata Aster SQL-MR) and reporting their response times. \n\n", "title": "BigBench: Towards an Industry Standard Benchmark for Big Data Analytics", "authors": [{"affiliation": "Teradata Corp.", "location": "El Segundo CA USA ", "name": "Ahmad Ghazal", "email": "Ghazal"}, {"affiliation": "University of Toronto", "location": "Toronto ON Canada ", "name": "Tilmann Rabl", "email": "Rabl"}, {"affiliation": "Teradata Corp.", "location": "El Segundo CA USA ", "name": "Minqing Hu", "email": "Hu"}, {"affiliation": "InfoSizing, Inc.", "location": "Manitou Springs CO USA ", "name": "Francois Raab", "email": "Raab"}, {"affiliation": "Oracle Corp.", "location": "Redwood Shores CA USA ", "name": "Meikel Poess", "email": "Poess"}, {"affiliation": "Teradata Corp.", "location": "El Segundo CA USA ", "name": "Alain Crolotte", "email": "Crolotte"}, {"affiliation": "University of Toronto", "location": "Toronto ON Canada ", "name": "Hans-Arno Jacobsen", "email": "Jacobsen"}]}, "sig067": {"session": "Industrial 5: Big Data III & More", "abstract": "This paper presents Polybase, a feature of SQL Server PDW V2 that allows users to manage and query data stored in a Hadoop cluster using the standard SQL query language.  Unlike other database systems that provide only a relational view over HDFS-resident data through the use of an external table mechanism, Polybase employs a split query processing paradigm in which SQL operators on HDFS-resident data are translated into MapReduce jobs by the PDW query optimizer and then executed on the Hadoop cluster.  The paper describes the design and implementation of Polybase along with a thorough performance evaluation that explores the benefits of employing a split query processing paradigm for executing queries that involve both structured data in a relational DBMS and unstructured data in Hadoop. Our results demonstrate that while the use of a split-based query execution paradigm can improve the performance of some queries by as much as 10X, one must employ a cost-based query optimizer that considers a broad set of factors when deciding whether or not it is advantageous to push a SQL operator to Hadoop. These factors include the selectivity factor of the predicate, the relative sizes of the two clusters, and whether or not their nodes are co-located. In addition, differences in the semantics of the Java and SQL languages must be carefully considered in order to avoid altering the expected results of a query.", "title": "Split Query Processing in Polybase", "authors": [{"affiliation": "Microsoft Corporation", "location": "Madison WI USA Jim Gray Systems Lab", "name": "David DeWitt", "email": "DeWitt"}, {"affiliation": "Microsoft Corporation", "location": "Madison WI USA Jim Gray Systems Lab", "name": "Alan Halverson", "email": "Halverson"}, {"affiliation": "Microsoft Corporation", "location": "Madison WI Uganda Jim Gray Systems Lab", "name": "Rimma Nehme", "email": "Nehme"}, {"affiliation": "Microsoft Corporation", "location": "Madison WI USA Jim Gray Systems Lab", "name": "Srinath Shankar", "email": "Shankar"}, {"affiliation": "Microsoft Corporation", "location": "Aliso Viejo DC USA ", "name": "Josep Aguilar-Saborit", "email": "Aguilar-Saborit"}, {"affiliation": "Microsoft Corporation", "location": "Aliso Viejo CA USA ", "name": "Artin Avanes", "email": "Avanes"}, {"affiliation": "Microsoft Corporation", "location": "Aliso Viejo CA USA ", "name": "Miro Flasza", "email": "Flasza"}, {"affiliation": "Microsoft Corporation", "location": "Aliso Viejo CA USA ", "name": "Jim Gramling", "email": "Gramling"}]}, "mod13nrs": {"session": "PTAbstract", "abstract": "", "title": "SIGMOD 2013 New Researcher Symposium", "authors": [{"affiliation": "Google", "location": "New York CA USA ", "name": "Anish Das Sarma", "email": "Das Sarma"}, {"affiliation": "Google", "location": "Mountain View CA USA ", "name": "Xin Luna Dong", "email": "Luna Dong"}]}, "sig469": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "This paper describes SONDY, a tool for analysis of trends and dynamics in online social network data. SONDY addresses two audiences: (i) end-users who want to explore social activity and (ii) researchers who want to experiment and compare mining techniques on social data. SONDY helps end-users like media analysts or journalists understand social network users interests and activity by providing emerging topics and events detection as well as network analysis functionalities. To this end, the application proposes visualizations such as interactive time-lines that summarize information and colored user graphs that reflect the structure of the network. SONDY also provides researchers an easy way to compare and evaluate recent techniques to mine social data, implement new algorithms and extend the application without being concerned with how to make it accessible. In the demo, participants will be invited to explore information from several datasets of various sizes and origins (such as a dataset consisting of 7,874,772 messages published by 1,697,759 Twitter users during a period of 7 days) and apply the different functionalities of the platform in real-time.", "title": "SONDY: An Open Source Platform for Social Dynamics Mining and Analysis", "authors": [{"affiliation": "Lyon 2 University", "location": "Bron  France ERIC Lab", "name": "Adrien Guille", "email": "Guille"}, {"affiliation": "Lyon 2 University", "location": "Bron  France ERIC Lab", "name": "C\u00e9cile Favre", "email": "Favre"}, {"affiliation": "Alcatel-Lucent Bell Labs France", "location": "Nozay  France ", "name": "Hakim Hacid", "email": "Hacid"}, {"affiliation": "Lyon 2 University", "location": "Lyon  France Institute of Human Science", "name": "Djamel Zighed", "email": "Zighed"}]}, "sig610": {"session": "Tutorial 4: Machine Learning on Big Data", "abstract": "Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. \n\nThe tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.\n\n", "title": "Machine Learning for Big Data", "authors": [{"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Tyson Condie", "email": "Condie"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Paul Mineiro", "email": "Mineiro"}, {"affiliation": "UC Santa Cruz", "location": "Santa Cruz CA USA ", "name": "Neoklis Polyzotis", "email": "Polyzotis"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Markus Weimer", "email": "Weimer"}]}, "sig460": {"session": "Data Streams", "abstract": "As users of \u0093big data\u0094 applications expect fresh results, we witness a new breed of stream processing systems (SPS) that  are designed to scale to large numbers of cloud-hosted machines. Such systems face new challenges: (i) to benefit from the \u0093pay-as-you-go\u0094 model of cloud computing, they must scale out on demand, acquiring additional virtual machines (VMs) and parallelising operators when the workload increases; (ii) failures are common with deployments on hundreds of VMs\u0097systems must be fault-tolerant with fast recovery times, yet low per-machine overheads. An open question is how to achieve these two goals when stream queries include stateful operators, which must be scaled out and recovered without affecting query results.\n\n\n\nOur key idea is to expose internal operator state explicitly to the SPS through a set of state management primitives. Based on them, we describe an integrated approach for dynamic scale out and recovery of stateful operators. Externalised operator state is checkpointed periodically by the SPS and backed up to upstream VMs. The SPS identifies individual operator bottlenecks and automatically scales them out by allocating new VMs and partitioning the checkpointed state. At any point, failed operators are recovered by restoring checkpointed state on a new VM and replaying unprocessed tuples. We evaluate this approach with the Linear Road Benchmark on the Amazon EC2 cloud platform and show that it can scale automatically to a load factor of L=350 with 50 VMs, while recovering quickly from failures.", "title": "Integrating Scale Out and Fault Tolerance in Stream Processing using Operator State Management", "authors": [{"affiliation": "Imperial College London", "location": "London  United Kingdom Department of Computing", "name": "Raul Castro Fernandez", "email": "Castro Fernandez"}, {"affiliation": "University of Kent", "location": "Canterbury  United Kingdom ", "name": "Matteo Migliavacca", "email": "Migliavacca"}, {"affiliation": "Imperial College London", "location": "London  United Kingdom Department of Computing", "name": "Evangelia Kalyvianaki", "email": "Kalyvianaki"}, {"affiliation": "Imperial College London", "location": "London  United Kingdom Department of Computing", "name": "Peter Pietzuch", "email": "Pietzuch"}]}, "sig462": {"session": "Demo 3: Database Optimization; Performance", "abstract": "Iterative algorithms occur in many domains of data analysis, such as machine learning or graph analysis. With increasing interest to run those algorithms on very large data sets, we see a need for new techniques to execute iterations in a massively parallel fashion. In prior work, we have shown how to extend and use a parallel data flow system to efficiently run iterative algorithms in a shared-nothing environment. Our approach supports the incremental processing nature of many of those algorithms.\n\n\n\nIn this demonstration proposal we illustrate the process of implementing, compiling, optimizing, and executing iterative algorithms on Stratosphere using examples from graph analysis and machine learning. For the first step, we show the algorithm's code and a visualization of the produced data flow programs. The second step shows the optimizer's execution plan choices, while the last phase monitors the execution of the program, visualizing the state of the operators and additional metrics, such as per-iteration runtime and number of updates.\n\n\n\nTo show that the data flow abstraction supports easy creation of custom programming APIs, we also present programs written against a lightweight Pregel API that is layered on top of our system with a small programming effort.", "title": "Iterative Parallel Data Processing with Stratosphere: An Inside Look", "authors": [{"affiliation": "Technische Universit\u00e4t Berlin", "location": "Berlin  Germany ", "name": "Stephan Ewen", "email": "Ewen"}, {"affiliation": "Technische Universit\u00e4t Berlin", "location": "Berlin  Germany ", "name": "Sebastian Schelter", "email": "Schelter"}, {"affiliation": "Technische Universit\u00e4t Berlin", "location": "Berlin  Germany ", "name": "Kostas Tzoumas", "email": "Tzoumas"}, {"affiliation": "International Computer Science Institute, Berkeley", "location": "Berkeley  USA ", "name": "Daniel Warneke", "email": "Warneke"}, {"affiliation": "Technische Universit\u00e4t Berlin", "location": "Berlin  Germany ", "name": "Volker Markl", "email": "Markl"}]}, "sig509": {"session": "Demo 1: Data Intensive Applications", "abstract": "This demo presents Noah: a dynamic ridesharing system. Noah supports large scale real-time ridesharing with service guarantee on road networks. Taxis and trip requests are dynamically matched. Different from traditional systems, a taxi can have more than one customer on board given that all waiting time and service time constraints of trips are satisfied. Noah's real-time response relies on three main components: (1) a fast shortest path algorithm with caching on road networks; (2) fast dynamic matching algorithms to schedule ridesharing on the fly; (3) a spatial indexing method for fast retrieving moving taxis. Users will be able to submit requests from a smartphone, choose specific parameters such as number of taxis in the system, service constraints, and matching algorithms, to explore the internal functionalities and implementations of Noah. The system analyzer will show the system performance including average waiting time, average detour percentage, average response time,  and average level of  sharing. Taxis, routes, and requests will be animated and visualized through Google Maps API. The demo is based on trips of 17,000 Shanghai taxis for one day (May 29, 2009); the dataset contains 432,327 trips. Each trip includes the starting and destination coordinates and the start time. An iPhone application is implemented to allow users to submit a trip request to the Noah system during the demonstration.", "title": "Noah: A Dynamic Ridesharing System", "authors": [{"affiliation": "University of North Texas", "location": "Denton TX USA ", "name": "Charls Tian", "email": "Tian"}, {"affiliation": "University of North Texas", "location": "Denton TX USA ", "name": "Yan Huang", "email": "Huang"}, {"affiliation": "University of North Texas", "location": "Denton TX USA ", "name": "Zhi Liu", "email": "Liu"}, {"affiliation": "Massachusetts Institute of Technology", "location": "Cambridge MA USA ", "name": "Favyen Bastani", "email": "Bastani"}, {"affiliation": "Kent State University", "location": "Kent OH USA ", "name": "Ruoming Jin", "email": "Jin"}]}, "sig615": {"session": "PTAbstract", "abstract": "With the rise of online social networks, studies have tried to confirm the six degrees of separation theory and confirmed the average distance to 4.67 degrees in Twitter and 3.74 degrees in Facebook. Even with so small distances, there is currently no tool to find paths between a given pair of users of those social networks in real time. Classical solutions include the Dijkstra algorithm, and the heuristics Best-First-Search and A*. However, such algorithms perform an extensive search in the graph, which is impractical due to the large size of the social graph and the access restrictions imposed by the social networks\u0092 owners.\n\nThis work introduces FriendRouter, a real-time path finder in Social networks. \n\n\n\nFriendRouter uses a bidirectional search algorithm based on A* with an heuristic (optimal path not guaranteed) to find short paths between two users. Its biggest contribution is its ability to dramatically minimize the access on the graph, making real-time path finding over online social networks possible. Its heuristic allows the algorithm to find short paths with are close to the average minimum separation, by choosing nodes with higher connectivity which are close to the destination node, thus directing the search to a local cluster where it can be completed with less effort. In our experimental analysis (based on Twitter), we show that FriendRouter finds paths between generic pairs of users in the network. Its advantages are emphasised: the search is optimised, expanding typically less than 40 nodes, which represents a performance gain of about 10^5 times if compared to the optimum Dijkstra algorithm. In conclusion, geographical data can be very useful as a heuristic for path finding in social networks. FriendRouter application is available online at http://friendrouter.com.", "title": "FriendRouter - Real-Time Path Finder in Social Networks", "authors": [{"affiliation": "UFMG", "location": "Belo Horizonte  Brazil ", "name": "Wladston Viana", "email": "Viana"}, {"affiliation": "UFMG", "location": "Belo Horizonte  Brazil ", "name": "Mirella Moro", "email": "Moro"}]}, "sig389": {"session": "Demo 3: Database Optimization; Performance", "abstract": "Anomaly detection is an important data mining task, aiming at the discovery of elements that show significant diversion from the expected behavior; such elements are termed as outliers. One of the most widely employed criteria for determining whether an element is an outlier is based on the number of neighboring elements within a fixed distance ($R$), against a fixed threshold ($k$).\n\nSuch outliers are referred to as {\\it distance-based} outliers and are the focus of this work. In this demo, we show both an extendible  framework for outlier detection algorithms and specific outlier detection algorithms for the demanding case where outlier detection is continuously performed over a data stream. More specifically: i) first we demonstrate a novel flavor of an open-source publicly available tool for Massive Online Analysis (MOA) that is endowed with capabilities to encapsulate algorithms that continuously detect outliers and ii) second, we present four online outlier detection algorithms. Two of these algorithms have been designed by the authors of this demo,\n\nwith a view to improving on key aspects related to outlier mining, such as running time, flexibility and space requirements.", "title": "Continuous Outlier Detection in Data Streams: An Extensible Framework and State-Of-The-Art Algorithms", "authors": [{"affiliation": "Aristotle University ", "location": "Thessaloniki  Greece ", "name": "Dimitrios Georgiadis", "email": "Georgiadis"}, {"affiliation": "Aristotle University ", "location": "Thessaloniki  Greece ", "name": "Maria Kontaki", "email": "Kontaki"}, {"affiliation": "Aristotle University ", "location": "Thessaloniki  Greece ", "name": "Anastasios Gounaris", "email": "Gounaris"}, {"affiliation": "Aristotle University ", "location": "Thessaloniki  Greece ", "name": "Apostolos Papadopoulos", "email": "Papadopoulos"}, {"affiliation": "Aristotle University", "location": "Thessaloniki  Greece ", "name": "Kostas Tsichlas", "email": "Tsichlas"}, {"affiliation": "Aristotle University", "location": "Thessaloniki  Greece ", "name": "Yannis Manolopoulos", "email": "Manolopoulos"}]}, "phdx08": {"session": "PT", "abstract": "We present RDF-4G, the first three miles towards a large-\n\nscale graph-analytics engine built on top of the state-of-the-art RDF engine, RDF-3X. The algorithmic building blocks that make up this work help answering fundamental questions about relationships between entities in a graph-structured world. More precisely, our system provides insights into what we define as the trilogy of relationship analyis: Is there a relationship between entities? Who participates in the connection? How can the relationship be characterized? While the first two questions correspond to the algorithmic primitives of graph processing, reachability and shortest path queries, for answering the third question we propose a novel graph-theoretic concept, relatedness cores. The technical contributions we make in this work are efficient index structures for reachability and shortest path query processing together with a new notion of and algorithms for relationship characterization. The latter can be efficiently computed based on the techniques we have developed in our work on graph indexing. All our methods are integrated into the RDF-3X engine, the state-of-the-art system for querying RDF-structured data. Future work includes the exposure of our algorithmic building blocks to the user, via extensions to the de-facto standard query language for graph-structured data, SPARQL.", "title": "RDF-4G: Algorithmic Building Blocks for Large-Scale Graph Analytics", "authors": [{"affiliation": "Max Planck Institute for Informatics", "location": "Saarbr\u00fccken  Germany ", "name": "Stephan Seufert", "email": "Seufert"}]}, "sig226": {"session": "Demo 1: Data Intensive Applications", "abstract": "We provide a description of Peckalytics,\n\nits technology and functionality. Peckalytics processes\n\nthe entire Twitter data stream in real time and provides\n\na flexible search interface to identify experts in any\n\ntopic area as well as users with interests in any\n\ntopic. It provides flexible analytics\n\naround sets of experts, their followers as well as\n\nsets of users with specific interests. The system\n\nis implemented to scale for large data sizes.\n\nAt the time of this writing it operates on an\n\narchive of 30 billion tweets,\n\nwith 220,000 new tweets crawled every minute.\n\nIn addition to raw tweets, the social graph of users, and profile information,\n\nPeckalytics makes novel use of Twitter lists to assess the expertise\n\nof different users.\n\nOur aim is to facilitate targeting\n\nand optimization of advertising campaigns on the Twitter platform.\n\n", "title": "Peckalytics: Analyzing Experts and Interests on Twitter", "authors": [{"affiliation": "University of Toronto", "location": "Toronto ON Canada Computer Science", "name": "Alex Cheng", "email": "Cheng"}, {"affiliation": "University of Toronto", "location": "Toronto ON Canada Computer Science", "name": "Nilesh Bansal", "email": "Bansal"}, {"affiliation": "University of Toronto", "location": "Toronto ON Canada Computer Science", "name": "Nick Koudas", "email": "Koudas"}]}, "phdg07": {"session": "PT", "abstract": "The hardware landscape is getting increasingly diverse. A modern desktop computer can contain multiple different processing architectures like multi-core CPUs or GPUs. This diversity is expected to grow significantly in the next ten years, with micro-architectures themselves diverging towards highly parallel and heterogeneous designs. We believe that preparing database systems to exploit this diverse landscape of processing architectures will be one of the major challenges for the coming decade in database research.\n\n\n\nIn this paper, we present our thoughts and results on modifying the components of a database system to efficiently use modern processing architectures. In particular, we discuss our work on offloading parts of the Query Optimizer to highly parallel processors such as graphics cards, and present our work on designing a hardware-oblivious Execution Engine that can run unchanged on a multitude of different processing architectures.", "title": "Designing a Database System for Modern Processing Architectures", "authors": [{"affiliation": "Technische Universit\u00e4t Berlin", "location": "Berlin  Germany ", "name": "Max Heimel", "email": "Heimel"}]}, "phdg06": {"session": "PT", "abstract": "Web applications store their data within various database models, such as relational, semi-structured, and graph data models to name a few. We study learning algorithms for queries for the above mentioned models. As a further goal, we aim to apply the results to learning cross-model database mappings, which can also be seen as queries across different schemas.", "title": "Learning Queries for Relational, Semi-structured, and Graph Databases", "authors": [{"affiliation": "University of Lille & INRIA", "location": "Lille  France ", "name": "Radu Ciucanu", "email": "Ciucanu"}]}, "sig384": {"session": "Demo 1: Data Intensive Applications", "abstract": "We present our system towards live city, called TsingNUS, aiming to provide users with more user-friendly location-aware search experiences. TsingNUS crawls location-based user-generated content from the Web (e.g., Foursquare and Twitter), cleans and integrates them to provide users with rich well-structured data. TsingNUS provides three user-friendly search paradigms: location-aware instant search, location-aware similarity search and direction-aware search. Instant search returns relevant answers instantly as users type in queries letter by letter, which can help users to save typing efforts significantly. Location-aware similarity search enables fuzzy matching between queries and the underlying data, which can tolerate typing errors. The two features boost the search performance and improve the experiences for mobile users who often misspell the keywords due to the limitation of the mobile phone's keyboard. In addition, users have direction-aware search requirements in many applications. For example, a driver on the highway wants to find the nearest gas station or restaurant. She has a search requirement that the answers should be in front of her driving direction. TsingNUS enables direction-aware search to address this problem and allows users to search in specific directions. Moreover, TsingNUS incorporates continuous search to efficiently support continuously moving queries in a client-server system which can reduce the number of queries submitted to the server and communication cost between the client and server. We have implemented and deployed a system which has been commonly used and widely accepted. \n\n", "title": "TsingNUS: A Location-Based Service System Towards Live City", "authors": [{"affiliation": "Tsinghua University", "location": "Beijing  China ", "name": "Guoliang Li", "email": "Li"}, {"affiliation": "Tsinghua University", "location": "Beijing  China ", "name": "Nan Zhang", "email": "Zhang"}, {"affiliation": "Tsinghua University", "location": "Beijing  China ", "name": "Ruicheng Zhong", "email": "Zhong"}, {"affiliation": "Tsinghua University", "location": "Beijing  China ", "name": "Sitong Liu", "email": "Liu"}, {"affiliation": "Tsinghua University", "location": "Beijing  China ", "name": "Weihuang Huang", "email": "Huang"}, {"affiliation": "NUS", "location": "Singapore  Singapore ", "name": "Ju Fan", "email": "Fan"}, {"affiliation": "NUS", "location": "Singapore  Singapore ", "name": "Kian-Lee Tan", "email": "Tan"}, {"affiliation": "Tsinghua University", "location": "Beijing  China ", "name": "Lizhu Zhou", "email": "Zhou"}, {"affiliation": "Tsinghua University", "location": "Beijing  China ", "name": "Jianhua Feng", "email": "Feng"}]}, "pods002": {"session": "PODS Tutorial 1", "abstract": "", "title": "Sketching via Hashing: from Heavy Hitters to Compressed Sensing to Sparse Fourier Transform", "authors": [{"affiliation": "MIT", "location": "Cambridge MA USA EECS", "name": "Piotr Indyk", "email": "Indyk"}]}, "pods001": {"session": "PODS Tutorial 2", "abstract": "Graph databases have gained renewed interest in the\n\nlast years, due to its applications in areas such as the Semantic Web\n\nand Social Networks Analysis. We study the problem of querying graph\n\n databases, and, in particular, the expressiveness and complexity of\n\n evaluation for several general-purpose query languages, such as the\n\n regular path queries and its extensions with conjunctions\n\n and inverses. We distinguish between two semantics for these\n\nlanguages. The first one, based on simple paths, easily leads to\n\nintractability, while the second one, based on arbitrary paths, allows\n\ntractable evaluation for an expressive family of languages. \n\n\n\nWe also study two recent extensions of these languages \n\nthat have been motivated by modern applications of graph databases. \n\n The first one allows to treat paths as first-class citizens, while\n\n the second one permits to express queries that combine the topology\n\n of the graph with its underlying data.  ", "title": "Querying Graph Databases", "authors": [{"affiliation": "University of Chile", "location": "Santiago  Chile ", "name": "Pablo Barcel\u00f3 Baeza", "email": "Barcel\u00f3 Baeza"}]}, "pods083": {"session": "Query processing/Verification", "abstract": "We introduce monadically defined queries (MODEQs) and nested monadically defined queries (NEMODEQs), two querying formalisms that extend conjunctive queries, conjunctive two-way regular path queries, and monadic Datalog queries. Both can be expressed as Datalog queries and in monadic second-order logic, yet they have a decidable query containment problem and favorable query answering complexities: a data complexity of P, and a combined complexity of NP (MODEQs) and PSpace (NEMODEQs).\n\nWe show that (NE)MODEQ answering remains decidable in the presence of a well-known generic class of tuple-generating dependencies. In addition, techniques to rewrite queries under dependencies into (NE)MODEQs are introduced. Rewriting can be applied partially, and (NE)MODEQ answering is still decidable if the non-rewritable part of the TGDs permits decidable (NE)MODEQ answering on other grounds.", "title": "Flag & Check: Data Access with Monadically Defined Queries", "authors": [{"affiliation": "Technische Universit\u00e4t Dresden", "location": "Dresden  Germany Fakult\u00e4t Informatik", "name": "Sebastian Rudolph", "email": "Rudolph"}, {"affiliation": "University of Oxford", "location": "Oxford  United Kingdom Department of Computer Science", "name": "Markus Kr\u00f6tzsch", "email": "Kr\u00f6tzsch"}]}, "sig618": {"session": "Graph Management", "abstract": "Given a query graph $q$ and a data graph $g$, the subgraph isomorphism search finds all occurrences of $q$ in $g$ and is considered one of the most fundamental query types for many real applications. While this problem belongs to NP-hard, many algorithms have been proposed to solve it in a reasonable time for real datasets. However, a recent study has shown, through an extensive benchmark with various real datasets, that all existing algorithms have serious problems in their matching order selection. Furthermore, all algorithms blindly permutate all possible mappings for query vertices, often leading to useless computations. In this paper, we present an efficient and robust subgraph search solution, called \\textsf{Turbo$_{\\mbox{\\tiny ISO}}$}, which is turbo-charged with two novel concepts, \\emph{candidate region exploration} and the \\emph{combine and permute} strategy (in short, \\textsc{Comb/Perm}). The candidate region exploration identifies on-the-fly candidate subgraphs (i.e, candidate regions), which contain  embeddings, and computes a robust matching order for each candidate region explored. The \\textsc{Comb/Perm} strategy exploits the novel concept of the \\emph{neighborhood equivalence class} (NEC). Each query vertex in the same NEC has identically matching data vertices. During subgraph isomorphism search, \\textsc{Comb/Perm} generates only combinations for each NEC instead of permutating all possible enumerations. Thus, if a chosen combination is determined to not contribute to a complete solution, all possible permutations for that combination will be safely pruned. Extensive experiments with many real datasets show that \\textsf{Turbo$_{\\mbox{\\tiny ISO}}$} consistently and significantly outperforms all competitors by up to several orders of magnitude.", "title": "Turbo<sub>ISO</sub>: Towards UltraFast and Robust Subgraph Isomorphism Search in Large Graph Databases", "authors": [{"affiliation": "Kyungpook National University", "location": "Daegu  South Korea School of Computer Science and Engineering", "name": "Wook-Shin Han", "email": "Han"}, {"affiliation": "Kyungpook National University", "location": "Daegu  South Korea School of Computer Science and Engineering", "name": "Jinsoo Lee", "email": "Lee"}, {"affiliation": "Kyungpook National University", "location": "Daegu  South Korea School of Computer Science and Engineering", "name": "Jeong-Hoon Lee", "email": "Lee"}]}, "sig619": {"session": "Social Media", "abstract": "The ubiquity of mobile devices and the popularity of location-based-services have generated, for the first time, rich datasets of people's location information at a very high fidelity.  These location datasets can be used to study people's behavior - for example, social studies have shown that people, who are seen together frequently at the same place and at the same time, are most probably socially related.  In this paper, we are interested in inferring these social connections by analyzing people's location information, which is useful in a variety of application domains from sales and marketing to intelligence analysis.  In particular, we propose an entropy-based model (EBM) that not only infers social connections but also estimates the strength of social connections by analyzing people's co-occurrences in space and time. We examine two independent ways: diversity and weighted frequency, through which co-occurrences contribute to social strength. In addition, we take the characteristics of each location into consideration in order to compensate for cases where only limited location information is available. We conducted extensive sets of experiments with real-world datasets including both people's location data and their social connections, where we used the latter as the ground-truth to verify the results of applying our approach to the former. We show that our approach outperforms the competitors.", "title": "EBM - An Entropy-Based Model to Infer Social Strength from Spatiotemporal Data", "authors": [{"affiliation": "University of Southern California", "location": "Los Angeles CA USA Computer Science Department", "name": "Huy Pham", "email": "Pham"}, {"affiliation": "University of Southern California", "location": "Los Angeles CA USA Computer Science Department", "name": "Cyrus Shahabi", "email": "Shahabi"}, {"affiliation": "University of Southern California", "location": "Los Angeles CA USA Computer Science Department", "name": "Yan Liu", "email": "Liu"}]}, "sig617": {"session": "Data Analytics", "abstract": "Recently, massively parallel processing relational database systems (MPPDBs) have gained much momentum in the big data analytic market. \n\nWith the advent of hosted cloud computing, we envision that the offering of MPPDB-as-a-Service (MPPDBaaS) will become attractive for companies having analytical tasks \n\non only hundreds gigabytes to some ten terabytes of data\n\nbecause they can enjoy high-end parallel analytics at a cheap cost.\n\nThis paper presents \\emph{Thrifty}, \n\na prototype implementation of MPPDB-as-a-service.\n\nThe major research issue is how to \n\nachieve a lower total cost of ownership by\n\nconsolidating thousands of MPPDB tenants on to a shared hardware infrastructure,\n\nwith a performance SLA that guarantees the tenants can obtain the query results as if they are executing their queries on dedicated machines.\n\nThrifty achieves the goal by using a \\emph{tenant-driven design}\n\nthat includes (1) a \\emph{cluster design} that carefully arranges the nodes in the cluster into groups and creates an MPPDB for each group of nodes, \n\n(2) a \\emph{tenant placement} that assigns each tenant to several MPPDBs (for high availability service through replication),\n\nand (3) a \\emph{query routing} algorithm that routes a tenant's query to the proper MPPDB at run-time.\n\nExperiments show that in a MPPDBaaS with 5000 tenants,\n\nwhere each tenant requests 2 to 32 nodes MPPDB to query against 200GB to {3.2TB} of data,\n\nThrifty can serve all the tenants with a 99.9\\% performance SLA guarantee and a high availability replication factor of 3,\n\nusing only {18.7\\%} of  the nodes requested by the tenants.\n\n", "title": "Parallel Analytics as a Service", "authors": [{"affiliation": "The Hong Kong Polytechnic University", "location": "Hong Kong  Hong Kong Department of Computing", "name": "Petrie Wong", "email": "Wong"}, {"affiliation": "The Hong Kong Polytechnic University", "location": "Hong Kong  Hong Kong Department of Computing", "name": "Zhian He", "email": "He"}, {"affiliation": "The Hong Kong Polytechnic University", "location": "Hong Kong  Hong Kong ", "name": "Eric Lo", "email": "Lo"}]}, "sig053": {"session": "Transactions", "abstract": "Modern implementations of DBMS software are intended to take advantage of high core counts that are becoming common in high-end servers. However, we have observed that several database platforms, including MySQL, Shore-MT, and a commercial system, exhibit throughput collapse as load increases, even for a workload with little or no logical contention for locks. Our analysis  of MySQL identifies latch contention within the lock manager as the bottleneck responsible for this collapse.  We design a lock manager with reduced latching, implement it in MySQL, and show that it avoids the collapse and generally improves performance. Our efficient implementation of a lock manager is enabled by a staged allocation and de-allocation of locks. Locks are pre-allocated in bulk, so that the lock manager only has to perform simple list-manipulation operations during the acquire and release phases of a transaction. De-allocation of the lock data-structures is also performed in bulk, which enables the use of fast implementations of lock acquisition and release, as well as concurrent deadlock checking.", "title": "A Scalable Lock Manager for Multicores", "authors": [{"affiliation": "NICTA", "location": "Sydney  Australia ", "name": "Hyungsoo Jung", "email": "Jung"}, {"affiliation": "Samsung Electronics", "location": "Hwasung  South Korea ", "name": "Hyuck Han", "email": "Han"}, {"affiliation": "University of Sydney", "location": "Sydney  Australia School of Information Technologies", "name": "Alan Fekete", "email": "Fekete"}, {"affiliation": "NICTA and UNSW", "location": "Sydney  Australia ", "name": "Gernot Heiser", "email": "Heiser"}, {"affiliation": "Seoul National University", "location": "Seoul  South Korea Department of Computer Science and Engineering", "name": "Heon Yeom", "email": "Yeom"}]}, "sig591": {"session": "Industrial 1: Big Data I", "abstract": "Espresso is a document-oriented distributed data serving platform that has been built to address LinkedIn\u0092s requirements for a scalable, performant, source-of-truth primary store. It provides a hierarchical document model, transactional support for modifications to related documents, real-time secondary indexing, on-the-fly schema evolution and provides a timeline consistent change capture stream. This paper describes the motivation and design principles involved in building Espresso, the data model and capabilities ex- posed to clients, details of the replication and secondary indexing implementation and presents a set of experimental results that characterize the performance of the system along various dimensions.\n\nWhen we set out to build Espresso, we chose to apply best practices in industry, already published works in research and our own internal experience with different consistency models. Along the way, we built a novel generic distributed cluster management framework, a partition-aware change- capture pipeline and a high-performance inverted index implementation.", "title": "On Brewing Fresh Espresso: LinkedIn\u0092s Distributed Data Serving Platform", "authors": [{"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Lin Qiao", "email": "Qiao"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Kapil Surlaker", "email": "Surlaker"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Shirshanka Das", "email": "Das"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Tom Quiggle", "email": "Quiggle"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Bob Schulman", "email": "Schulman"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Bhaskar Ghosh", "email": "Ghosh"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Antony Curtis", "email": "Curtis"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Oliver Seeliger", "email": "Seeliger"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Zhen Zhang", "email": "Zhang"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Aditya Auradar", "email": "Auradar"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Chris Beaver", "email": "Beaver"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Gregory Brandt", "email": "Brandt"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Mihir Gandhi", "email": "Gandhi"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Kishore Gopalakrishna", "email": "Gopalakrishna"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Wai Ip", "email": "Ip"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Swaroop Jgadish", "email": "Jgadish"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Shi Lu", "email": "Lu"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Alexander Pachev", "email": "Pachev"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Aditya Ramesh", "email": "Ramesh"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Abraham Sebastian", "email": "Sebastian"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Rupa Shanbhag", "email": "Shanbhag"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Subbu Subramaniam", "email": "Subramaniam"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Yun Sun", "email": "Sun"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Sajid Topiwala", "email": "Topiwala"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Cuong Tran", "email": "Tran"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "Jemiah Westerman", "email": "Westerman"}, {"affiliation": "LinkedIn, Inc", "location": "Mountain View  USA ", "name": "David Zhang", "email": "Zhang"}]}, "pods089": {"session": "Awards Session", "abstract": "We describe a general framework for static verification of systems that base their decisions upon queries to databases. The database is specified using constraints, typically a schema, and is not modified during a run of the system. The system is equipped with a finite number of registers for storing intermediate information from the database and the specification consists of a transition table described using quantifier-free formulas that can query either the database or the registers.\n\n \n\nOur main result concerns systems querying XML databases -- modeled as data trees -- using quantifier-free formulas with predicates such as the descendant axis or comparison of data values.  In this scenario we show an ExpSpace algorithm for deciding reachability.\n\n  \n\nOur technique is based on the notion of amalgamation and is quite general. For instance it also applies to relational databases (with an optimal PSpace algorithm). We also show that minor extensions of the model lead to undecidability.", "title": "Verification of Database-driven Systems via Amalgamation", "authors": [{"affiliation": "University of Warsaw", "location": "Warsaw  Poland ", "name": "Miko?aj Boja?czyk", "email": "Boja?czyk"}, {"affiliation": "INRIA and ENS Cachan", "location": "Cachan  France ", "name": "Luc Segoufin", "email": "Segoufin"}, {"affiliation": "University of Warsaw", "location": "Warsaw  Poland ", "name": "Szymon Toru?czyk", "email": "Toru?czyk"}]}, "sig611": {"session": "Tutorial 1: Rethinking Eventual Consistency", "abstract": "There has been a resurgence of work on replicated, distributed database systems to meet the demands of intermittently-connected clients and of disaster-tolerant databases that span data centers. Many systems weaken the criteria for replica-consistency or isolation, and in some cases add new mechanisms, to improve partition-tolerance, availability, and performance. We present a framework for comparing these criteria and mechanisms, to help architects navigate through this complex design space.", "title": "Rethinking Eventual Consistency", "authors": [{"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Philip Bernstein", "email": "Bernstein"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Sudipto Das", "email": "Das"}]}, "sig511": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "However, much of the prior work on those topics has been restricted to static networks, a primary reason being the lack of efficient temporal data management systems to store and query large dynamic network datasets. In this demonstration proposal, we present HiNGE (Historical Network/Graph Explorer), a system that enables interactive exploration and analytics over large evolving networks through visualization and node-centric metric computations. HiNGE is built on top of a distributed graph database system that stores the entire history of a network, and enables efficiently retrieving and analyzing multiple graph snapshots from arbitrary time points in the past. The cornerstone of our system is a novel hierarchical parallelizable index structure, called DeltaGraph, that enables compact recording of the historical trace of a network on disk, and supports efficient retrieval of historical snapshots for single-site or parallel processing. The other key component of our system is an in-memory graph data structure, called GraphPool, that can maintain hundreds of historical graph snapshots in main memory in a non-redundant manner. We demonstrate the efficient and usability of our system at performing temporal analytics over large-scale dynamic networks.", "title": "HiNGE : Enabling Temporal Network Analytics at Scale", "authors": [{"affiliation": "University of Maryland", "location": "College Park MD USA Computer Science Department", "name": "Udayan Khurana", "email": "Khurana"}, {"affiliation": "University of Maryland", "location": "College Park MD USA Computer Science Department", "name": "Amol Deshpande", "email": "Deshpande"}]}, "sig510": {"session": "Information Extraction", "abstract": "Dictionaries of terms and phrases (e.g. common person or \n\norganization names) are integral to information extraction \n\nsystems that extract structured information from \n\nunstructured text. Using noisy or unrefined dictionaries \n\nmay lead to many incorrect results even when highly precise \n\nand sophisticated extraction rules are used. In general, \n\nthe results of the system are dependent on dictionary \n\nentries in arbitrary complex ways, and removal of a set of \n\nentries can remove both correct and incorrect results.\n\nFurther, any such refinement critically requires laborious \n\nmanual labeling of the results.\n\n\n\nIn this paper, we study the dictionary refinement problem\n\nand address the above challenges. Using provenance of the\n\noutputs in terms of the dictionary entries, we formalize an \n\noptimization problem of maximizing the quality of the \n\nsystem with respect to the refined dictionaries, study \n\ncomplexity of this problem, and give efficient algorithms.\n\nWe also propose solutions to address incomplete labeling \n\nof the results where we estimate the missing labels \n\nassuming a statistical model. We conclude with a detailed \n\nexperimental evaluation using several real-world extractors \n\nand competition datasets to validate our solutions. Beyond \n\ninformation extraction, our provenance-based techniques and \n\nsolutions may find applications in view-maintenance in \n\ngeneral relational settings.", "title": "Provenance-based Dictionary Refinement in Information Extraction", "authors": [{"affiliation": "University of Washington", "location": "Seattle WA USA Computer Science and Engineering", "name": "Sudeepa Roy", "email": "Roy"}, {"affiliation": "IBM Research-Almaden", "location": "San Jose CA USA ", "name": "Laura Chiticariu", "email": "Chiticariu"}, {"affiliation": "IBM Research-Almaden", "location": "San Jose CA USA ", "name": "Vitaly Feldman", "email": "Feldman"}, {"affiliation": "IBM Research-Almaden", "location": "San Jose CA USA ", "name": "Frederick Reiss", "email": "Reiss"}, {"affiliation": "IBM Research-Almaden", "location": "San Jose CA USA ", "name": "Huaiyu Zhu", "email": "Zhu"}]}, "sig192": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "The World Wide Web (WWW) currently evolves into a Web of Linked Data where content providers publish and link their data as they have done with hypertext for the last 20 years. We understand this emerging dataspace as a huge, distributed database which is -at best- partially known to query execution systems. To tap the full potential of the Web, such a system must be able to answer a query using data from initially unknown data sources. For this purpose, traditional query execution paradigms are unsuitable because those assume a fixed set of potentially relevant data sources beforehand.\n\n\n\nWe demonstrate the query execution system SQUIN which implements a novel query execution approach. The main idea is to integrate the traversal of data links into the result construction process. This approach allows the execution engine to discover potentially relevant data during the query execution.\n\n\n\nIn our demonstration, attendees can query the Web of Linked Data using SQUIN and, thus, learn about the new query execution approach. Furthermore, attendees can experience the suitability of the approach for Web applications by using a simple, Linked Data based mash-up implemented on top of SQUIN.", "title": "SQUIN: A Traversal Based Query Execution System for the Web of Linked Data", "authors": [{"affiliation": "University of Waterloo", "location": "Waterloo ON Canada David R. Cheriton School of Computer Science", "name": "Olaf Hartig", "email": "Hartig"}]}, "sig195": {"session": "Demo 3: Database Optimization; Performance", "abstract": "Unanticipated load spikes or skewed data access patterns may lead to severe performance degradation in data serving applications, a typical problem of distributed NoSQL data-stores. In these cases, load balancing is a necessary operation. In this demonstration, we present the DBalancer, a generic distributed module that can be installed on top of a typical NoSQL data-store and provide an efficient and highly configurable load balancing mechanism. Balancing is performed by simple message exchanges and typical data movement operations supported by most modern NoSQL data-stores. We present the system's architecture, we describe in detail its modules and their interaction and we implement a suite of different algorithms on top of it. Through a web-based interactive GUI we allow the users to launch NoSQL clusters of various sizes, to apply numerous skewed and dynamic workloads and to compare the implemented load balancing algorithms. Videos and graphs showcasing each algorithm's effect on a number of indicative performance and cost metrics will be created on the fly for every setup. By browsing the results of different executions users will be able to grasp each algorithm's balancing mechanisms and performance impact in a number of representative setups.", "title": "DBalancer: Distributed Load Balancing for NoSQL Data-stores", "authors": [{"affiliation": "CSLAB, National Technical University of Athens", "location": "Athens  Greece ", "name": "Ioannis Konstantinou", "email": "Konstantinou"}, {"affiliation": "Department of Informatics Ionian University", "location": "Corfu  Greece ", "name": "Dimitrios Tsoumakos", "email": "Tsoumakos"}, {"affiliation": "CSLAB, National Technical University of Athens", "location": "Athens  Greece ", "name": "Ioannis Mytilinis", "email": "Mytilinis"}, {"affiliation": "CSLAB, National Technical University of Athens", "location": "Athens  Greece ", "name": "Nectarios Koziris", "email": "Koziris"}]}, "sig271": {"session": "Schema Matching and Spatial Databases I", "abstract": "Users often need to gather information about ``entities'' of interest. Recent efforts\n\ntry to automate this task by leveraging the vast corpus of HTML tables; this is referred to as ``entity augmentation''.\n\nThe accuracy of entity augmentation critically depends on semantic relationships between web tables as well as\n\nsemantic labels of those tables.\n\nCurrent techniques work well for string-valued and static attributes but\n\nperform poorly for numeric and time-varying attributes.\n\n\n\nIn this paper, we first build a semantic graph that (i) labels columns with unit, scale and timestamp\n\ninformation and (ii) computes semantic matches between columns even when the same numeric attribute\n\nis expressed in different units or scales.\n\nSecond, we develop a novel entity augmentation API suited for numeric and time-varying attributes\n\nthat leverages the semantic graph.\n\nBuilding the graph is challenging as such label information is often missing from the column headers.\n\nOur key insight is to\n\nleverage the wealth of tables on the web and\n\ninfer label information from semantically matching columns of other web tables;\n\nthis complements ``local'' extraction from column headers.\n\nHowever, this creates an interdependence between labels and semantic matches;\n\nwe address this challenge by representing the task as a probabilistic graphical model that jointly discovers labels and semantic matches over all columns.\n\nOur experiments on real-life datasets show that (i) our semantic graph contains higher quality labels and semantic matches\n\nand (ii) entity augmentation based on the above graph has significantly higher precision and recall\n\ncompared with the state-of-the-art.", "title": "InfoGather+: Semantic Matching and Annotation of Numeric and Time-Varying Attributes in Web Tables", "authors": [{"affiliation": "National University of Singapore", "location": "Singapore  Singapore ", "name": "Meihui Zhang", "email": "Zhang"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Kaushik Chakrabarti", "email": "Chakrabarti"}]}, "sig641": {"session": "Systems, Performance I", "abstract": "Recently, parallel search engines have been implemented based on scalable distributed file systems such as Google File System. However, we claim that building a massively-parallel search engine using a parallel DBMS can be an attractive alternative since it supports a higher-level (i.e., SQL-level) interface than that of a distributed file system for easy and less error-prone application development while providing scalability. Regarding higher-level functionality, we can draw a parallel with the traditional O/S file system vs. DBMS. In this paper, we propose a new approach of building a massively-parallel search engine using a DB-IR tightly-integrated parallel DBMS. To estimate the performance, we propose a hybrid (i.e., analytic and experimental) performance model for the parallel search engine. We argue that the model can accurately estimate the performance of a massively-parallel (e.g., 300-node) search engine using the experimental results obtained from a small-scale (e.g., 5-node) one. We show that the estimation error between the model and the actual experiment is less than 2.13\\% by observing that the bulk of the query processing time is spent at the slave (vs. at the master and network) and by estimating the time spent at the slave based on actual measurement. Using our model, we demonstrate a commercial-level scalability and performance of our architecture. Our proposed system ODYS is capable of handling 1 billion queries per day (81 queries/sec) for 30 billion Web pages by using only 43,472 nodes with an average query response time of 194 ms. By using twice as many (86,944) nodes, ODYS can provide an average query response time of 148 ms. These results show that building a massively-parallel search engine using a parallel DBMS is a viable approach with advantages of supporting the high-level (i.e., DBMS-level), SQL-like programming interface.", "title": "ODYS: An Approach to Building a Massively-Parallel Search Engine Using a DB-IR Tightly-Integrated Parallel DBMS for Higher-Level Functionality", "authors": [{"affiliation": "Korea Advanced Institute of Science and Technology (KAIST)", "location": "Daejeon  South Korea Department of Computer Science", "name": "Kyu-Young Whang", "email": "Whang"}, {"affiliation": "Korea Advanced Institute of Science and Technology (KAIST)", "location": "Daejeon  South Korea Department of Computer Science", "name": "Tae-Seob Yun", "email": "Yun"}, {"affiliation": "Korea Advanced Institute of Science and Technology (KAIST)", "location": "Daejeon  South Korea Department of Computer Science", "name": "Yeon-Mi Yeo", "email": "Yeo"}, {"affiliation": "Drexel University", "location": "Philadelphia  USA College of Information Science and Technology", "name": "Il-Yeol Song", "email": "Song"}, {"affiliation": "Korea Advanced Institute of Science and Technology (KAIST)", "location": "Daejeon  South Korea Department of Computer Science", "name": "Hyuk-Yoon Kwon", "email": "Kwon"}, {"affiliation": "Korea Advanced Institute of Science and Technology (KAIST)", "location": "Daejeon  South Korea Department of Computer Science", "name": "In-Joong Kim", "email": "Kim"}]}, "sig045": {"session": "Industrial 2: Enterprise Data management", "abstract": "SQL Server 2012 introduced two innovations targeted for data warehousing workloads: column store indexes and batch (vectorized) processing mode. Together they greatly improve performance of typical data warehouse queries, routinely by 10X and in some cases by a 100X or more. The main limitations of the initial version are addressed in the upcoming release. Column store indexes are updatable and can be used as the base storage for a table. The repertoire of batch mode operators has been expanded, existing operators have been improved, and query optimization has been enhanced. This paper gives an overview of SQL Server\u0092s column stores and batch processing, in particular the enhancements introduced in the upcoming release.", "title": "Enhancements to SQL Server Column Stores", "authors": [{"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Per-Ake Larson", "email": "Larson"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Cipri Clinciu", "email": "Clinciu"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Campbell Fraser", "email": "Fraser"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Eric Hanson", "email": "Hanson"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Mostafa Mokhtar", "email": "Mokhtar"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Michal Nowakiewicz", "email": "Nowakiewicz"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Vassilis Papadimos", "email": "Papadimos"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Susan Price", "email": "Price"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Srikumar Rangarajan", "email": "Rangarajan"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Remus Rusanu", "email": "Rusanu"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Mayukh Saubhasik", "email": "Saubhasik"}]}, "pods019": {"session": "Indexing/Query Answering", "abstract": "Nearest-neighbor (NN) search, which returns the nearest neighbor of a query point in a set of points, is an important and widely studied problem in many fields, and it has wide range of applications. In many of them, such as sensor databases, location-based services, face recognition, and mobile data, the location of data is imprecise. We therefore study nearest neighbor queries in a probabilistic framework in which the location of each input point is specified as a probability distribution function. We present efficient algorithms for (i) computing all points that are nearest neighbors of a query point with nonzero probability; (ii) estimating, within a specified additive error, the probability of a point being the nearest neighbor of a query point; (iii) using it to return the point that maximizes the probability being the nearest neighbor, or all the points with probabilities greater than some threshold to be the NN. We also present some experimental results to demonstrate the effectiveness of our approach.", "title": "Nearest Neighbor Searching Under Uncertainty II", "authors": [{"affiliation": "Duke University", "location": "Durham, NC  USA ", "name": "Pankaj Agarwal", "email": "Agarwal"}, {"affiliation": "Polytechnic Institute of NYU", "location": "Brooklyn, New York  USA ", "name": "Boris Aronov", "email": "Aronov"}, {"affiliation": "University of Illinois", "location": "Urbana, IL   USA ", "name": "Sariel Har-Peled", "email": "Har-Peled"}, {"affiliation": "University of Utah", "location": "Salt Lake City, UT  USA ", "name": "Jeff Phillips", "email": "Phillips"}, {"affiliation": "Hong Kong University of Science and Technology", "location": "Hong Kong, China  China ", "name": "Ke Yi", "email": "Yi"}, {"affiliation": "Duke University", "location": "Durham, NC  USA ", "name": "Wuzhou Zhang", "email": "Zhang"}]}, "phdx11": {"session": "PT", "abstract": "  Disambiguating named entities in natural language texts maps ambiguous names to canonical entities registered in a knowledge base such as DBpedia, Freebase, or YAGO. Knowing the specific entity is an important asset for several other tasks, e.g. entity-based information retrieval or higher-level information extraction. Our approach to named entity disambiguation makes use of several ingredients: the prior probability of an entity being mentioned, the similarity between the context of the mention in the text and an entity, as well as the coherence among the entities. Extending this method, we present a novel and highly efficient measure to compute the semantic coherence between entities. This measure is especially powerful for long-tail entities or  such entities that are not yet present in the knowledge base. Reliably identifying names in the  input text that are not part of the knowledge base is the current focus of our work.", "title": "Discovering and Disambiguating Named Entities in Text", "authors": [{"affiliation": "Max Planck Institute for Informatics", "location": "Saarbr\u00fccken  Germany ", "name": "Johannes Hoffart", "email": "Hoffart"}]}, "pods015": {"session": "PODS Tutorial 2", "abstract": "An uncertain database is defined as a relational database in which primary keys need not be satisfied. A repair (or possible world) of such database is obtained by selecting a maximal number of tuples without ever selecting two distinct tuples with the same primary key value. For a Boolean query q, the decision problem CERTAINTY(q) takes as input an uncertain database db and asks whether q is satisfied by every repair of db. Our main focus is on acyclic Boolean conjunctive queries without self-join. Previous work has introduced the notion of (directed) attack graph of such queries, and has proved that CERTAINTY(q) is first-order expressible if and only if the attack graph of q is acyclic. The current paper investigates the boundary between tractability and intractability of CERTAINTY(q). We first classify cycles in attack graphs as either weak or strong, and then prove among others the following. If the attack graph of a query q contains a strong cycle, then CERTAINTY(q) is coNP-complete. If the attack graph of q contains no strong cycle and every weak cycle is terminal (i.e., no edge leads from a vertex in the cycle to a vertex outside the cycle), then CERTAINTY(q) is in P. We then partially address the only remaining open case, i.e., when the attack graph contains some nonterminal cycle and no strong cycle. Finally, we establish a relationship between the complexities of CERTAINTY(q) and evaluating q on probabilistic databases.\n\n", "title": "Charting the Tractability Frontier of Certain Conjunctive Query Answering", "authors": [{"affiliation": "Universit\u00e9 de Mons (UMONS)", "location": "Mons  Belgium ", "name": "Jef Wijsen", "email": "Wijsen"}]}, "pods016": {"session": "Query processing/Verification", "abstract": "The SQL standard offers three primitive operations (insert,\n\ndelete, and update which is here called modify) to update a\n\nrelation based on a generic query.  This paper compares the\n\nexpressiveness of programs composed of these three operations,\n\nwith the general notion of update that simply replaces the\n\ncontent of the relation by the result of a query.  It turns out\n\nthat replacing cannot be expressed in terms of insertions,\n\ndeletions, and modifications, and neither can modifications be\n\nexpressed in terms of insertions and deletions.  The expressive\n\npower gained by if-then-else control flow in programs is\n\ninvestigated as well.  Different ways to perform replacing are\n\ndiscussed: using a temporary variable; using the new SQL merge\n\noperation; using SQL's data change delta tables; or using\n\nqueries involving object creation or arithmetic.  Finally the\n\npaper investigates the power of alternating the different\n\nprimitives.  For example, an insertion followed by a modification\n\ncannot always be expressed as a modification followed by an\n\ninsertion.\n\n", "title": "On the Expressive Power of Update Primitives", "authors": [{"affiliation": "Hasselt University and transnational University of Limburg", "location": "Hasselt  Belgium ", "name": "Tom Ameloot", "email": "Ameloot"}, {"affiliation": "Hasselt University and transnational University of Limburg", "location": "Hasselt  Belgium ", "name": "Jan Van den Bussche", "email": "Van den Bussche"}, {"affiliation": "Universit\u00e9 Paris-Sud", "location": "Orsay  France ", "name": "Emmanuel Waller", "email": "Waller"}]}, "phdx14": {"session": "PT", "abstract": "In this paper, we propose efficient algorithms for result diversification over indexed multidimensional data. We develop algorithms under the prism of a centralized approach, as in a database. Specifically, we rely on widely used multidimensional indexes, like the R-tree. In principle, our schemes adopt a maximal marginal relevance (MMR) ranking strategy and leverage interchange and greedy diversification techniques. Hitherto, mostly combinatorial aspects of this problem have been considered which require scanning the entire data, and therefore, existing solutions are costly.", "title": "The Tantalizing New Prospect of Index-Based Diversified Retrieval", "authors": [{"affiliation": "National Technical University of Athens", "location": "Athens  Greece ", "name": "George Tsatsanifos", "email": "Tsatsanifos"}]}, "phdx17": {"session": "PT", "abstract": "The widespread use and growing popularity of online collaborative content sites (e.g., Yelp, Amazon, IMDB) has created rich resources for users to consult in order to make purchasing decisions on various items such as restaurants, e-commerce products, movies, etc. It has also created new opportunities for producers of such items to improve business by designing better products, composing succinct advertisement snippets and building smart personalized recommendation systems. This motivates us to develop a framework for exploratory mining of user feedback on items in collaborative content sites. Typically, the amount of user feedback associated with item(s) can easily reach hundreds or thousands of ratings, tags or reviews, resulting in an overwhelming amount of information, which users may find difficult to cope with. For example, popular restaurants listed in the review site Yelp routinely receive several thousand ratings and reviews. Moreover, most online activities involve interactions between multiple items and different users, and interpreting such complex user-item interactions becomes intractable too. My PhD research concerns developing novel data mining and exploration algorithms, that account for the above-mentioned challenges, for performing aggregate analytics over available user feedback. Our analysis goal is focused towards helping (a) content consumers make more informed judgment (e.g., if a user will enjoy eating at a particular restaurant), as well as (b) content producers conduct better business (e.g., a re-designed menu to attract more people of a certain demographic group to a restaurant). My dissertation identifies a family of mining tasks, and proposes a suite of algorithms - exact, approximation with theoretical properties, and efficient heuristics - for solving the problems. We conduct a comprehensive set of experiments on the proposed techniques over both synthetic and real data crawled from the web to validate the effectiveness of our framework.", "title": "Exploratory Mining of Collaborative Social Content", "authors": [{"affiliation": "University of Texas at Arlington", "location": "Arlington TX USA Computer Science and Engineering", "name": "Mahashweta Das", "email": "Das"}]}, "pods013": {"session": "Data Mining/Information Retrieval", "abstract": "An intrinsic part of information extraction is the creation and manipulation of relations extracted from text. In this paper, we develop a foundational framework where the central construct is what we call a spanner. A spanner maps an input string into relations over the spans (intervals specified by bounding indices) of the string. The focus of this paper is on the representation of spanners. Conceptually, there are two kinds of such representations. Spanners defined in a primitive representation extract relations directly from the input string; those defined in an algebra apply algebraic operations to the primitively represented spanners.  This framework is driven by SystemT, an IBM commercial product for text analysis, where the primitive representation is that of regular expressions with capture variables.\n\n\n\nWe define additional types of primitive spanner representations by means of two kinds of automata that assign spans to variables.  We prove that the first kind has the same expressive power as regular expressions with capture variables; the second kind expresses precisely the algebra of the regular spanners---the closure of the first kind under standard relational operators.  The core spanners extend the regular ones by string-equality selection (an extension used in SystemT).  We give some fundamental results on the expressiveness of regular and core spanners. As an example, we prove that regular spanners are closed under difference (and complement), but core spanners are not. Finally, we establish connections with related notions in the literature.", "title": "Spanners: A Formal Framework for Information Extraction", "authors": [{"affiliation": "IBM Research - Almaden", "location": "San Jose CA USA ", "name": "Ronald Fagin", "email": "Fagin"}, {"affiliation": "IBM Research - Almaden", "location": "San Jose CA USA ", "name": "Benny Kimelfeld", "email": "Kimelfeld"}, {"affiliation": "IBM Research - Almaden", "location": "San Jose CA USA ", "name": "Frederick Reiss", "email": "Reiss"}, {"affiliation": "Universit\u00e9 Libre de Bruxelles (ULB)", "location": "Bruxelles  Belgium ", "name": "Stijn Vansummeren", "email": "Vansummeren"}]}, "sig256": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "In a previous paper, we laid out the vision of a novel graph query processing paradigm where instead of processing a visual query graph after its construction, it interleaves visual query formulation and processing by exploiting the latency offered by the GUI [4]. Our recent attempts at implementing this vision [4,6], show significant improvement in the system response time (SRT) for subgraph queries. However, these efforts are designed specifically for graph databases containing a large collection of small or medium-sized graphs. Consequently, its frequent fragment-based action-aware indexing schemes and query processing strategy are unsuitable for supporting subgraph queries on large networks containing thousands of nodes and edges. In this demonstration, we present a novel system called QUBLE (QUery Blender for Large nEtworks) to realize this novel paradigm on large networks. We demonstrate various innovative features of QUBLE and its promising performance.", "title": "QUBLE: Blending Visual Subgraph Query Formulation with Query Processing on Large Networks", "authors": [{"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engg", "name": "Ho Hoang Hung", "email": "Hung"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engg", "name": "Sourav S Bhowmick", "email": "S Bhowmick"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore School of Computer Engg", "name": "Ba Quan Truong", "email": "Truong"}, {"affiliation": "Hong Kong Baptist University", "location": "Hong Kong  Hong Kong Department of Computer Science", "name": "Byron Choi", "email": "Choi"}, {"affiliation": "Fudan University", "location": "Shanghai  China School of Computer Science", "name": "Shuigeng Zhou", "email": "Zhou"}]}, "sig609": {"session": "Tutorial 6: Data Stream Warehousing", "abstract": "", "title": "Data Stream Warehousing", "authors": [{"affiliation": "University of Waterloo", "location": "Waterloo ON Canada ", "name": "Lukasz Golab", "email": "Golab"}, {"affiliation": "AT&T Labs - Research", "location": "Florham Park NJ USA ", "name": "Theodore Johnson", "email": "Johnson"}]}, "sig608": {"session": "Tutorial 5: Data Management Perspectives on Business Process Management", "abstract": "Traditional approaches to Business Process Management (BPM) focus primarily on the process aspects, and treat the persistent data accessed and manipulated by the business processes as second class citizens. A recent approach to BPM, based on \"business artifacts\", is centered on a modeling framework that places data and process on an equal footing. The approach has been shown useful in various application domains, and one variant of business artifacts forms the basis of the emerging OMG Case Management Model and Notation (CMMN) standard. Research results have been developed around conceptual models, enterprise interoperation, business intelligence, and verification. This data-centric approach has the potential to provide the basis for a new generation of BPM technology in support of diverse application, and fueled by the insights into abstraction and data management that have been the hallmark of database research since the 70's.", "title": "Data Management Perspectives on Business Process Management", "authors": [{"affiliation": "IBM Watson Research Center", "location": "Yorktown Heights NY USA ", "name": "Richard Hull", "email": "Hull"}, {"affiliation": "University of California at Santa Barbara", "location": "Santa Barbara CA USA ", "name": "Jianwen Su", "email": "Su"}, {"affiliation": "IBM Watson Research Center", "location": "Yorktown Heights NY USA ", "name": "Roman Vaculin", "email": "Vaculin"}]}, "sig250": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "We describe our proposed demonstration of GeoDeepDive, a system that helps\n\ngeoscientists discover information and knowledge buried in the text,\n\ntables, and figures of geology journal articles. This requires solving\n\na host of classical data management challenges including data\n\nacquisition (e.g., from scanned documents), data extraction, and data\n\nintegration. SIGMOD attendees will see demonstrations of three aspects\n\nof our system: (1) an end-to-end system that is of a high enough\n\nquality to perform novel geological science, but is written by a small\n\nenough team so that each aspect can be manageably explained; (2) a\n\nsimple feature engineering system that allows a user to write in\n\nfamiliar SQL or Python; and (3) the effect of different sources\n\n  of feedback on result quality including expert labeling, distant\n\nsupervision, traditional rules, and crowd-sourced data.\n\n\n\nOur prototype builds on our work integrating statistical inference and\n\nlearning tools into traditional database\n\nsystems. If successful, our demonstration will\n\nallow attendees to see that data processing systems that use machine\n\nlearning contain many familiar data processing problems such as\n\nefficient querying, indexing, and supporting tools for database-backed\n\nwebsites, none of which are machine-learning problems, per se.\n\n", "title": "GeoDeepDive: Statistical Inference using Familiar Data-Processing Languages", "authors": [{"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Ce Zhang", "email": "Zhang"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Vidhya Govindaraju", "email": "Govindaraju"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Jackson Borchardt", "email": "Borchardt"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Tim Foltz", "email": "Foltz"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Christopher R\u00e9", "email": "R\u00e9"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Shanan Peters", "email": "Peters"}]}, "sig444": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "Fact checking and data journalism are currently strong trends. The sheer amount of data at hand makes it difficult even for trained professionals to spot biased, outdated or simply incorrect information. We propose to demonstrate FactMinder, a fact checking and analysis assistance application. SIGMOD attendees will be able to analyze documents using FactMinder and experience how background knowledge and open data repositories help build insightful overviews of current topics.", "title": "Fact Checking and Analyzing the Web", "authors": [{"affiliation": "LRI, Universit\u00e9 Paris-Sud & Inria Saclay", "location": " Orsay  France ", "name": "Fran\u00e7ois Goasdou\u00e9", "email": "Goasdou\u00e9"}, {"affiliation": "IBM Almaden Research Center", "location": "San Jose CA USA ", "name": "Konstantinos Karanasos", "email": "Karanasos"}, {"affiliation": "UCSD Database group & Research Center WebDam project, Inria Saclay", "location": "San Diego CA USA ", "name": "Yannis Katsis", "email": "Katsis"}, {"affiliation": "LRI, Universit\u00e9 Paris-Sud & Inria Saclay", "location": "Orsay  France ", "name": "Julien Leblay", "email": "Leblay"}, {"affiliation": "Inria Saclay & LRI, Universit\u00e9 Paris-Sud", "location": "Orsay  France ", "name": "Ioana Manolescu", "email": "Manolescu"}, {"affiliation": "Inria Saclay & LRI, Universit\u00e9 Paris-Sud", "location": "Orsay  France ", "name": "Stamatis Zampetakis", "email": "Zampetakis"}]}, "sig317": {"session": "Road Networks and Trajectories", "abstract": "Given two locations s and t in a road network, a distance query returns the minimum network distance from s to t, while a shortest path query computes the actual route that achieves the minimum distance. These two types of queries find important applications in practice, and a plethora of solutions have been proposed in past few decades. The existing solutions, however, are optimized for either practical or asymptotic performance, but not both. In particular, the techniques with enhanced practical efficiency are mostly heuristic-based, and they offer unattractive worst-case guarantees in terms of space and time. On the other hand, the methods that are worst-case efficient often entail prohibitive preprocessing or space overheads, which render them inapplicable for the large road networks (with millions of nodes) commonly used in modern map applications.\n\n\n\nThis paper presents Arterial Hierarchy (AH), an index structure that narrows the gap between theory and practice in answering shortest path and distance queries on road networks. On the theoretical side, we show that, under a realistic assumption, AH answers any distance query in $\\tilde{O}(\\log \\r)$ time, where $\\r = d_{max}/d_{min}$, and $d_{max}$ (resp.\\ $d_{min}$) is the largest (resp.\\ smallest) $L_\\infty$ distance between any two nodes in the road network. In addition, any shortest path query can be answered in $\\tilde{O}(k + \\log \\r)$ time, where k is the number of nodes on the shortest path. On the practical side, we experimentally evaluate AH on a large set of real road networks with up to twenty million nodes, and we demonstrate that (i) AH outperforms the state of the art in terms of query time, and (ii) its space and pre-computation overheads are moderate.", "title": "Shortest Path and Distance Queries on Road Networks: Towards Bridging Theory and Practice", "authors": [{"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore ", "name": "Andy Diwen Zhu", "email": "Zhu"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore ", "name": "Hui Ma", "email": "Ma"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore ", "name": "Xiaokui Xiao", "email": "Xiao"}, {"affiliation": "Fudan University", "location": "Shanghai  China ", "name": "Siqiang Luo", "email": "Luo"}, {"affiliation": "Nanyang Technological University", "location": "Singapore  Singapore ", "name": "Youze Tang", "email": "Tang"}, {"affiliation": "Fudan University", "location": "Shanghai  China ", "name": "Shuigeng Zhou", "email": "Zhou"}]}, "sig519": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "Parallel coordinates are an established technique to visualize high-dimensional data, in particular for data mining purposes. A major challenge is the ordering of axes, as any axis can have at most two neighbors when placed in parallel on a 2D plane. By extending this concept to a 3D visualization space we can place several axes next to each other. However, finding a good arrangement often does not necessarily become easier, as still not all axes can be arranged pairwise adjacently to each other.\n\nHere, we provide a tool to explore complex data sets using 3D-parallel-coordinate-trees, along with a number of approaches to arrange the axes.", "title": "Interactive Data Mining with 3D-Parallel-Coordinate-Trees", "authors": [{"affiliation": "Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen", "location": "M\u00fcnchen  Germany Institut f\u00fcr Informatik", "name": "Elke Achtert", "email": "Achtert"}, {"affiliation": "Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen", "location": "M\u00fcnchen  Germany Institut f\u00fcr Informatik", "name": "Hans-Peter Kriegel", "email": "Kriegel"}, {"affiliation": "Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen", "location": "M\u00fcnchen  Germany Institut f\u00fcr Informatik", "name": "Erich Schubert", "email": "Schubert"}, {"affiliation": "Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen", "location": "M\u00fcnchen  Germany Institut f\u00fcr Informatik", "name": "Arthur Zimek", "email": "Zimek"}]}, "sig607": {"session": "Tutorial 3: Knowledge Harvesting in the Big-Data Era", "abstract": "The proliferation of knowledge-sharing communities such as Wikipedia and the progress in scalable information extraction from Web and text sources have enabled the automatic construction of very large knowledge bases. Endeavors of this kind include projects such as DBpedia, Freebase,  KnowItAll, ReadTheWeb, and YAGO. These projects provide automatically constructed knowledge bases of facts about named entities, their semantic classes, and their mutual relationships. They contain millions of entities and hundreds of millions of facts about them. Such world knowledge in turn enables cognitive applications and  knowledge-centric services like disambiguating natural-language text, semantic search for entities and relations in Web and enterprise data, and entity-oriented analytics over unstructured contents. Prominent examples of how knowledge bases can be harnessed include the Google Knowledge Graph and the IBM Watson question answering system. This tutorial presents state-of-the-art methods, \n\nrecent advances, research opportunities, and open challenges along this avenue of knowledge harvesting and its applications. Particular emphasis will be on the twofold role of knowledge bases for big-data analytics: using scalable distributed algorithms for harvesting knowledge from Web and text sources, and leveraging entity-centric knowledge for deeper interpretation of and better intelligence with Big Data.\n\n", "title": "Knowledge Harvesting in the Big-Data Era", "authors": [{"affiliation": "Max Planck Institute for Informatics", "location": "Saarbruecken  Germany ", "name": "Fabian Suchanek", "email": "Suchanek"}, {"affiliation": "Max Planck Institute for Informatics", "location": "Saarbruecken  Germany ", "name": "Gerhard Weikum", "email": "Weikum"}]}, "sig314": {"session": "Demo 1: Data Intensive Applications", "abstract": "Evolving data has attracted considerable research attention. Researchers have focused on modeling and querying of schema/instance-level structural changes, such as, insertion, deletion and modification of attributes. Databases with such a functionality are known as temporal databases. A limitation of the temporal databases is that they treat changes as independent events, while often the appearance (or elimination) of some structure in the database is the result of an evolution of some existing structure. We claim that maintaining the causal relationship between the two structures is of major importance since it allows additional reasoning to be performed and answers to be generated for queries that previously had no answers. \n\nWe present the TrenDS, a system for exploiting the evolution relationships between the structures in the database. In particular, our system combines different structures that are associated through evolution relationships into virtual structures to be used during query answering. The virtual structures define ``possible'' database instances, in a fashion similar to the possible worlds in the probabilistic databases. TrenDS uses a query answering mechanism that allows queries to be answered over these possible databases without materializing them. Evaluation of such queries raises many technical challenges, since it requires the discovery of Steiner forests on the evolution graphs. ", "title": "A Query Answering System for Data with Evolution Relationships", "authors": [{"affiliation": "University of Trento", "location": "Trento  Italy ", "name": "Siarhei Bykau", "email": "Bykau"}, {"affiliation": "Statistics Canada", "location": "Ottawa  Canada ", "name": "Flavio Rizzolo", "email": "Rizzolo"}, {"affiliation": "University of Trento", "location": "Trento  Italy ", "name": "Yannis Velegrakis", "email": "Velegrakis"}]}, "sig601": {"session": "Tutorial 2: Workload Management for Big Data Analytics", "abstract": "", "title": "Workload Management for Big Data Analytics", "authors": [{"affiliation": "University of Waterloo", "location": "Waterloo ON Canada ", "name": "Ashraf Aboulnaga", "email": "Aboulnaga"}, {"affiliation": "Duke University", "location": "Durham NC USA ", "name": "Shivnath Babu\u0003", "email": "Babu\u0003"}]}, "sig258": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "Modern database systems have to process huge amounts of data and should provide results with low latency at the same time. To achieve this, data is nowadays typically hold completely in main memory, to benefit of its high bandwidth and low access latency that could never be reached with disks. Current in-memory databases are usually column-stores that exchange columns or vectors between operators and suffer from a high tuple reconstruction overhead.\n\nIn this demonstration proposal, we present DexterDB, which implements our novel prefix tree-based processing model that makes indexes the first-class citizen of the database system.\n\nThe core idea is that each operator takes a set of indexes as input and builds a new index as output that is indexed on the attribute requested by the successive operator.\n\nWith that, we are able to build composed operators, like the multi-way-select-join-group. Such operators speed up the processing of complex OLAP queries so that DexterDB outperforms state-of-the-art in-memory databases.\n\nOur demonstration focuses on the different optimization options for such query plans. Hence, we built an interactive GUI that connects to a DexterDB instance and allows the manipulation of query optimization parameters.\n\nThe generated query plans and important execution statistics are visualized to help the visitor to understand our processing model. ", "title": "Query Processing on Prefix Trees Live", "authors": [{"affiliation": "TU Dresden", "location": "Dresden  Germany ", "name": "Thomas Kissinger", "email": "Kissinger"}, {"affiliation": "TU Dresden", "location": "Dresden  Germany ", "name": "Benjamin Schlegel", "email": "Schlegel"}, {"affiliation": "TU Dresden", "location": "Dresden  Germany ", "name": "Dirk Habich", "email": "Habich"}, {"affiliation": "TU Dresden", "location": "Dresden  Germany ", "name": "Wolfgang Lehner", "email": "Lehner"}]}, "sig310": {"session": "Demo 3: Database Optimization; Performance", "abstract": "Sharing resources of a single database server among multiple tenants is common in multi-tenant Database-as-a-Service providers, such as Microsoft SQL Azure. Multi-tenancy enables cost reduction for the cloud service provider which it can pass on as savings to the tenants. However, resource sharing can adversely affect a tenant's performance due to other tenants' workloads contending for shared resources. Service providers today do not provide any assurances to a tenant in terms of isolating its performance from other co-located tenants. SQLVM, a project at Microsoft Research, is an abstraction for performance isolation which is built on a promise of reserving key database server resources, such as CPU, I/O and memory, for each tenant. The key challenge is in supporting this abstraction within a RDBMS without statically allocating resources to tenants, while ensuring low overheads and scaling to large numbers of tenants. This demonstration will show how SQLVM can effectively isolate a tenant's performance from other tenant workloads co-located at the same database server. Our demonstration will use various scripted scenarios and a data collection and visualization framework to illustrate performance isolation using SQLVM.", "title": "A Demonstration of SQLVM: Performance Isolation in Multi-Tenant Relational Database-as-a-Service", "authors": [{"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Vivek Narasayya", "email": "Narasayya"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Sudipto Das", "email": "Das"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Manoj Syamala", "email": "Syamala"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Surajit Chaudhuri", "email": "Chaudhuri"}, {"affiliation": "National University of Singapore", "location": "Singapore  Singapore ", "name": "Feng Li", "email": "Li"}, {"affiliation": "Stanford University", "location": "Stanford CA USA ", "name": "Hyunjung Park", "email": "Park"}]}, "sig687": {"session": "Industrial 5: Big Data III & More", "abstract": "At Facebook, we use various types of databases and storage system to satisfy the needs of different applications.  The solutions built around these data store systems have a common set of requirements: they have to be highly scalable, maintenance costs should be low and they have to perform efficiently.  We use a sharded mySQL+memcache solution to support real-time access of tens of petabytes of data and we use TAO to provide consistency of this web-scale database across geographical distances. We use Haystack data store for storing the 3 billion new photos we host every week. We use Apache Hadoop to mine intelligence from 100 petabytes of click logs and combine it with the power of Apache HBase to store all Facebook Messages.\n\nThis paper describes the reasons why each of these databases is appropriate for that workload and the design decisions and tradeoffs that were made while implementing these solutions. We touch upon the consistency, availability and partitioning tolerance of each of these solutions. We touch upon the reasons why some of these systems need ACID semantics and other systems do not. We describe the techniques we have used to map the Facebook Graph Database into a set of relational tables. We speak of how we plan to do big-data deployments across geographical locations and our requirements for a new breed of pure-memory and pure-SSD based transactional database.\n\nEsteemed researchers in the Database Management community have benchmarked query latencies on Hive/Hadoop to be less performant than a traditional Parallel DBMS.  We describe why these benchmarks are insufficient for Big Data deployments and why we continue to use Hadoop/Hive. We present an alternate set of benchmark techniques that measure capacity of a database, the value/byte in that database and the efficiency of inbuilt crowd-sourcing techniques to reduce administration costs of that database.\n\n", "title": "Petabyte Scale Databases and Storage Systems at Facebook", "authors": [{"affiliation": "Facebook", "location": "menlo park CA USA ", "name": "dhruba borthakur", "email": "borthakur"}]}, "sig686": {"session": "PTAbstract", "abstract": "With current trends in integrating phylogenetic analysis into pharma-research, computing systems that integrate the two areas can help the drug discovery field.  DrugTree is a tool that overlays ligand data on a protein-motivated phylogenetic tree.  While initial tests of DrugTree are successful, it has been noticed that there are a number of lags concerning querying the tree.  Due to the interleaving nature of the data, query optimization can become problematic since the data is being obtained from multiple sources, integrated and then presented to the user with the phylogenetic imposed upon the phylogenetic analysis layer.  This poster presents our initial methodologies for addressing the query optimization issues.  Our approach applies standards as well as uses novel mechanisms to help improve performance time.   ", "title": "Mobile Interaction and Query Optimizationin a Protein-Ligand Data Analysis System", "authors": [{"affiliation": "Montclair State University", "location": "Montclair NJ USA Computer Science", "name": "Marvin Lapeine", "email": "Lapeine"}, {"affiliation": "Montclair State University", "location": "Montclair NJ USA Computer Science", "name": "Katherine Herbert", "email": "Herbert"}, {"affiliation": "Montclair State University", "location": "Montclair NJ USA Computer Science", "name": "Emily Hill", "email": "Hill"}, {"affiliation": "Montclair State University", "location": "Montclair NJ USA Chemistry and Biochemistry", "name": "Nina Goodey", "email": "Goodey"}]}, "sig524": {"session": "Demo 2: Data Analysis and Mining; Privacy; Security", "abstract": "Data confidentiality is one of the main concerns for users of public cloud services. The key problem is protecting sensitive data from being accessed by cloud administrators who have root privileges and can remotely inspect the memory and disk contents of the cloud servers. While encryption is the basic mechanism that can leveraged to provide data confidentiality, providing an efficient database-as-a-service that can run on encrypted data raises several interesting challenges. In this demonstration we outline the functionality of Cipherbase --- a full fledged SQL database system that supports the full generality of a database system while providing high data confidentiality. Cipherbase has a novel architecture that tightly integrates custom-designed trusted hardware for performing operations on encrypted data securely such that an administrator cannot get access to any plaintext corresponding to sensitive data.", "title": "Secure Database-as-a-Service with Cipherbase", "authors": [{"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Arvind Arasu", "email": "Arasu"}, {"affiliation": "University of Wisconsin", "location": "Madison WI USA ", "name": "Spyros Blanas", "email": "Blanas"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Ken Eguro", "email": "Eguro"}, {"affiliation": "Stanford University", "location": "Stanford CA USA ", "name": "Manas Joglekar", "email": "Joglekar"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Raghav Kaushik", "email": "Kaushik"}, {"affiliation": "ETH", "location": "Zurich  Switzerland ", "name": "Donald Kossmann", "email": "Kossmann"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Ravi Ramamurthy", "email": "Ramamurthy"}, {"affiliation": "University of Washington", "location": "Seattle WA USA ", "name": "Prasang Upadhyaya", "email": "Upadhyaya"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Ramarathnam Venkatesan", "email": "Venkatesan"}]}, "sig683": {"session": "PTAbstract", "abstract": "As more and more RDF data becomes available, such as DBpedia, Yago and Freebase, it is desired to provide users with simple interfaces to access the datasets. Although the SPARQL query language is a standard way to query RDF data, it remains tedious and difficult even for expert users because of the formality of the language and the complexity of the underlying schema of RDF data. An ideal system should allow users to express queries in their own languages. In this work, we propose a methodology to translate natural\n\nlanguage questions into SPARQL queries, which can be answered by existing RDF engines and fulfill users\u0092 information need. ", "title": "Natural Language Question Answering over RDF Data", "authors": [{"affiliation": "Peking University", "location": "Beijing  China ", "name": "Ruizhe Huang", "email": "Huang"}, {"affiliation": "Peking University", "location": "Beijing  China ", "name": "Lei Zou", "email": "Zou"}]}, "sig527": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "In this demonstration, we show-case a database management system extended with a new type of component that we call a Data Use Manager (DUM). The DUM enables DBAs to attach policies to data loaded into the DBMS. It then monitors how users query the data, flags potential policy violations, recommends possible fixes, and supports offline analysis of user activities related to data policies. The demonstration uses real healthcare data.", "title": "The Power of Data Use Management in Action", "authors": [{"affiliation": "University of Washington", "location": "Seattle WA USA Department of Computer Science and Engineering", "name": "Prasang Upadhyaya", "email": "Upadhyaya"}, {"affiliation": "University of Washington", "location": "Seattle WA USA ", "name": "Nick Anderson", "email": "Anderson"}, {"affiliation": "University of Washington", "location": "Seattle WA USA Department of Computer Science and Engineering", "name": "Magdalena Balazinska", "email": "Balazinska"}, {"affiliation": "University of Washington", "location": "Seattle WA USA ", "name": "Bill Howe", "email": "Howe"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Raghav Kaushik", "email": "Kaushik"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Ravi Ramamurthy", "email": "Ramamurthy"}, {"affiliation": "University of Washington", "location": "Seattle WA USA ", "name": "Dan Suciu", "email": "Suciu"}]}, "sig528": {"session": "Demo 1: Data Intensive Applications", "abstract": "We propose to demonstrate Pyxis, a system that optimizes database applications by pushing computation to the database server. Our system applies program analysis techniques to the application source code to determine pieces of application logic that should be moved to the database server to improve performance. This frees the developer from the need to understand the intricacies of database operations or learn a new programming language for stored procedures. In addition, by dynamically monitoring resource utilization on the database server, Pyxis can migrate computation between application and database in response to workload changes. Our previous experiments have shown that Pyxis can decrease latency up to 3x for transactional applications, and improve throughput up to 1.7x when compared to a standard implementation using embedded SQL statements in application logic. We will demonstrate these\n\ncapabilities via a visualization of real-time performance as well as an interactive code partitioning tool we have developed.", "title": "Speeding up Database Applications with Pyxis", "authors": [{"affiliation": "MIT CSAIL", "location": "Cambridge MA USA ", "name": "Alvin Cheung", "email": "Cheung"}, {"affiliation": "Cornell University", "location": "Ithaca NY USA Computer Science", "name": "Owen Arden", "email": "Arden"}, {"affiliation": "MIT CSAIL", "location": "Cambridge MA USA ", "name": "Samuel Madden", "email": "Madden"}, {"affiliation": "Cornell University", "location": "Ithaca NY USA Computer Science", "name": "Andrew Myers", "email": "Myers"}]}, "sig679": {"session": "PTAbstract", "abstract": "With the availability of large main memory capacities, in-memory index structures have become an important component of modern data management platforms. Current research even suggests index-based query processing as an alternative or supplement for traditional tuple-at-a-time processing models. However, while simple sequential scan operations can fully exploit the high bandwidth provided by main memory, indexes are mainly latency bound and spend most of their time waiting for memory accesses.\n\nConsidering current hardware trends, the problem of high memory latency is further exacerbated as modern shared-memory multiprocessors with non-uniform memory access (NUMA) become increasingly common. On those NUMA platforms, the execution time of index operations is dominated by memory access latency that increases dramatically when accessing memory on remote sockets. Therefore, good index performance can only be achieved through careful optimization of the index structure to the given topology.\n\nBUZZARD is a NUMA-aware in-memory indexing system. Using adaptive data partitioning techniques, BUZZARD distributes a prefix-tree-based index across the NUMA system and hands off incoming requests to worker threads located on each partition\u0092s respective NUMA node. This approach reduces the number of remote memory accesses to a minimum and improves cache utilization. In addition, all indexes inside BUZZARD are only accessed by their respective owner, eliminating the need for synchronization primitives like compare-and-swap.", "title": "BUZZARD: A NUMA-Aware In-Memory Indexing System", "authors": [{"affiliation": "Technische Universit\u00e4t Dresden", "location": "Dresden  Germany ", "name": "Lukas Maas", "email": "Maas"}, {"affiliation": "Technische Universit\u00e4t Dresden", "location": "Dresden  Germany ", "name": "Thomas Kissinger", "email": "Kissinger"}, {"affiliation": "Technische Universit\u00e4t Dresden", "location": "Dresden  Germany ", "name": "Dirk Habich", "email": "Habich"}, {"affiliation": "Technische Universit\u00e4t Dresden", "location": "Dresden  Germany ", "name": "Wolfgang Lehner", "email": "Lehner"}]}, "phdx20": {"session": "PT", "abstract": "With the rapid development of the Internet and multimedia technologies\n\nover the last decade, a huge amount of data has become\n\navailable, from text corpus, to collections of online images and\n\nvideos. Cheap storage cost and modern database technologies have\n\nmade it possible to accumulate large-scale datasets. However, the\n\never-growing sizes of the datasets make it harder to search useful\n\ninformation from such data. A fundamental computational primitive\n\nfor dealing with massive multimedia datasets is the similarity\n\nsearch problem. Multimedia similarity search aims to preprocess a\n\ndatabase so that given a query object, one can quickly find its similar\n\nobjects in the database. Searching similar objects from a large\n\ndataset in high-dimensional spaces is at the heart of many multimedia\n\napplications, such as near-duplicate retrieval, multimedia tagging,\n\nrecommendation, and so on. Driven by its significance, lots\n\nof efforts have been made on this topic. The goal of my research\n\nis to design efficient hashing methods for large-scale multimedia\n\nsearch. In this paper, we first present the general framework for\n\nmultimedia similarity search and discuss the latest improvements\n\nand progresses in the field. Then we describe the contributions we\n\nhave made to effectively and efficiently search similar multimedia\n\nobjects from large-scale databases. Finally, we discuss the future\n\nwork and draw a conclusion.", "title": "Effective Hashing for Large-scale Multimedia Search", "authors": [{"affiliation": "The University of Queensland", "location": "Brisbane  Australia ", "name": "Jingkuan song", "email": "song"}]}, "phdg22": {"session": "PT", "abstract": "Nowadays scientists receive increasingly large volumes of data daily. These volumes and accompanying metadata that describes them are collected in scientific file repositories. Today's scientists need a data management tool that makes these file repositories accessible and performs a number of exploration steps near-instantly. Current database technology, however, has a long data-to-insight time, and does not provide enough interactivity to shorten the exploration time. We envision that exploiting metadata helps solving these problems. To this end, we propose a novel query execution paradigm, in which we decompose the query execution into two stages. During the first stage, we process only metadata, whereas the rest of the data is processed during the second stage. So that, we can exploit metadata to boost interactivity and to ingest only required data per query transparently. Preliminary experiments show that up-front ingestion time is reduced by orders of magnitude, while query performance remains similar. Motivated by these results, we identify the challenges on the way from the new paradigm to efficient interactive data exploration.", "title": "Turning Scientists into Data Explorers", "authors": [{"affiliation": "CWI & University of Amsterdam", "location": "Amsterdam  Netherlands ", "name": "Ya??z Karg?n", "email": "Karg?n"}]}, "phdx23": {"session": "PT", "abstract": "In numerous real applications, uncertainty is inherently introduced when massive data are generated. Modern database management systems aim to incorporate and handle data with uncertainties as a first-class citizen, where uncertain data are represented as probabilistic relations. In my thesis, my work has focused on monitoring and summarization of large probabilistic data. Specifically, we extended the distributed threshold monitoring problem to distributed probabilistic data. Instead, we actually need to monitor the aggregated value (e.g. sum) of distributed probabilistic data against both the score threshold and the probability threshold, which make the techniques designed for deterministic data are not directly applicable. Our algorithms have significantly reduced both the communication\n\nand computation costs as shown by an extensive experimental evaluation on large real datasets. On the other hand, building histograms to summarize the distribution of certain feature in a large data set is a fundamental problem in data management. Recent work have extended this studies to probabilistic data,  but their methods suffer\n\nfrom the limited scalability. We present novel methods to build scalable histograms over large probabilistic data using distributed and parallel algorithms. Extensive experiments on large real data sets have demonstrated the superb scalability and efficiency achieved by our implementations in MapReduce, when compared to the existing,\n\nstate-of-the-art centralized methods.", "title": "Efficient and Scalable Monitoring and Summarization of Large Probabilistic Data", "authors": [{"affiliation": "University of Utah", "location": "Salt Lake City UT USA School of Computing", "name": "Mingwang Tang", "email": "Tang"}]}, "sig02": {"session": "PTAbstract", "abstract": "Over the past decade global securities markets have dramatically changed. Evolution of market structure in combination with advances in computer technologies led to emergence of electronic securities trading. Securities transactions that used to be conducted in person and over the phone are now predominantly executed by automated trading systems. This resulted in significant fragmentation of the markets, vast increase in the exchange volumes and even greater increase in the number of orders.\n\n\n\nIn this talk we present and analyze forces behind the wide proliferation of electronic securities trading in US stocks and options markets. We also make a high-level introduction into electronic securities market structure. We discuss trading objectives of different classes of market participants and analyze how their activity affects data volumes.  We also present typical securities trading firm data flow and analyze various types of data it uses in its trading operations.  \n\n\n\nWe close with the implications this \"sea change\" has on DBMS requirements in capital markets.  \n\n", "title": "Big Data in Capital Markets", "authors": [{"affiliation": "Middle Lake Partners, LLC", "location": "Wilmette IL USA ", "name": "Alex Nazaruk", "email": "Nazaruk"}, {"affiliation": "Middle Lake Partners, LLC", "location": "Wilmette IL USA ", "name": "Michael Rauchman", "email": "Rauchman"}]}, "sig01": {"session": "Welcome", "abstract": "", "title": "Welcome Message from the SIGMOD Chairs", "authors": [{"affiliation": "Columbia University", "location": "New York City NY USA ", "name": "Kenneth Ross", "email": "Ross"}, {"affiliation": "AT&T Labs-Research", "location": "Florham Park NJ USA ", "name": "Divesh Srivastava", "email": "Srivastava"}, {"affiliation": "HKUST", "location": "Hong Kong  Hong Kong ", "name": "Dimitris Papadias", "email": "Papadias"}, {"affiliation": "HKUST", "location": "Hong Kong  Hong Kong ", "name": "Stavros Papadopoulos", "email": "Papadopoulos"}]}, "sig05": {"session": "PTAbstract", "abstract": "Paul Yaron is responsible for Non-Mainframe, Relational Database Architecture, Engineering and Strategy for JPMC globally. JP Morgan is a leading financial services firm with assets over $2 trillion, operates 40 major datacenters around the globe, servicing over 60 countries with over 250,000 employees. It partners with 170 regulators and manages 230 Petabytes of data, JPMC depends on over 23,000 database instances to service multiple business units. With a deployment of such scope, JPMC leverages solutions from most major database, security and operating system vendors.\n\n\n\nThis talk will discuss the challenges and strategies of managing the evolving ecosystem of \u0093all data\u0094, from information security, to internal virtualization strategies. Engineering reliable globally scalable and compliant data management solutions demands a model for proactively measuring the risk complexity of an ecosystem for expert focus and potential proactive remediation. The research for quantitative measurement of database (or other) ecosystem entropy appears sparse.  JPMC is looking to share its ideas in this space with the academic community as the need for such quantitative measures are increasingly important as ecosystems move from islands of single tenant risk into multi-tenant risk clusters.  \n\n", "title": "Managing Database Technology at Enterprise Scale", "authors": [{"affiliation": "J. P. Morgan Chase", "location": "New York City NY USA ", "name": "Paul Yaron", "email": "Yaron"}]}, "sig377": {"session": "Distributed Systems", "abstract": "  We consider the problem of separating consistency-related safety\n\n  properties from availability and durability in distributed data\n\n  stores via the application of a ``bolt-on'' shim layer that upgrades\n\n  the safety of an underlying general-purpose data store. This shim\n\n  provides the same consistency guarantees atop a wide range of widely\n\n  deployed but often inflexible stores. As causal consistency is one\n\n  of the strongest consistency models that remain available during\n\n  system partitions, we develop a shim layer that upgrades eventually\n\n  consistent stores to provide convergent causal\n\n  consistency. Accordingly, we leverage widely deployed eventually\n\n  consistent infrastructure as a common substrate for providing causal\n\n  guarantees. We describe algorithms and shim implementations that are\n\n  suitable for a large class of application-level causality\n\n  relationships and evaluate our techniques using an existing,\n\n  production-ready data store and with real-world explicit causality\n\n  relationships.", "title": "Bolt-on Causal Consistency", "authors": [{"affiliation": "UC Berkeley", "location": "Berkeley CA USA ", "name": "Peter Bailis", "email": "Bailis"}, {"affiliation": "UC Berkeley and KTH/Royal Institute of Technology", "location": "Berkeley CA USA ", "name": "Ali Ghodsi", "email": "Ghodsi"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA ", "name": "Joseph Hellerstein", "email": "Hellerstein"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA ", "name": "Ion Stoica", "email": "Stoica"}]}, "sig132": {"session": "Spatial Databases II", "abstract": "Efficient spatial joins are pivotal for many applications and particularly important for geographical information systems or for the simulation sciences\n\nwhere scientists work with spatial models. Past research has primarily focused on disk-based spatial joins; efficient in-memory approaches, however,\n\nare important for two reasons: a) main memory has grown so large that many datasets fit in it and b) the in-memory join is a very time-consuming part\n\nof all disk-based spatial joins.\n\n\n\nIn this paper we develop TOUCH, a novel in-memory spatial join algorithm that uses hierarchical data-oriented space partitioning, thereby keeping both\n\nits memory footprint and the number of comparisons low. Our results show that TOUCH outperforms known in-memory spatial-join algorithms as well as in-memory\n\nimplementations of disk-based join approaches. In particular, it has a one order of magnitude advantage over the memory-demanding state of the art in terms of\n\nnumber of comparisons (i.e., pairwise object comparisons), as well as execution time, while it is two orders of magnitude faster when compared to approaches\n\nwith a similar memory footprint. Furthermore, TOUCH is more scalable than competing approaches as data density grows.", "title": "TOUCH: In-Memory Spatial Join by Hierarchical Data-Oriented Partitioning", "authors": [{"affiliation": "National University of Singapore", "location": "Singapore  Singapore School of Computing", "name": "Sadegh Nobari", "email": "Nobari"}, {"affiliation": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", "location": "Lausanne  Switzerland Data-Intensive Applications and Systems Lab", "name": "Farhan Tauheed", "email": "Tauheed"}, {"affiliation": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", "location": "Lausanne  Switzerland Data-Intensive Applications and Systems Lab", "name": "Thomas Heinis", "email": "Heinis"}, {"affiliation": " Rutgers University", "location": "New Brunswick NJ USA Department of Management Science and Information Systems", "name": "Panagiotis Karras", "email": "Karras"}, {"affiliation": "National University of Singapore", "location": "Singapore  Singapore School of Computing", "name": "St\u00e9phane Bressan", "email": "Bressan"}, {"affiliation": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", "location": "Lausanne  Switzerland Data-Intensive Applications and Systems Lab", "name": "Anastasia Ailamaki", "email": "Ailamaki"}]}, "pods023": {"session": "Query Languages", "abstract": "We consider the evaluation of first-order queries over classes of databases with bounded expansion. The notion of bounded expansion is fairly broad and generalizes bounded degree, bounded treewidth and exclusion of at least one minor. It was known that over a class of databases with bounded expansion, first-order sentences could be evaluated in time linear in the size of the database. We first give a different proof of this result. Moreover, we show that answers to first-order queries can be enumerated with constant delay after a linear time preprocessing. We also show that counting the number of answers to a query can be done in time linear in the size of the database.", "title": "Enumeration of First-Order Queries on Classes of Structures With Bounded Expansion", "authors": [{"affiliation": "INRIA and ENS Cachan", "location": "Cachan  France ", "name": "Wojciech Kazana", "email": "Kazana"}, {"affiliation": "INRIA and ENS Cachan", "location": "Cachan  France ", "name": "Luc Segoufin", "email": "Segoufin"}]}, "sig581": {"session": "Text Databases", "abstract": "The problem of finding matches of a regular expression (RE) on a string exists in many applications such as text editing, biosequence search, and shell commands.  Existing techniques first identify candidates using substrings in the RE, then verify each of them using an automaton.  These techniques become inefficient when there are many candidate occurrences that need to be verified.  In this paper we propose a novel technique that prunes false negatives by utilizing {\\em negative factors}, which are substrings that {\\em cannot} appear in an answer.  A main advantage of the technique is that it can be integrated with many existing algorithms to improve their efficiency significantly.  We give a full specification of this technique.  We develop an efficient algorithm that utilizes negative factors to prune candidates, then improve it by using bit operations to process negative factors in parallel.  We show that negative factors, when used together with necessary factors (substrings that must appear in each answer), can achieve much better pruning power.  We analyze the large number of negative factors, and develop an algorithm for finding a small number of high-quality negative factors.  We conducted a thorough experimental study of this technique on real data sets, including DNA sequences, proteins, and text documents, and show the significant performance improvement when applying the technique in existing algorithms.  For instance, it improved the search speed of the popular Gnu Grep tool by 11 to 74 times for text documents.", "title": "Improving Regular-Expression Matching on Strings Using Negative Factors", "authors": [{"affiliation": "Northeastern University", "location": "Shenyang  China ", "name": "Xiaochun Yang", "email": "Yang"}, {"affiliation": "Northeastern University", "location": "Shenyang  China ", "name": "Bin Wang", "email": "Wang"}, {"affiliation": "Northeastern University", "location": "Shenyang  China ", "name": "Tao Qiu", "email": "Qiu"}, {"affiliation": "Northeastern University", "location": "Shenyang  China ", "name": "Yaoshu Wang", "email": "Wang"}, {"affiliation": "UC Irvine", "location": "Irvine  USA ", "name": "Chen Li", "email": "Li"}]}, "sig134": {"session": "Demo 1: Data Intensive Applications", "abstract": "Scientists in all disciplines increasingly rely on simulations to develop a better understanding of the subject they are studying. For example the neuroscientists we collaborate with in the Blue Brain project have started to simulate the brain on a supercomputer. The level of detail of their models is unprecedented as they model details on the subcellular level (e.g., the neurotransmitter). This level of detail, however, also leads to a true data deluge and the neuroscientists have only few tools to efficiently analyze the data.\n\n\n\n\n\nThis demonstration showcases three innovative spatial management techniques that have substantial impact on computational neuroscience and other disciplines in that they allow to build, analyze and simulate bigger and more detailed models. More particularly, we demonstrate a tool that integrates three spatial data management techniques that have enabled breakthroughs in neuroscience: FLAT that enables efficient querying of spatial data, SCOUT that allows for fast exploration of spatial data and HiDOP that makes efficient data discovery possible.\n\n", "title": "Data-driven Neuroscience: Enabling Breakthroughs Via Innovative Data Management", "authors": [{"affiliation": "EPFL", "location": "Lausanne  Switzerland ", "name": "Alexandros Stougiannis", "email": "Stougiannis"}, {"affiliation": "EPFL", "location": "Lausanne  Switzerland ", "name": "Mirjana Pavlovic", "email": "Pavlovic"}, {"affiliation": "EPFL", "location": "Lausanne  Switzerland ", "name": "Farhan Tauheed", "email": "Tauheed"}, {"affiliation": "EPFL", "location": "Lausanne  Switzerland ", "name": "Thomas Heinis", "email": "Heinis"}, {"affiliation": "EPFL", "location": "Lausanne  Switzerland ", "name": "Anastasia Ailamaki", "email": "Ailamaki"}]}, "sig638": {"session": "Graph Management", "abstract": "We propose a new exact method for shortest-path distance queries on large-scale networks.\n\nOur method precomputes distance labels for vertices by performing a breadth-first search from every vertex.\n\nSeemingly too obvious and too inefficient at first glance,\n\nthe key ingredient introduced here is pruning during breadth-first searches.\n\nWhile we can still answer the correct distance for any pair of vertices from the labels,\n\nit surprisingly reduces the search space and sizes of labels.\n\nMoreover, we show that we can perform 32 or 64 breadth-first searches simultaneously\n\nexploiting bitwise operations.\n\nWe experimentally demonstrate that\n\nthe combination of these two techniques is efficient and robust on various kinds of large-scale real-world networks.\n\nIn particular, our method can handle social networks and web graphs with hundreds of millions of edges,\n\nwhich are two orders of magnitude larger than the limits of previous exact methods,\n\nwith comparable query time to those of previous methods. ", "title": "Fast Exact Shortest-Path Distance Queries on Large Networks by Pruned Landmark Labeling", "authors": [{"affiliation": "The University of Tokyo ", "location": "Tokyo  Japan ", "name": "Takuya Akiba", "email": "Akiba"}, {"affiliation": "The University of Tokyo ", "location": "Tokyo  Japan ", "name": "Yoichi Iwata", "email": "Iwata"}, {"affiliation": "National Institute of Informatics, and Preferred Infrastructure, Inc.", "location": "Tokyo  Japan ", "name": "Yuichi Yoshida", "email": "Yoshida"}]}, "sig639": {"session": "Information Extraction", "abstract": "Personalized PageRank (PPR) has been successfully applied to various applications. \n\nIn real applications, it is important to set PPR parameters in an ad-hoc manner when finding similar nodes because of dynamically changing nature of graphs. \n\nThrough interactive actions, interactive similarity search supports users to enhance the efficacy of applications. \n\nUnfortunately, if the graph is large, interactive similarity search is infeasible due to its high computation cost. \n\nPrevious PPR approaches cannot effectively handle interactive similarity search since they need precomputation or approximate computation of similarities. \n\nThe goal of this paper is to efficiently find the top-k nodes with exact node ranking so as to effectively support interactive similarity search based on PPR. \n\nOur solution is Castanet. \n\nThe key Castanet operations are (1) estimate upper/lower bounding similarities iteratively, and (2) prune unnecessary nodes dynamically to obtain top-k nodes in each iteration. \n\nExperiments show that our approach is much faster than existing approaches. \n\n", "title": "Efficient Ad-hoc Search for Personalized PageRank", "authors": [{"affiliation": "NTT", "location": "Musashino-shi  Japan ", "name": "Yasuhiro Fujiwara", "email": "Fujiwara"}, {"affiliation": "NTT", "location": "Yokosuka-shi  Japan ", "name": "Makoto Nakatsuji", "email": "Nakatsuji"}, {"affiliation": "NTT", "location": "Musashino-shi  Japan ", "name": "Hiroaki Shiokawa", "email": "Shiokawa"}, {"affiliation": "NTT", "location": "Musashino-shi  Japan ", "name": "Takeshi Mishima", "email": "Mishima"}, {"affiliation": "NTT", "location": "Musashino-shi  Japan ", "name": "Makoto Onizuka", "email": "Onizuka"}]}, "sig631": {"session": "Data Storage", "abstract": "Temporal and multi-version databases are ideal candidates for a distributed store, which offers large storage space, and parallel and distributed processing power from a cluster of (commodity) machines. A key challenge is to achieve a good load balancing algorithm for storage and processing of these data, which is done by partitioning the database. We introduce the concept of {\\em optimal splitters} for temporal and multi-version databases, which induce a partition of the input data set, and guarantee that the size of the maximum bucket be minimized among all possible configurations, given a budget for the desired number of buckets. We design efficient methods for memory- and disk resident data respectively, and show that they significantly outperform competing baseline methods both theoretically and empirically on large real data sets.", "title": "Optimal Splitters for Temporal and Multi-version Databases", "authors": [{"affiliation": "School of Computing, University of Utah", "location": "Salt Lake City UT USA ", "name": "Wangchao Le", "email": "Le"}, {"affiliation": "School of Computing, University of Utah", "location": "Salt Lake City UT USA ", "name": "Feifei Li", "email": "Li"}, {"affiliation": "Chinese University of Hong Kong; Korea Advanced Institute of Science and Technology", "location": "Hong Kong  China ", "name": "Yufei Tao", "email": "Tao"}, {"affiliation": "School of Computing, University of Utah", "location": "Salt Lake City UT USA ", "name": "Robert Christensen", "email": "Christensen"}]}, "sig632": {"session": "Schema Matching and Spatial Databases I", "abstract": "\\begin{abstract}\n\nThe creation of values to represent incomplete information, often referred to as \\textit{value invention}, is central in data exchange. Within schema mappings, Skolem functions have long been used for value invention as they permit a precise representation of missing information. Recent work on a powerful mapping language called \\textit{second-order tuple generating dependencies} (SO tgds), has drawn attention to the fact that the use of arbitrary Skolem functions can have negative computational and programmatic properties in data exchange. In this paper, we present two techniques for understanding when the Skolem functions needed to represent the correct semantics of incomplete information are computationally well-behaved. Specifically, we consider when the Skolem functions in second-order (SO) mappings have a first-order (FO) semantics and are therefore programmatically and computationally more desirable for use in practice. Our first technique, \\textit{linearization}, significantly extends the Nash, Bernstein and Melnik \\emph{unskolemization} algorithm, by understanding when the sets of arguments of the Skolem functions in a mapping are related by set inclusion. We show that such a linear relationship leads to mappings that have FO semantics and are expressible in popular mapping languages including source-to-target tgds  and nested tgds. Our second technique uses source semantics, specifically functional dependencies (including keys), to transform SO mappings into equivalent FO mappings. We show that our algorithms are applicable to a strictly larger class of mappings than previous approaches, but more importantly we present an extensive experimental evaluation that quantifies this difference (about 78\\% improvement) over an extensive schema mapping benchmark and illustrates the applicability of our results on real mappings.\n\n\\end{abstract}", "title": "Value Invention in Data Exchange", "authors": [{"affiliation": "University of Toronto", "location": "Toronto ON Canada Department of Computer Science", "name": "Patricia Arocena", "email": "Arocena"}, {"affiliation": "Illinois Institute of Technology", "location": "Chicago  IL USA Department of Computer Science", "name": "Boris Glavic", "email": "Glavic"}, {"affiliation": "University of Toronto", "location": "Toronto ON Canada Department of Computer Science", "name": "Renee Miller", "email": "Miller"}]}, "sig295": {"session": "Demo 3: Database Optimization; Performance", "abstract": "In this demonstration, we present Ibex, a novel storage engine featuring hybrid, FPGA-accelerated query processing. In Ibex, an FPGA is inserted along the path between the storage devices and the database engine. The FPGA acts as an intelligent storage engine supporting query off-loading from the query engine. Apart from significant performance improvements for many common SQL queries, the demo will show how Ibex reduces data movement, CPU usage, and overall energy consumption in database appliances.", "title": "Less Watts, More Performance: An Intelligent Storage Engine for Data Appliances", "authors": [{"affiliation": "ETH Zurich", "location": "Zurich  Switzerland Systems Group, Department of Computer Science", "name": "Louis Woods", "email": "Woods"}, {"affiliation": "TU Dortmund University", "location": "Dortmund  Germany DBIS Group, Department of Computer Science", "name": "Jens Teubner", "email": "Teubner"}, {"affiliation": "ETH Zurich", "location": "Zurich  Switzerland Systems Group, Department of Computer Science", "name": "Gustavo Alonso", "email": "Alonso"}]}, "sig634": {"session": "Data Streams", "abstract": " A fundamental problem in data management and analysis is to generate descriptions of the distribution of data.  It is most common to give such descriptions in terms of the cumulative distribution, which is characterized by the quantiles of the data. The design and engineering of efficient methods to find these quantiles has attracted much study, especially in the case where the data is described incrementally, and we must compute the quantiles in an online, streaming fashion. Yet while such algorithms have proved to be tremendously useful in practice, there has been limited formal comparison of the competing methods, and no comprehensive study of their performance. In this paper, we remedy this deficit by providing a taxonomy of different methods, and describe efficient implementations. In doing so, we propose and analyze variations that have not been explicitly studied before, yet which turn out to perform the best. To illustrate this, we provide detailed experimental comparisons demonstrating the tradeoffs between space, time, and accuracy for quantile computation.", "title": "Quantiles over Data Streams: An Experimental Study", "authors": [{"affiliation": "The Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Lu Wang", "email": "Wang"}, {"affiliation": "The Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Ge Luo", "email": "Luo"}, {"affiliation": "The Hong Kong University of Science and Technology", "location": "Hong Kong  Hong Kong Department of Computer Science and Engineering", "name": "Ke Yi", "email": "Yi"}, {"affiliation": "AT&T Labs -- Research", "location": "Florham Park NJ USA ", "name": "Graham Cormode", "email": "Cormode"}]}, "sig248": {"session": "XML", "abstract": "A great deal of research into the learning of schemas from XML data has been conducted in recent years to enable the automatic discovery of XML Schemas from XML documents when no schema, or only a low-quality one is available.  Unfortunately, and in strong contrast to, for instance, the relational model, the automatic discovery of even the simplest of XML constraints, namely XML keys, has been left largely unexplored in this context. A major obstacle here is the unavailability of a theory on reasoning about XML keys in the presence of XML schemas, which is needed to validate the quality of candidate keys. The present paper embarks on a fundamental study of such a theory and classifies the complexity of several crucial properties concerning XML keys in the presence of an XSD, like, for instance, testing for consistency, boundedness, satisfiability, universality, and equivalence. Of independent interest, novel results are obtained related to cardinality estimation of XPath result sets. A mining algorithm is then developed within the framework of levelwise search. The algorithm leverages known discovery algorithms for functional dependencies in the relational model, but incorporates the above mentioned properties to assess and refine the quality of derived keys. An experimental study on an extensive body of real world XML data evaluating the effectiveness of the proposed algorithm is provided. ", "title": "Discovering XSD Keys from XML Data", "authors": [{"affiliation": "PUC Chile & University of Oxford", "location": "Santiago  Chile ", "name": "Marcelo Arenas", "email": "Arenas"}, {"affiliation": "Hasselt University & Transnational University of Limburg", "location": "Hasselt  Belgium ", "name": "Jonny Daenen", "email": "Daenen"}, {"affiliation": "Hasselt University & Transnational University of Limburg", "location": "Hasselt  Belgium ", "name": "Frank Neven", "email": "Neven"}, {"affiliation": "PUC Chile", "location": "Santiago  Chile ", "name": "Martin Ugarte", "email": "Ugarte"}, {"affiliation": "Hasselt University & Transnational University of Limburg", "location": "Hasselt  Belgium ", "name": "Jan Van den Bussche", "email": "Van den Bussche"}, {"affiliation": "Universit\u00e9 Libre de Bruxelles (ULB)", "location": "Brussels  Belgium ", "name": "Stijn Vansummeren", "email": "Vansummeren"}]}, "sig636": {"session": "Text Databases", "abstract": "A string similarity measure quantifies the similarity between two text strings for approximate string matching or comparison. For example, the strings \"Sam\" and \"Samuel\" can be considered similar. Most existing work that computes the similarity of two strings only considers syntactic similarities, e.g., number of common words or q-grams. While these are indeed indicators of similarity, there are many important cases where syntactically different strings can represent the same real-world object. For example, \"Bill\" is a short form of \"William\". Given a collection of predefined synonyms, the purpose of the paper is to explore such existing knowledge to evaluate string similarity measures more effectively and efficiently, thereby boosting the quality of string matching.\n\nIn particular, we first present an expansion-based framework to measure string similarities efficiently while considering synonyms. Because using synonyms in similarity measures is, while expressive, computationally expensive (NP-hard), we propose an efficient algorithm, called selective-expansion, which guarantees the optimality in many real scenarios. We then study a novel indexing structure called SI-tree, which combines both signature and length filtering strategies, for efficient string similarity joins with synonyms. We develop an estimator to approximate the size of candidates to enable an online selection of signature filters to further improve the efficiency. This estimator provides strong low-error, high-confidence guarantees while requiring only logarithmic space and time costs, thus making our method attractive both in theory and in practice. Finally, the results from an empirical study of the algorithms verify the effectiveness and efficiency of our approach.", "title": "String Similarity Measures and Joins with Synonyms", "authors": [{"affiliation": "School of Information and DEKE, MOE, Renmin University of China", "location": "Beijing  China ", "name": "Jiaheng Lu", "email": "Lu"}, {"affiliation": "School of Information and DEKE, MOE, Renmin University of China;", "location": "Beijing  China ", "name": "Chunbin Lin", "email": "Lin"}, {"affiliation": "University of New South Wales", "location": "Sydney  Australia ", "name": "Wei Wang", "email": "Wang"}, {"affiliation": "University of California, Irvine", "location": "California  USA ", "name": "Chen Li", "email": "Li"}, {"affiliation": "School of Information and DEKE, MOE, Renmin University of China;", "location": "Beijing  China ", "name": "Haiyong Wang", "email": "Wang"}]}, "sig637": {"session": "Query Processing and Optimization", "abstract": "Reordering more than traditional joins (e.g. outerjoins, antijoins) requires some care, since not all reorderings are valid. To prevent invalid plans, two approaches have been described in the literature. We show that both approaches still produce invalid plans.\n\nWe present three conflict detectors. All of them are (1) correct, i.e., prevent invalid plans, (2) easier to understand and implement than the previous (buggy) approaches, (3) more flexible in the sense that the restriction that all predicates must reject nulls is no longer required, and (4) extensible in the sense that it is easy to add new operators. Further, the last of our three approaches is complete, i.e., it allows for the generation of all valid plans within the core search space.", "title": "On the Correct and Complete Enumeration of the Core Search Space", "authors": [{"affiliation": "University of Mannheim", "location": "Mannheim  Germany ", "name": "Guido Moerkotte", "email": "Moerkotte"}, {"affiliation": "University of Mannheim", "location": "Mannheim  Germany ", "name": "Pit Fender", "email": "Fender"}, {"affiliation": "University of Mannheim", "location": "Mannheim  Germany ", "name": "Marius Eich", "email": "Eich"}]}, "sig436": {"session": "Security", "abstract": "We consider a stream outsourcing setting, where a data owner\n\ndelegates the management of a set of disjoint data streams to an\n\nuntrusted server. The owner authenticates his streams via\n\nsignatures. The server processes continuous queries on the union of\n\nthe streams for clients trusted by the owner. Along with the results,\n\nthe server sends proofs of result correctness derived from the owner's\n\nsignatures, which are easily verifiable by the clients. We design\n\nnovel constructions for a collection of fundamental problems over streams\n\nrepresented as linear algebraic queries. In particular, our basic schemes\n\nauthenticate dynamic vector sums and dot products, \n\nas well as dynamic matrix products. These techniques can be adapted for authenticating a wide range of important operations in streaming environments, including group by queries, joins, in-network aggregation, similarity matching, and event processing. All our schemes are very lightweight, and offer strong cryptographic guarantees derived from formal definitions and proofs. We experimentally confirm the practicality of our schemes. ", "title": "Lightweight Authentication of Linear Algebraic Queries on Data Streams", "authors": [{"affiliation": "HKUST", "location": "Hong Kong  Hong Kong ", "name": "Stavros Papadopoulos", "email": "Papadopoulos"}, {"affiliation": "AT&T Labs-Research", "location": "Florham Park NJ USA ", "name": "Graham Cormode", "email": "Cormode"}, {"affiliation": "Technical University of Crete", "location": "Chania  Greece ", "name": "Antonios Deligiannakis", "email": "Deligiannakis"}, {"affiliation": "Technical University of Crete", "location": "Chania  Greece ", "name": "Minos Garofalakis", "email": "Garofalakis"}]}, "sig667": {"session": "Data Mining", "abstract": "Despite the wealth of research on frequent graph pattern mining, how to efficiently mine the complete set of those with constraints still poses a huge challenge to the existing algorithms mainly due to the inherent bottleneck in the mining paradigm. In essence, mining requests with explicitly-specified constraints cannot be handled in a way that is direct and precise.\n\nIn this paper, we propose a direct mining framework to solve the problem and illustrate our ideas in the context of a particular type of constrained\n\nfrequent patterns --- the ``skinny'' patterns, which are graph patterns with a long backbone\n\nfrom which short twigs branch out.  These patterns, which we formally define as $l$-long $\\delta$-skinny\n\npatterns, are able to reveal insightful spatial and temporal trajectory patterns in mobile data mining, information diffusion, adoption propagation, and many others.\n\n\n\nBased on the key concept of a \\emph{canonical diameter}, we develop \\textsf{SkinnyMine}, an efficient algorithm to mine all the $l$-long $\\delta$-skinny patterns guaranteeing\n\nboth the completeness of our mining result as well as the unique generation of each target pattern. We also present\n\na general direct mining framework together with two properties of \\emph{reducibility} and \\emph{continuity} for qualified constraints.\n\nOur experiments on both synthetic and real data demonstrate the effectiveness and scalability of our approach.", "title": "A Direct Mining Approach To Efficient Constrained Graph Pattern Discovery", "authors": [{"affiliation": "Singapore Management University", "location": "Singapore  Singapore School of Information Systems", "name": "Feida Zhu", "email": "Zhu"}, {"affiliation": "Unversity of Science and Technology of China", "location": "Hefei  China School of Computer Science", "name": "Zequn Zhang", "email": "Zhang"}, {"affiliation": "Aarhus University", "location": "Arhus  Denmark Department of Computer Science", "name": "Qiang Qu", "email": "Qu"}]}, "sig430": {"session": "Demo 3: Database Optimization; Performance", "abstract": "Sharing aggregate statistics of private data can be of great value when data mining can be performed in real-time to understand important phenomena such as influenza outbreaks or traffic congestion. However, to this date there have been no tools for releasing real-time aggregated data with differential privacy, a strong and provable privacy guarantee. We propose FAST, a real-time system that allows differentially private aggregate sharing and time-series analytics.   FAST employs a set of novel, adaptive strategies to improve the utility of shared/released data while guaranteeing the user-specified level of differential privacy.  We will demonstrate the challenges and our solutions in the context of prepared data sets as well as live participation data dynamically collected among the SIGMOD'13 attendees.  ", "title": "FAST: Differentially Private Real-Time Aggregate Monitor with Filtering and Adaptive Sampling", "authors": [{"affiliation": "Emory University", "location": "Atlanta GA USA ", "name": "Liyue Fan", "email": "Fan"}, {"affiliation": "Emory University", "location": "Atlanta GA USA ", "name": "Li Xiong", "email": "Xiong"}, {"affiliation": "Emory University", "location": "Atlanta GA USA ", "name": "Vaidy Sunderam", "email": "Sunderam"}]}, "sig538": {"session": "Demo 1: Data Intensive Applications", "abstract": "Although originally designed to accelerate pixel monsters, graphics Processing Units (GPUs) have been used for some time as accelerators for selected data base operations. However, to the best of our knowledge, no one has yet reported building a complete system that allows executing complex analytics queries, much less an entire data warehouse benchmark at realistic scale.  In this demo, we showcase such a complete system prototype running on a high-end GPU paired with an IBM storage system that achieves >90 % hardware efficiency. Our solution delivers sustainable high throughput for business analytics queries in a realistic scenario, i.e., the Star Schema Benchmark at scale factor 1,000. Attendees can interact with our system through a graphical user interface on a tablet PC. They will be able to experience first hand how queries that require processing more than six billion rows, or 100 GB of data, are answered in less than 20 seconds. The user interface allows submitting queries, live performance monitoring of the current query all the way down to the operator level, and viewing the result once the query completes. ", "title": "WoW: What the World of (Data) Warehousing Can Learn from the World of Warcraft", "authors": [{"affiliation": "IBM Research - Almaden", "location": "San Jose CA USA ", "name": "Rene Mueller", "email": "Mueller"}, {"affiliation": "IBM Research - Almaden", "location": "San Jose CA USA ", "name": "Tim Kaldewey", "email": "Kaldewey"}, {"affiliation": "IBM Research - Almaden", "location": "San Jose CA USA ", "name": "Guy Lohman", "email": "Lohman"}, {"affiliation": "IBM Research - Almaden", "location": "San Jose CA USA ", "name": "John McPherson", "email": "McPherson"}]}, "sig537": {"session": "Cloud computing", "abstract": " Computations performed by graph algorithms are data driven, and require a high degree of random data access. Despite the great progresses made in disk technology, it still cannot provide the level of efficient random access required by graph computation. On the other hand, memory-based approaches usually do not scale due to the capacity limit of single machines. In this paper, we introduce Trinity, a general purpose graph engine over a distributed memory cloud. Through optimized memory management and network communication, Trinity supports fast graph exploration as well as efficient parallel computing. In particular, Trinity leverages graph access patterns in both online and offline computation to optimize memory and communication for best performance. These enable Trinity to support efficient online query processing and offline analytics on large graphs with just a few commodity machines. Furthermore, Trinity provides a high level specification language called TSL for users to declare data schema and communication protocols, which brings great ease-of-use for general purpose graph management and computing. Our experiments show Trinity's performance in both low latency graph queries as well as high throughput graph analytics on web-scale, billion-node graphs.", "title": "Trinity: A Distributed Graph Engine on a Memory Cloud", "authors": [{"affiliation": "Microsoft Research Asia", "location": "Beijing  China ", "name": "Bin Shao", "email": "Shao"}, {"affiliation": "Microsoft Research Asia", "location": "Beijing  China ", "name": "Haixun Wang", "email": "Wang"}, {"affiliation": "HKUST", "location": "Hong Kong  Hong Kong ", "name": "Yatao Li", "email": "Li"}]}, "sig536": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "Acting on time-critical events by processing ever growing social media, news or cyber data streams is a major technical challenge.  Many of these data sources can be modeled as multi-relational graphs.  Mining and searching for subgraph patterns in a continuous setting requires an efficient approach to incremental graph search.  The goal of our work is to enable real-time search capabilities for graph databases.  This demonstration will present a dynamic graph query system that leverages the structural and semantic characteristics of the underlying multi-relational graph.  ", "title": "StreamWorks - A system for Dynamic Graph Search", "authors": [{"affiliation": "Pacific Northwest National Laboratory", "location": "Rchland WA USA ", "name": "Sutanay Choudhury", "email": "Choudhury"}, {"affiliation": "Washington State University", "location": "Pullman WA USA ", "name": "Lawrence Holder", "email": "Holder"}, {"affiliation": "Pacific Northwest National Laboratory", "location": "Richland WA USA ", "name": "George Chin", "email": "Chin"}, {"affiliation": "Washington State University", "location": "Pullman WA USA ", "name": "Abhik Ray", "email": "Ray"}, {"affiliation": "Pacific Northwest National Laboratory", "location": "Richland WA USA ", "name": "Sherman Beus", "email": "Beus"}, {"affiliation": "Pacific Northwest National Laboratory", "location": "Richland WA USA ", "name": "John Feo", "email": "Feo"}]}, "sig533": {"session": "Data Mining", "abstract": "Frequent sequence mining is one of the fundamental building blocks in data mining. While the problem has been extensively studied, few of the available techniques are sufficiently scalable to handle datasets with billions of sequences; such large-scale datasets arise, for instance, in text mining and session analysis. In this paper, we propose MG-FSM, a scalable algorithm for frequent sequence mining on MapReduce. MG-FSM can handle so-called \"gap constraints\", which can be used to limit the output to a controlled set of frequent sequences. At its heart, MG-FSM partitions the input database in a way that allows us to mine each partition independently using any existing frequent sequence mining algorithm. We introduce the notion of w-equivalency, which is a generalization of the notion of a \"projected database\" used by many frequent pattern mining algorithms. We also present a number of optimization techniques that minimize partition size, and therefore computational and communication costs, while still maintaining correctness. Our experimental study in the context of text mining suggests that MG-FSM is significantly more efficient and scalable than alternative approaches.", "title": "Mind the Gap: Large-Scale Frequent Sequence Mining", "authors": [{"affiliation": "Max Planck Institute for Informatics", "location": "Saarbr\u00fccken  Germany ", "name": "Iris Miliaraki", "email": "Miliaraki"}, {"affiliation": "Max Planck Institute for Informatics", "location": "Saarbr\u00fccken  Germany ", "name": "Klaus Berberich", "email": "Berberich"}, {"affiliation": "Max Planck Institute for Informatics", "location": "Saarbr\u00fccken  Germany ", "name": "Rainer Gemulla", "email": "Gemulla"}, {"affiliation": "Max Planck Institute for Informatics", "location": "Saarbr\u00fccken  Germany ", "name": "Spyros Zoupanos", "email": "Zoupanos"}]}, "pods034": {"session": "Indexing/Query Answering", "abstract": "Bounded Derivation Depth property (BDD) and Finite Controllability (FC) are two \n\nproperties of sets of datalog rules and tuple generating dependencies (known as Datalog$^\\exists$ programs), \n\nwhich recently attracted  some attention. \n\nWe conjecture that the first of these properties implies the second, and support this conjecture \n\nby some evidence proving, among other results, that it holds true for all theories over binary signature.", "title": "On the BDD/FC Conjecture", "authors": [{"affiliation": "University of Wroc?aw", "location": "Wroc?aw  Poland Institute of Computer Science", "name": "Tomasz Gogacz", "email": "Gogacz"}, {"affiliation": "University of Wroc?aw", "location": "Wroc?aw  Poland Institute of Computer Science", "name": "Jerzy Marcinkowski", "email": "Marcinkowski"}]}, "sig627": {"session": "Cloud computing", "abstract": "\\begin{abstract}\n\nA multitenant database management system (DBMS) in the cloud must continuously monitor the trade-off between efficient resource sharing among multiple application databases (tenants) and their performance. Considering the scale of \\attn{hundreds to} thousands of tenants in such multitenant DBMSs, manual approaches for continuous monitoring are not tenable. A self-managing controller of a multitenant DBMS faces several challenges. For instance, how to characterize a tenant given its variety of workloads, how to reduce the impact of tenant colocation,  and how to detect and mitigate a performance crisis where one or more tenants' desired service level objective (SLO) is not achieved.\n\n\n\nWe present \\textbf{\\controller}, a self-managing system controller for a multitenant DBMS, and \\textbf{\\classifier}, a technique to learn behavior through observation and supervision using DBMS-agnostic database level performance measures. \\classifier\\ accurately learns tenant behavior even when multiple tenants share a database process, learns good and bad tenant consolidation plans (or packings), and maintains a per-tenant history to detect behavior changes.  \\attn{\\controller\\ detects performance crises, and leverages \\classifier\\ to suggests remedial actions using a hill-climbing search algorithm to identify a new tenant placement strategy to mitigate violating SLOs}. Our evaluation using a variety of tenant types and workloads shows that \\classifier\\ can learn a tenant's behavior with more than $92\\%$ accuracy and learn the quality of packings with more than $86\\%$ accuracy. During a performance crisis, \\controller\\ is able to reduce 99th percentile latencies by 80\\%, and can consolidate 45\\% more tenants than a greedy baseline, \\attn{which balances tenant load without modeling tenant behavior}.\n\n\\end{abstract}", "title": "Characterizing Tenant Behavior for Placement and Crisis Mitigation in Multitenant DBMSs", "authors": [{"affiliation": "UC Santa Barbara", "location": "Santa Barbara CA USA ", "name": "Aaron Elmore", "email": "Elmore"}, {"affiliation": "Microsoft Research", "location": "Redmond WA USA ", "name": "Sudipto Das", "email": "Das"}, {"affiliation": "UC Santa Barbara", "location": "Santa Barbara CA USA ", "name": "Alexander Pucher", "email": "Pucher"}, {"affiliation": "UC Santa Barbara", "location": "Santa Barbara CA USA ", "name": "Divyakant Agrawal", "email": "Agrawal"}, {"affiliation": "UC Santa Barbara", "location": "Santa Barbara CA USA ", "name": "Amr El Abbadi", "email": "El Abbadi"}, {"affiliation": "UC Santa Barbara", "location": "Santa Barbara CA USA ", "name": "Xifeng Yan", "email": "Yan"}]}, "sig020": {"session": "Industrial 1: Big Data I", "abstract": "The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.", "title": "The Big Data\" Ecosystem at LinkedIn\"", "authors": [{"affiliation": "LinkedIn", "location": "Mountain View CA USA ", "name": "Roshan Sumbaly", "email": "Sumbaly"}, {"affiliation": "LinkedIn", "location": "Mountain View CA USA ", "name": "Jay Kreps", "email": "Kreps"}, {"affiliation": "LinkedIn", "location": "Mountain View CA USA ", "name": "Sam Shah", "email": "Shah"}]}, "sig624": {"session": "Indexing", "abstract": "Large scale data warehouses rely heavily on secondary indexes, such as bitmaps\n\nand b-trees, to limit access to slow IO devices. However, with the advent\n\nof large main memory systems, cache conscious secondary indexes are needed to\n\nimprove also the transfer bandwidth between memory and cpu. In this paper, we\n\nintroduce column imprint, a simple but efficient cache conscious secondary\n\nindex. A column imprint is a collection of many small bit vectors, each\n\nindexing the data points of a single cacheline. An imprint is used during query\n\nevaluation to limit data access and thus minimize memory traffic. The\n\ncompression for imprints is cpu friendly and exploits the empirical observation\n\nthat data often exhibits local clustering or partial ordering as a side-effect\n\nof the construction process. Most importantly, column imprint compression\n\nremains effective and robust even in the case of unclustered data, while other\n\nstate-of-the-art solutions fail. We conducted an extensive experimental\n\nevaluation to assess the applicability and the performance impact of the column\n\nimprints. The storage overhead, when experimenting with real world datasets, is\n\njust a few percent over the size of the columns being indexed. The evaluation\n\ntime for over 40000 range queries of varying selectivity revealed the\n\nefficiency of the proposed index compared to zonemaps and bitmaps with WAH\n\ncompression.", "title": "Column Imprints: A Secondary Index Structure", "authors": [{"affiliation": "CWI", "location": "Amsterdam  Netherlands ", "name": "Lefteris Sidirourgos", "email": "Sidirourgos"}, {"affiliation": "CWI", "location": "Amsterdam  Netherlands ", "name": "Martin Kersten", "email": "Kersten"}]}, "sig623": {"session": "Complex Event Processing", "abstract": "Complex Event Processing (CEP) has emerged as a technology for monitoring event streams in search of user specified event patterns. When a CEP system is deployed in sensitive environments the user may wish to mitigate leaks of private information while ensuring that useful nonsensitive patterns are still reported. In this paper we consider how to suppress events in a stream to reduce the disclosure of sensitive patterns while maximizing the detection of nonsensitive patterns. We first formally define the problem of utility-maximizing event suppression with privacy preferences, and analyze its computational hardness. We then design a suite of real-time solutions to solve this problem. Our first solution optimally solves the problem at the event-type level. The second solution, at the event-instance level, further optimizes the event-type level solution by exploiting runtime event distributions using advanced pattern match cardinality estimation techniques. Our user study and experimental evaluation over both real-world and synthetic event streams show that our algorithms are effective in maximizing utility yet still efficient enough to offer near real-time system responsiveness.", "title": "Utility-Maximizing Event Stream Suppression", "authors": [{"affiliation": "Worcester Polytechnic Institute", "location": "Worcester MA USA ", "name": "Di Wang", "email": "Wang"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Yeye He", "email": "He"}, {"affiliation": "Worcester Polytechnic Institute", "location": "Worcester  MA USA ", "name": "Elke Rundensteiner", "email": "Rundensteiner"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Jeffrey Naughton", "email": "Naughton"}]}, "sig622": {"session": "Privacy", "abstract": "Existing \\emph{differential privacy}~(DP) studies mainly consider aggregation on data sets where each entry corresponds to a particular participant to be protected. In many situations, a user may pose a relational algebra query on a database with sensitive data, and desire differentially private aggregation on the result of the query. However, no existing work is able to release such aggregation when the query contains unrestricted join operations. This severely limits the applications of existing DP techniques because many data analysis tasks require unrestricted joins. One example is subgraph counting on a graph. Furthermore, existing methods for differentially private subgraph counting support only edge DP and are subject to very simple subgraphs. Until recent, whether any nontrivial graph statistics can be released with reasonable accuracy for arbitrary kind of input graphs under node DP was still an open problem.\n\n\n\nIn this paper, we propose a novel differentially private mechanism that supports unrestricted joins, to release an approximation of a linear statistic of the result of some positive relational algebra calculation over a sensitive database. The error bound of the approximate answer is roughly proportional to the \\emph{empirical sensitivity} of the query --- a new notion that measures the maximum possible change to the query answer when a participant withdraws its data from the sensitive database. For subgraph counting, our mechanism provides a solution to achieve node DP, for any kind of subgraphs.\n\n", "title": "Recursive Mechanism: Towards Node Differential Privacy and Unrestricted Joins", "authors": [{"affiliation": "Fudan University", "location": "Shanghai  China ", "name": "Shixi Chen", "email": "Chen"}, {"affiliation": "Fudan University", "location": "Shanghai  China ", "name": "Shuigeng Zhou", "email": "Zhou"}]}, "sig621": {"session": "Road Networks and Trajectories", "abstract": "Due to the  prevalence of GPS-enabled devices and wireless communications technologies, spatial trajectories that describe the movement history of moving objects are being generated and accumulated at an unprecedented pace. Trajectory data in a database are intrinsically   heterogeneous, as they represent discrete approximations of  original continuous paths derived using different sampling strategies and different sampling rates. Such heterogeneity can have a negative impact on the effectiveness of trajectory similarity measures, which are the basis of many crucial trajectory processing tasks. In this paper, we pioneer a systematic approach to trajectory calibration that is a process to transform a heterogeneous trajectory dataset to one with (almost) unified sampling strategies.  Specifically, we propose an anchor-based calibration system that aligns trajectories to a set of anchor points, which are fixed locations independent of  trajectory data. After examining  four different types of anchor points for the purpose of building a stable reference system, we propose a geometry-based calibration approach that considers the spatial relationship between anchor points and trajectories. Then a more advanced model-based calibration method is presented, which exploits the power of machine learning techniques to train inference models from historical trajectory data to improve  calibration effectiveness. Finally, we conduct extensive experiments using real trajectory datasets to demonstrate the effectiveness and efficiency of the proposed calibration system. ", "title": "Calibrating Trajectory Data for Similarity-based Analysis", "authors": [{"affiliation": "University of Queensland", "location": "Brisbane  Australia ", "name": "Han Su", "email": "Su"}, {"affiliation": "University of Queensland", "location": "Brisbane  Australia ", "name": "Kai Zheng", "email": "Zheng"}, {"affiliation": "University of Queensland", "location": "Brisbane  Australia ", "name": "Haozhou Wang", "email": "Wang"}, {"affiliation": "Nanjing University", "location": "Nanjing  China ", "name": "Jiamin Huang", "email": "Huang"}, {"affiliation": "University of Queensland", "location": "Brisbane  Australia ", "name": "Xiaofang Zhou", "email": "Zhou"}]}, "sig620": {"session": "Distributed Systems", "abstract": "In the cloud services industry, a key issue for cloud operators is to minimize operational costs. In this paper, we consider algorithms that elastically contract and expand a cluster of in-memory databases depending on tenants' behavior over time while maintaining response time guarantees.\n\n\n\nWe evaluate our tenant placement algorithms using traces obtained from one of SAP's production on-demand applications. Our experiments reveal that our approach lowers operating costs for the database cluster of this application by a factor of 2.2 to 10, measured in Amazon EC2 hourly rates, in comparison to the state of the art. \n\n\n\nIn addition, we carefully study the trade-off between cost savings obtained by continuously migrating tenants and the robustness of servers towards load spikes and failures.", "title": "RTP: Robust Tenant Placement for Elastic In-Memory Database Clusters", "authors": [{"affiliation": "Hasso Plattner Institute", "location": "Potsdam  Germany ", "name": "Jan Schaffner", "email": "Schaffner"}, {"affiliation": "SAP AG", "location": "Walldorf  Germany ", "name": "Tim Januschowski", "email": "Januschowski"}, {"affiliation": "SAP AG", "location": "Walldorf  Germany ", "name": "Megan Kercher", "email": "Kercher"}, {"affiliation": "Brown University", "location": "Providence  USA ", "name": "Tim Kraska", "email": "Kraska"}, {"affiliation": "Hasso Plattner Institute", "location": "Potsdam  Germany ", "name": "Hasso Plattner", "email": "Plattner"}, {"affiliation": "UC Berkeley", "location": "Berkeley  USA AMPLab", "name": "Michael Franklin", "email": "Franklin"}, {"affiliation": "SAP AG", "location": "Walldorf  Germany ", "name": "Dean Jacobs", "email": "Jacobs"}]}, "sig152": {"session": "Industrial 4: Systems & New Hardware Trends", "abstract": "Hekaton is a new database engine optimized for memory resident data and OLTP workloads. Hekaton is fully integrated into SQL Server; it is not a separate system. To take advantage of Hekaton, a user simply declares a table memory optimized. Hekaton tables are fully transactional and durable and accessed using T-SQL in the same way as regular SQL Server tables. A query can reference both Hekaton tables and regular tables and a transaction can update data in both types of tables. T-SQL stored procedures that reference only Hekaton tables can be compiled into machine code for further performance improvements. The engine is designed for high con-currency. To achieve this it uses only latch-free data structures and a new optimistic, multiversion concurrency control technique. This paper gives an overview of the design of the Hekaton engine and reports some experimental results.", "title": "Hekaton: SQL Server\u0092s Memory-Optimized OLTP Engine", "authors": [{"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Cristian Diaconu", "email": "Diaconu"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Craig Freedman", "email": "Freedman"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Erik Ismert", "email": "Ismert"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Per-Ake Larson", "email": "Larson"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Pravin Mittal", "email": "Mittal"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Ryan Stonecipher", "email": "Stonecipher"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Nitin Verma", "email": "Verma"}, {"affiliation": "Microsoft", "location": "Redmond WA USA ", "name": "Mike Zwilling", "email": "Zwilling"}]}, "sig629": {"session": "Data Cleaning", "abstract": "The relative accuracy problem is to determine, given tuples $t_1$ and $t_2$ that refer to the same entity $e$, whether $t_1[A]$ is more accurate than $t_2[A]$, i.e., $t_1[A]$ is closer to the true value of the $A$ attribute of $e$ than $t_2[A]$. This has been a longstanding issue for data quality, and is challenging when the true values of $e$ are unknown. This paper proposes a model for determining relative accuracy. \n\n(1) We introduce a class of accuracy rules and an inference system with a chase procedure, to deduce relative accuracy. \n\n(2) We identify and study several fundamental problems for relative accuracy. Given a set $I_e$ of tuples pertaining to the same entity $e$ and a set of accuracy rules, these problems are to decide whether the chase process terminates, is Church-Rosser, and leads to a unique target tuple $t_e$ composed of the most accurate values from $I_e$ for all the attributes of $e$. \n\n(3) We propose a framework for inferring accurate values with user interaction. \n\n(4) We provide algorithms underlying the framework, to find the unique target tuple $t_e$ whenever possible; when there is no enough information to decide a complete $t_e$, we compute top-$k$ candidate targets based on a preference model. \n\n(5) Using real-life and synthetic data, we experimentally verify the effectiveness and efficiency of our method.", "title": "Determining the Relative Accuracy of Attributes", "authors": [{"affiliation": "School of Informatics, University of Edinburgh; Big Data Research Center and SKLSDE Lab, Beihang University", "location": "Beijing  China ", "name": "Yang Cao", "email": "Cao"}, {"affiliation": "School of Informatics, University of Edinburgh; Big Data Research Center and SKLSDE Lab, Beihang University", "location": "Edinburgh  United Kingdom ", "name": "Wenfei Fan", "email": "Fan"}, {"affiliation": "School of Informatics, University of Edinburgh", "location": "Edinburgh  United Kingdom ", "name": "Wenyuan Yu", "email": "Yu"}]}, "sig487": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "A large body of recent work has proposed analytical and empirical\n\ntechniques for quantifying the data consistency properties of\n\ndistributed data stores. In this demonstration, we begin to explore\n\nthe wide range of new database functionality they enable, including\n\ndynamic query tuning, consistency SLAs, monitoring, and\n\nadministration. Our demonstration will exhibit how both application\n\nprogrammers and database administrators can leverage these features.\n\nWe describe three major application scenarios and present a system\n\narchitecture for supporting them. We also describe our experience in\n\nintegrating Probabilistically Bounded Staleness (PBS) predictions into\n\nCassandra, a popular NoSQL store and sketch a demo platform that will\n\nallow SIGMOD attendees to experience the importance and applicability\n\nof real-time consistency metrics.", "title": "PBS at Work: Advancing Data Management with Consistency Metrics", "authors": [{"affiliation": "UC Berkeley", "location": "Berkeley CA USA ", "name": "Peter Bailis", "email": "Bailis"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA ", "name": "Shivaram Venkataraman", "email": "Venkataraman"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA ", "name": "Michael Franklin", "email": "Franklin"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA ", "name": "Joseph Hellerstein", "email": "Hellerstein"}, {"affiliation": "UC Berkeley", "location": "Berkeley CA USA ", "name": "Ion Stoica", "email": "Stoica"}]}, "sig149": {"session": "Systems, Performance II", "abstract": "Factor graphs and Gibbs sampling are a popular combination for\n\nBayesian statistical methods that are used to solve diverse problems\n\nincluding insurance risk models, pricing models, and information\n\nextraction. Given a fixed sampling method and a fixed amount of time,\n\nan implementation of a sampler that achieves a higher throughput of\n\nsamples will achieve a higher quality than a lower-throughput\n\nsampler. We study how (and whether) traditional data processing\n\nchoices about materialization, page layout, and buffer-replacement\n\npolicy need to be changed to achieve high-throughput Gibbs sampling\n\nfor factor graphs that are larger than main memory. We find that both\n\nnew theoretical and new algorithmic techniques are required to understand\n\nthe tradeoff space for each choice. On both real and synthetic data,\n\nwe demonstrate that traditional baseline approaches may achieve two\n\norders of magnitude lower throughput than an optimal approach. For a\n\nhandful of popular tasks across several storage backends, including\n\nHBase and traditional unix files, we show that our simple prototype\n\nachieves competitive (and sometimes better) throughput compared to\n\nspecialized state-of-the-art approaches on factor graphs that are\n\nlarger than main memory.", "title": "Towards High-Throughput Gibbs Sampling at Scale: A Study across Storage Managers", "authors": [{"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Ce Zhang", "email": "Zhang"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA ", "name": "Christopher R\u00e9", "email": "R\u00e9"}]}, "sig371": {"session": "Query Processing and Optimization", "abstract": "Top-k queries return to the user only the k best objects based on the individual user preferences and comprise an essential tool for rank-aware query processing. Assuming a stored data set of user preferences, reverse top-k queries have been introduced for retrieving the users that deem a given database object as one of their top-k results. Reverse top-k queries have already attracted significant interest in research, due to numerous real-life applications such as market analysis and product placement. Currently, the most efficient algorithm for computing the reverse top-k set is RTA. RTA has two main drawbacks when processing a reverse top-k query: (i) it needs to access all stored user preferences, and (ii) it cannot avoid executing a top-k query for each user preference that belongs to the result set. To address these limitations, in this paper, we identify useful properties for processing reverse top-k queries without accessing each user's individual preferences nor executing the top-k query. We propose an intuitive branch-and-bound algorithm for processing reverse top-k queries efficiently and discuss novel optimizations to boost its performance. Our experimental evaluation demonstrates the efficiency of the proposed algorithm that outperforms RTA by a large margin.", "title": "Branch-and-Bound Algorithm for Reverse Top-k Queries", "authors": [{"affiliation": "Norwegian University of Science and Technology (NTNU)", "location": "Trondheim  Norway ", "name": "Akrivi Vlachou", "email": "Vlachou"}, {"affiliation": "University of Piraeus", "location": "Piraeus  Greece ", "name": "Christos Doulkeridis", "email": "Doulkeridis"}, {"affiliation": "Norwegian University of Science and Technology (NTNU)", "location": "Trondheim  Norway ", "name": "Kjetil N\u00f8rv\u00e5g", "email": "N\u00f8rv\u00e5g"}, {"affiliation": "Athens University of Economics and Business (AUEB)", "location": "Athens  Greece ", "name": "Yannis Kotidis", "email": "Kotidis"}]}, "sig140": {"session": "Query Processing and Optimization", "abstract": "Fast and accurate estimations for complex queries are profoundly beneficial for large databases with heavy workloads. In this research, we propose a statistical summary for a database, called CS2 (Correlated Sample Synopsis), to provide rapid and accurate result size estimations for all queries with joins and arbitrary selections. Unlike the state-of-the-art techniques, CS2 does not completely rely on simple random samples, but mainly consists of correlated sample tuples that retain join relationships with less storage. We introduce a statistical technique, called reverse sample, and design a powerful estimator, called reverse estimator, to fully utilize correlated sample tuples for query estimation. We prove both theoretically and empirically that the reverse estimator is unbiased and accurate using CS2. Extensive experiments on multiple datasets show that CS2 is fast to construct and derives more accurate estimations than existing methods with the same space budget.", "title": "CS2: A New Database Synopsis for Query Estimation", "authors": [{"affiliation": "Southern Illinois University", "location": "Carbondale IL USA Department of Computer Science", "name": "Feng Yu", "email": "Yu"}, {"affiliation": "Southern Illinois University", "location": "Carbondale IL USA Department of Computer Science", "name": "Wen-Chi Hou", "email": "Hou"}, {"affiliation": "Coppin State University", "location": "Baltimore MD USA ", "name": "Cheng Luo", "email": "Luo"}, {"affiliation": "Southern Illinois University", "location": "Carbondale IL USA Department of Computer Science", "name": "Dunren Che", "email": "Che"}, {"affiliation": "Southern Illinois University", "location": "Carbondale IL USA Department of Computer Science", "name": "Mengxia Zhu", "email": "Zhu"}]}, "sig376": {"session": "Demo 1: Data Intensive Applications", "abstract": "We present the WebdamLog system for managing distributed\n\ndata on the Web in a peer-to-peer manner. We demonstrate the main features of the system through an application called Wepic for sharing pictures between attendees\n\nof the sigmod conference. Using Wepic, the attendees will\n\nbe able to share, download, rate and annotate pictures in\n\na highly decentralized manner. We show how WebdamLog\n\nhandles heterogeneity of the devices and services used to\n\nshare data in such a Web setting. We exhibit the simple\n\nrules that define the Wepic application and show how to\n\neasily modify the Wepic application.\n\n", "title": "Rule-Based Application Development using Webdamlog", "authors": [{"affiliation": "Inria Saclay & ENS Cachan ", "location": "Cachan  France ", "name": "Serge Abiteboul", "email": "Abiteboul"}, {"affiliation": "Inria Saclay & ENS Cachan ", "location": "Cachan  France ", "name": "\u00c9milien Antoine", "email": "Antoine"}, {"affiliation": "Inria Saclay & UMass Amherst ", "location": "Amherst   USA ", "name": "Gerome Miklau", "email": "Miklau"}, {"affiliation": "Drexel University & Skoltech ", "location": "Philadelphia  USA ", "name": "Julia Stoyanovich", "email": "Stoyanovich"}, {"affiliation": "Inria Saclay & McGill U. ", "location": "Montr\u00e9al  Canada ", "name": "Jules Testard", "email": "Testard"}]}, "sig541": {"session": "Demo 4: Graphs and Networks; Potpourrice", "abstract": "A process trace describes the processes taken in a workflow to generate a particular result. Given many process traces, each with a large amount of very low level information, it is a challenge to make process traces meaningful to different users. It is more challenging to compare two complex process traces generated by heterogenous systems and have different levels of granularity. We present \\textsf{CTrace}, a system that (1) lets users explore the conceptual abstraction of large process traces with different levels of granularity, and (2) provides semantic comparison among traces in which both the structural and the semantic similarity are considered. The above functions are underpinned by a novel notion of multi-granularity process trace and efficient multi-granularity similarity comparison algorithms.\n\n", "title": "CTrace: Semantic Comparison of Multi-Granularity Process Traces", "authors": [{"affiliation": "Intelligent Sensing and System Lab, CSIRO", "location": "Hobart  Australia ", "name": "Qing Liu", "email": "Liu"}, {"affiliation": "Information Engineering Lab, CSIRO", "location": "Canberra  Australia ", "name": "Kerry Taylor", "email": "Taylor"}, {"affiliation": "University of New South Wales", "location": "Sydney  Australia ", "name": "Xiang Zhao", "email": "Zhao"}, {"affiliation": "Information Engineering Lab, CSIRO", "location": "Canberra  Australia ", "name": "Geoffrey Squire", "email": "Squire"}, {"affiliation": "University of New South Wales", "location": "Sydney  Australia ", "name": "Xuemin Lin", "email": "Lin"}, {"affiliation": "Intelligent Sensing and Systems Lab, CSIRO", "location": "Hobart  Australia ", "name": "Corne Kloppers", "email": "Kloppers"}, {"affiliation": "Intelligent Sensing and Systems Lab, CSIRO", "location": "Hobart  Australia ", "name": "Richard Miller", "email": "Miller"}]}, "pods13": {"session": "Welcome", "abstract": "", "title": "The ACM PODS Alberto O. Mendelzon test-of-time award 2013", "authors": [{"affiliation": "University of Oxford", "location": "Oxford  United Kingdom ", "name": "Michael Benedikt", "email": "Benedikt"}, {"affiliation": "Tel Aviv University", "location": "Tel Aviv  Israel ", "name": "Tova Milo", "email": "Milo"}, {"affiliation": "Indiana University", "location": "Bloomington  USA ", "name": "Dirk Van Gucht", "email": "Van Gucht"}]}, "sig545": {"session": "Industrial 3: Big Data II & Web", "abstract": "A knowledge base (KB) contains a set of concepts, instances, and relationships. Over the past decade, numerous KBs have been built, and used to power a growing array of applications. Despite this flurry of activities, however, surprisingly little has been published about the end-to-end process of building, maintaining, and using such KBs in industry. In this paper we describe such a process. In particular, we describe how we build, update, and curate a large KB at Kosmix, a Bay Area startup, and later at WalmartLabs, a development and research lab of Walmart. We discuss how we use this KB to power a range of applications, including query understanding, Deep Web search, in-context advertising, event monitoring in social media, product search, social gifting, and social mining. Finally, we discuss how the KB team is organized, and the lessons learned. Our goal with this paper is to provide a real-world case study, and to contribute to the emerging direction of building, maintaining, and using knowledge bases for data management applications.", "title": "Building, Maintaining, and Using Knowledge Bases: A Report from the Trenches", "authors": [{"affiliation": "@WalmartLabs", "location": "San Bruno CA USA ", "name": "Omkar Deshpande", "email": "Deshpande"}, {"affiliation": "@WalmartLabs", "location": "San Bruno CA USA ", "name": "Digvijay Lamba", "email": "Lamba"}, {"affiliation": "Google", "location": "New York City NY USA ", "name": "Michel Tourn", "email": "Tourn"}, {"affiliation": "University of Wisconsin-Madison", "location": "Madison WI USA Computer Sciences", "name": "Sanjib Das", "email": "Das"}, {"affiliation": "@WalmartLabs", "location": "San Bruno CA USA ", "name": "Sri Subramaniam", "email": "Subramaniam"}, {"affiliation": "Formerly at @WalmartLabs", "location": "San Bruno CA USA ", "name": "Anand Rajaraman", "email": "Rajaraman"}, {"affiliation": "Formerly at @WalmartLabs", "location": "San Bruno CA USA ", "name": "Venky Harinarayan", "email": "Harinarayan"}, {"affiliation": "@WalmartLabs", "location": "San Bruno CA USA ", "name": "AnHai Doan", "email": "Doan"}]}}