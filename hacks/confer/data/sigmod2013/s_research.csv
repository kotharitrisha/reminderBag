Audience,Session,Session Name,ID,Paper Title,Abstract,Authors,Author Emails,Subject Areas,Tuesday Morning,,,,,,,,,"SIGMOD Keynote I &Welcome",,Welcome by Organizers,,,,,,,,"Keynote: Data Management in Capital Marketsby Alex Nazaruk and Michael Rauchman",,"Alex Nazaruk (Partner, Middle Lake Partners, LLC), Michael Rauchman (Partner, Middle Lake Partners, LLC)",,150,1,Data Analytics,174,ABCDE: Optimizing Statistical Data Analysis in the Cloud,"We present ABCDE, a system designed to help users rapidly develop and intelligently deploy matrix-based big-data analysis programs in the cloud. ABCDE features a flexible execution model and new operators especially suited for such workloads. We show how to implement ABCDE on top of Hadoop/HDFS while avoiding limitations of MapReduce, and demonstrate ABCDE's performance advantages over existing Hadoop-based systems for statistical data analysis. To support intelligent deployment in the cloud according to time/budget constraints, ABCDE goes beyond database-style optimization to make choices automatically on not only physical operators and their parameters, but also hardware provisioning and configuration settings. We apply a suite of benchmarking, simulation, modeling, and search techniques to support effective cost-based optimization over this rich space of deployment plans.","Botong Huang*, Duke University; Shivnath Babu, Duke; Jun Yang, Duke University",bhuang@cs.duke.edu; shivnath@cs.duke.edu; junyang@cs.duke.edu,"Cloud Computing, Map Reduce, Parallel, Distributed, P2P Systems*",,,576,Shark: SQL and Rich Analytics at Scale,"AnonSys is a new data analysis system that marries query processing with complex analytics on large clusters. It leverages a novel distributed memory abstraction to provide a unified engine that can run SQL queries and sophisticated analytics functions (e.g. iterative machine learning) at scale, and recovers from failures efficiently mid-query. This allows AnonSys to run SQL queries up to 100X faster than Apache Hive, and machine learning programs up to 100X faster than Hadoop. Unlike previous systems, AnonSys shows that it is possible to achieve these speedups while retaining a MapReduce-like execution engine, and the fine-grained fault tolerance properties that such engines provide. It extends such an engine in several ways, including column-oriented in-memory storage and dynamic mid-query replanning, to effectively execute SQL. The result is a system that matches the speedups reported for MPP analytic databases over MapReduce, while offering fault tolerance properties and complex analytics capabilities that they lack.","Reynold Xin*, UC Berkeley; Josh Rosen, ; Matei Zaharia, ; Michael Franklin, UC Berkeley; Ion Stoica, UC Berkeley; Scott Shenker,",rxin@cs.berkeley.edu; joshrosen@eecs.berkeley.edu; matei@eecs.berkeley.edu; franklin@cs.berkeley.edu; istoica@cs.berkeley.edu; shenker@icsi.berkeley.edu,"Cloud Computing, Map Reduce, Parallel, Distributed, P2P Systems*",,,617,Parallel Analytics as a Service,"Recently, massively parallel processing database systems (MPPDBs) have gained much momentum in the big data analytic market. With the advent of hosted cloud computing, we envision that the offering of MPPDB-as-a-Service (MPPDBaaS) will become attractive for companies having analytical tasks on only hundreds gigabytes to some ten terabytes of data because they can enjoy high-end parallel analytics at a cheap cost. This paper presents Thrifty, a prototype implementation of MPPDB as a service. The major research issue is how to achieve a lower total cost of ownership by consolidating thousands of MPPDB tenants on to a shared hardware infrastructure, with a performance SLA that guarantees the tenants can obtain the query results as if they are executing their queries on dedicated machines. Thrifty achieves the goal by using a tenant-driven design that includes (1) a cluster design that carefully arranges the nodes in the cluster into groups and creates an MPPDB for each group of nodes, (2) a tenant placement that assigns each tenant to several MPPDBs (for high availability service through replication), and (3) a query routing algorithm that routes a tenant's query to the proper MPPDB at run-time. Experiments show that in a MPPDBaaS with 5000 tenants, where each tenant requests two to ten nodes MPPDB to query against 200GB to 1TB of data, Thrifty can serve all the tenants with a 99.9\% performance SLA guarantee and a high availability replication factor of 3, using only 21\% of the nodes requested by the tenants.","Petrie Wong, The Hong Kong Polytechnic University; Eric Lo*, Hong Kong Polytechnic Universi; Zhian He, Hong Kong Polytechnic University",cskfwong@comp.polyu.edu.hk; ericlo@comp.polyu.edu.hk; cszahe@comp.polyu.edu.hk,"Cloud Computing, Map Reduce, Parallel, Distributed, P2P Systems*"50,2,XML,87,MESSIAH: Missing Element-Conscious SLCA Nodes Search in XML Data,"Keyword search for smallest lowest common ancestors (SLCAs) in XML data has been widely accepted as a meaningful way to identify matching nodes where their subtrees contain an input set of keywords. Although SLCA and its variants (e.g.,MLCA) perform admirably in identifying matching nodes, surprisingly, they perform poorly for searches on irregular schemas that have missing elements, that is, (sub)elements that are optional, or appear in some instances of an element type but not all, e.g., a < population > subelement in a < city > element might be optional appearing when the population is known and absent when the population is unknown. In this paper, we generalize the SLCA search paradigm to support queries involving missing elements. Specifically, we propose a novel property called optionality-resilience that specifies the desired behaviors of an XKS approach for queries involving missing elements. Then, we present two variants of a novel algorithm called MESSIAH (Missing Element-conSciouS hIgh-quality SLCA searcH), which are optionality resilient to irregular documents. MESSIAH logically transforms an XML document to a minimal full document where all missing elements are represented as empty elements, i.e., the irregular schema is made “regular”, and then employs efficient strategies to identify partial and complete full SLCA nodes (SLCA nodes in the full document) from it. Specifically, it generates the same SLCA nodes as any state-of-the-art approach when the query does not involve missing elements but avoids irrelevant results when missing elements are involved. Our experimental study demonstrates the ability of MESSIAH to produce superior quality search results.","Ba Quan Truong, Nanyang Technological Univ; Sourav S Bhowmick*, Nanyang Technological Univ; Curtis Dyreson, Utah State University; Aixin Sun, Nanyang Technological Univ",bqtruong@ntu.edu.sg; assourav@ntu.edu.sg; curtis.dyreson@usu.edu; axsun@ntu.edu.sg,"Text Databases, XML, Keyword Search*",,,635,Indexing for Subtree Similarity-Search,"Given a tree Q and a large set of trees T ={t1,...,tn}, the subtree similarity-search problem is that of finding the subtrees of trees among T that are most similar to Q, using the tree edit distance metric. Subtree similarilty-search is a convenient way to query large XML documents. In particular, it is more user-friendly than precise query languages, such as XQuery, and allows greater expressivity than standard keyword search. While subtree similarity-search has been studied in the past, solutions required traversal of all of T, which poses a severe bottleneck in processing time, as T grows larger. This paper proposes the first index structure for subtree similarity-search, provided that the unit cost function is used. Extensive experimentation and comparison to previous work shows the huge improvement gained when using the proposed index structure and processing algorithm.","Sara Cohen*, Hebrew University of Jerusalem",sara@cs.huji.ac.il,"Text Databases, XML, Keyword Search*",,,248,Discovering XSD keys from XML data,"The absence of XML Schemas for the numerous XML documents on the Web has initiated a great deal of research on learning of schemas from XML data. Unfortunately, in strong contrast to, for instance, the relational model, the automatic discovery of even the simplest of XML constraints, XML keys, has been left largely unexplored. A major obstacle here is the unavailability of a theory on reasoning about XML keys in the presence of XML schemas, needed to validate the quality of candidate keys. The present paper embarks on such a fundamental study classifying the complexity of several crucial properties concerning XML keys like, for instance, testing for consistency, boundedness, satisfiability, universality, and equivalence of XML keys in the presence of an XML Schema. Of independent interest, novel results are obtained related to cardinality estimation of XPath result sets. A mining algorithm is then developed within the framework of levelwise search which additionally leverages on discovery algorithms for functional dependencies in the relational model. Furthermore, the mining algorithm incorporates the above mentioned properties to refine the quality of derived keys. An experimental study on an extensive body of real world XML data is provided evaluating the effectiveness of the proposed algorithm.","Marcelo Arenas, PUC Chile; Jonny Daenen, ; Frank Neven*, Hasselt University; Jan Van den Bussche, ; Martin Ugarte, ; Stijn Vansummeren,",marenas@ing.puc.cl; jonny.daenen@uhasselt.be; frank.neven@uhasselt.be; jan.vandenbussche@uhasselt.be; mgugarte@uc.cl; stijn.vansummeren@ulb.ac.be,"Text Databases, XML, Keyword Search*"100,3,Transactions,53,A Scalable Lock Manager for Multicores,"Modern implementations of DBMS software are intended to take advantage of high core counts that are becoming common in high-end servers. However, we have observed that several database platforms, including MySQL, Shore-MT, and a commercial system, exhibit throughput collapse as load increases, even for a workload with little or no logical contention for locks. Our analysis of MySQL identifies latch contention within the lock manager as the bottleneck responsible for this collapse. We design a lock manager with reduced latching, implement it in MySQL, and show that it avoids the collapse and generally improves performance. Our efficient implementation of a lock manager is enabled by a staged allocation and de-allocation of locks. Locks are pre-allocated in bulk, so that the lock manager only has to perform simple list-manipulation operations during the acquire and release phases of a transaction. De-allocation of the lock data structures is also performed in bulk, which enables the use of fast implementations of lock acquisition and release, as well as concurrent deadlock checking.","Hyungsoo Jung*, NICTA; Hyuck Han, Samsung; Alan Fekete, University of Sdyney; Gernot Heiser, NICTA and UNSW; Heon Yeom, Seoul National University",hyungsoo.jung@gmail.com; hyuck.han@samsung.com; alan.fekete@sydney.edu.au; gernot@nicta.com.au; yeom@snu.ac.kr,"Systems, Performance, Transaction Processing*",,,657,Controlled lock violation,"In databases with a large buffer pool, a short transaction may complete in much less time than it takes to log the transaction’s commit record on stable storage. Such cases motivate a technique called early lock release: immediately after appending its commit record to the log buffer in memory, a transaction may release its locks. Thus, it cuts overall lock duration to a fraction and reduces lock contention accordingly. Early lock release also has its problems. The initial mention of early lock release was incomplete, the first detailed description and implementation was incorrect with respect to read-only transactions, and the most recent design initially had errors and still does not cover unusual lock modes such as ‘increment’ locks. Thus, we set out to achieve the same goals as early lock release but with a different, simpler, and more robust approach. The resulting technique, controlled lock violation, applies to any lock mode, requires no new theory, promises less implementation effort and slightly less run-time effort, and also optimizes distributed transactions, e.g., in systems that rely on multiple replicas for high availability and high reliability. In essence, controlled lock violation retains locks until the transaction is durable but permits other transactions to violate its locks while flushing its commit log record to stable storage.","Goetz Graefe*, Hewlett-Packard Laboratories; Harumi Kuno, Hewlett-Packard Laboratories; Mark Lillibridge, Hewlett-Packard Laboratories; Joseph Tucek, Hewlett-Packard Laboratories; Alistair Veitch, Hewlett-Packard Laboratories",goetz.graefe@hp.com; harumi.kuno@hp.com; mark.lillibridge@hp.com; joseph.tucek@hp.com; alistair.veitch@hp.com,"Systems, Performance, Transaction Processing*",,,659,X-FTL: Transactional FTL for SQLite Databases,"In this era of smartphones and mobile computing, many popular applications such as Facebook, twitter, Gmail, and even Angry birds game manage their data using SQLite. This is mainly due to the development productivity and solid transactional support provided by the SQL interface. For transactional atomicity, however, SQLite relies on less sophisticated but costlier page-oriented journaling mechanisms. Hence, this is often cited as the main cause of tardy responses in mobile applications. Flash memory does not allow data to be updated in place, and the copy-on-write strategy is adopted by most flash storage devices. In this paper, we propose X-FTL, a transactional flash translation layer (FTL) for SQLite databases. By offloading the burden of guaranteeing the transactional atomicity from a host system to flash storage and by taking advantage of the copy-on-write update strategy used in modern FTLs, X-FTL drastically improves the transactional throughput almost for free without resorting to costly journaling schemes. X-FTL attempts to turn the weakness of flash memory (i.e., being unable to update in place) into a strong point (i.e., inherently atomic propagation of multiple updated pages). We have implemented X-FTL on an SSD development board called OpenSSD, which is characteristically identical to a commercial SSD product, and also modified SQLite and ext4 file system to make them compatible with the extended abstractions provided by X-FTL. We demonstrate the effectiveness of X-FTL using several real SQLite workloads obtained by running smartphone applications, synthetic workloads (such as RL benchmark for smartphone applications), TPC-C benchmark for OLTP databases, and FIO benchmark for file systems.","Woon-Hak Kang, Sungkyunkwan University; Sang-Won Lee, Sungkyunkwan University; Bongki Moon*, Seoul National University; GIHWAN OH, SungKyunKwan University; Changwoo Min, Sungkyunkwan University",woonagi319@skku.edu; swlee@skku.edu; bkmoon@cs.arizona.edu; wurikiji@skku.edu; multics69@skku.edu,"Systems, Performance, Transaction Processing*",Tuesday Afternoon I,,,,,,,150,4,Data Storage,631,Optimal Splitters for Temporal and Multi-version Databases,"Temporal and multi-version databases often generate massive amounts of data, due to the increasing availability of large storage space and the increasing importance of mining and auditing operations from historical data. For example, Google now allows users to limit and rank search results by setting a time range. These databases are ideal candidates for a distributed store, that offers large storage space and parallel and distributed processing power from a cluster of (commodity) machines. A key challenge is to achieve a good load balancing algorithm for storage and processing of these data, which is done by partitioning the database. In this paper we introduce the concept of optimal splitters for temporal and multiversion databases, which induce a partition of the input database, and guarantee that the size of the maximum bucket is minimized among all possible configurations, given a budget for the desired number of buckets. We design efficient methods for memory- and disk resident data respectively, and show that they significantly outperform competing baseline methods both theoretically and empirically in large real data sets.","Wangchao Le, University of Utah; Feifei Li*, University of Utah; Yufei Tao, CUHK; Robert Christensen, University of Utah",lew@cs.utah.edu; lifeifei@cs.utah.edu; taoyf@cse.cuhk.edu.hk; robertc@eng.utah.edu,"Spatial, Temporal, Multimedia and Scientific Databases*",,,640,Building an efficient RDF store over a relational database,"Efficient storage and querying of RDF data is of increasing importance, due to the increased popularity and widespread acceptance of RDF on the web and in the enterprise. In this paper, we describe a novel storage and query mechanism for RDF which works on top of existing relational representations. Reliance on relational representations of RDF means that one can take advantage of 35+ years of research on efficient storage and querying, industrial-strength transaction support, locking, security, etc. However, there are significant challenges in storing RDF in relational, which include data sparsity and schema variability. We describe novel mechanisms to shred RDF into relational, and novel query translation techniques to maximize the advantages of this shredded representation. We show that these mechanisms result in consistently good performance across multiple RDF benchmarks, even when compared with current state-of-the-art stores.","Anastasios Kementsietsidis*, IBM Research; Mihaela Bornea, ; Julian Dolby, ; Kavitha Srinivas, ; Patrick Dantressangle, ; Octavian Udrea, ; Bishwaranjan Bhattacharjee, IBM T.J. Watson",akement@us.ibm.com; mabornea@us.ibm.com; dolby@us.ibm.com; ksrinivs@us.ibm.com; dantress@uk.ibm.com; oudrea@us.ibm.com; bhatta@us.ibm.com,"Storage, Indexing and Physical Database Design*",,,672,Automatic Synthesis of Out-of-Core Algorithms,"We present a framework and system for the automatic syn- thesis of efficient algorithms specialized for a particular memory hierarchy and a set of storage devices. The devel- oper provides two high level, orthogonal specifications: 1) an abstract algorithm that ignores memory hierarchy and external storage aspects; and 2) a description of the mem- ory hierarchy itself, including its topology and data transfer parameters. Our system is able to automatically synthesize memory-hierarchy and storage device-aware algorithms out of those specifications, for tasks such as joins and sorting. The framework is extensible and allows to quickly synthesize customized out-of-core algorithms for data management as new storage technologies become available.","Andrej Spielmann*, EPFL; Andres Notzli, ; Christoph Koch, EPFL; Viktor Kuncak, EPFL; Yannis Klonatos, EPFL",andrej.spielmann@epfl.ch; andres.notzli@epfl.ch; christoph.koch@epfl.ch; viktor.kuncak@epfl.ch; yannis.klonatos@epfl.ch,"Storage, Indexing and Physical Database Design*"50,5,Schema Matching and Spatial Databases I,271,Semantic Matching and Annotation of Numeric and Time-Varying Attributes in Web Tables,"Users often need to gather information about ``entities'' of interest. Recent efforts try to automate this task by leveraging the vast corpus of HTML tables; this is referred to as ``entity augmentation''. The accuracy of entity augmentation critically depends on semantic relationships between web tables as well as semantic labels of those tables. Current techniques work well for string-valued and static attributes but perform poorly for numeric and time-varying attributes. In this paper, we first build a semantic graph that (i) labels columns with unit, scale and timestamp information and (ii) computes semantic matches between columns even when the same numeric attribute is expressed in different units or scales. Second, we develop a novel entity augmentation API suited for numeric and time-varying attributes that leverages the semantic graph. Building the graph is challenging as such label information is often missing from the column headers. Our key insight is to leverage the wealth of tables on the web and infer label information from semantically matching columns of other web tables; this complements ``local'' extraction from column headers. However, this creates an interdependence between labels and semantic matches; we address this challenge by representing the task as a probabilistic graphical model that jointly discover labels and semantic matches over all columns. Our experiments on real-life datasets show that (i) our semantic graph contains higher quality labels and semantic matches and (ii) entity augmentation based on the above graph has significantly higher precision and recall compared with the state-of-the-art.","Kaushik Chakrabarti, Microsoft Research; Meihui Zhang*, NUS",kaushik@microsoft.com; mhzhang@comp.nus.edu.sg,"Database Models, Uncertainty, Schema Matching, Data Integration*",,,632,Value Invention in Data Exchange,"Skolem functions are an important tool in information integration as they permit a precise representation of missing information. The powerful mapping language of second-order tuple generating dependencies (SO tgds) permits arbitrary Skolem functions and has been proven to be the right choice for modeling many data exchange and integration problems, such as composition and correlation of mappings. This language is strictly more powerful than the languages used in many data exchange systems, including source-to-target and nested tgds which are both first-order (FO) mapping languages (commonly known as GLAV and nested GLAV mappings). These FO mapping languages are known to have more desirable programmatic and computational properties. In this paper, we present two techniques for translating some SO tgds into equivalent, more manageable FO mappings. Our results rely on understanding conditions under which we can guarantee that the Skolem functions in a mapping are well-behaved. Our first technique, linearization, significantly extends the Nash, Bernstein and Melnik unskolemization algorithm for SO tgds, by applying an SO quantifier elimination method. We then present the first algorithm for using source semantics, specifically functional dependencies (including but not limited to keys), to create FO mappings that are equivalent to an SO mapping in the presence of the source constraints. We show that these algorithms are applicable to a strictly larger class of SO tgds than previous approaches, but more importantly we present an extensive experimental evaluation that quantifies this difference (about 78 % improvement) and the applicability of our results on real mappings. The evaluation shows that in practice our techniques generate well-behaved (FO) mappings in a very large number of scenarios.","Patricia Arocena*, University of Toronto; Boris Glavic, Illinois Institute of Technology; Renee Miller, University of Toronto",prg@cs.toronto.edu; bglavic@iit.edu; miller@cs.toronto.edu,"Database Models, Uncertainty, Schema Matching, Data Integration*",,,670,Indexing Methods for Moving Object Databases: Games and Other Applications,"Game applications require keeping track of objects that move and thus the database of objects must be constantly updated. The cover fieldtree (also known as the loose quadtree or octree) is designed to overcome the drawback of spatial data structures that associate objects with their minimum enclosing quadtree (octree) cells which is that the size of these cells depends more on the position of the objects and less on their size. In fact, the size of these cells may be as large as the entire space from which the objects are drawn. The loose quadtree (octree) achieves this by expanding the size of the space that is spanned by each quadtree (octree) cell c of width w by a cell expansion factor p (p > 0) so that the expanded cell is of width (1+p)·w and an object is associated with its minimum enclosing expanded quadtree (octree) cell. The novelty of our work lies in our demonstration that the loose quadtree, the maximum possible width w of c given an object o with minimum bounding hypercube box b of radius r is determined as just a function of r and p, and is independent of the position of o. More importantly, we explore the range of possible ratios of the width of the cell and the width of the minimum bounding hypercube box w/2r as a function of p, and prove that for p≥0.5, w/2r takes on at most two values. This makes updating very simple and fast as for p≥0.5, there are at most two possible new cells associated with the moved object. Since such quadtrees are usually implemented with a pointerless representation using a B-tree, motion updates reduce to B-tree updates in contrast to more complicated, and significantly slower, updates as is the case with representations such as an R-tree. Experiments show that the update time to support motion is minimized when p is infinitesimally < 1 with as much as a one order of magnitude increase in the number of updates that can be handled vs the p=0 case in a given unit of time. Similar results hold for an N-body simulation.","Hanan Samet*, University of Maryland; Jagan Sankaranarayanan, NEC labs America; Michael Auerbach, University of Maryland, College Park",hjs@cs.umd.edu,"Storage, Indexing and Physical Database Design*"100,6,Graph Connectivity,163,I/O Efficient: Computing SCCs in Massive Graphs,"A strongly connected component (SCC) is a maximal subgraph of a directed graph G in which every pair of nodes are reachable from each other in the SCC. With such a property, a general directed graph can be represented by a directed acyclic graph (DAG) by contracting an SCC of G to a node in DAG. In many real applications that need graph pattern matching, topological sorting, and reachability query processing, the best way to deal with a general directed graph is to deal with its DAG representation. Therefore, finding all SCCs in a directed graph G is a critical operation. The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. the size of a graph. However, when a graph cannot resident entirely in the main memory, the existing external or semi-external algorithms to find all SCCs have limitation to achieve high I/O efficiency. In this paper, we study new I/O efficient semi-external algorithms to find all SCCs for a massive directed graph G that cannot reside in main memory entirely. To overcome the deficiency of the existing DFS based semi-external algorithm that heavily relies on a total order, we explore a weak order based on which we investigate new algorithms. We propose a new two phase algorithm, namely, tree construction and tree search. In the tree construction phase, a spanning tree of G can be constructed in bounded sequential scans of G. In the tree search phase, it needs to sequentially scan the graph once to find all SCCs. We also propose a new single phase algorithm, which combines the tree construction and tree search phases into a single phase, with three new optimization techniques. They are early acceptance, early rejection, and batch processing. By the single phase algorithm with optimizations, we can significantly reduce the number of I/Os and CPU cost. We conduct extensive experimental studies using 4 real datasets and several synthetic datasets to confirm the I/O efficiency of our approaches.","Zhiwei Zhang*, CUHK; Jeffrey Yu, CUHK; Lu Qin, CUHK; Lijun Chang, CUHK; Xuemin Lin, UNSW",zwzhang@se.cuhk.edu.hk; yu@se.cuhk.edu.hk; lqin@se.cuhk.edu.hk; ljchang@se.cuhk.edu.hk; lxue@cse.unsw.edu.au,"Graph Management, Social Networks*",,,551,TF-Label: a Topological-Folding Labeling Scheme for Reachability Querying in a Large Graph,"Reachability querying is a basic graph operation with numerous important applications in databases, network analysis, computational biology, software engineering, etc. Although many indexes have been proposed to answer reachability queries, most of them are only efficient for handling relatively small graphs. We propose TF-label, an efficient and scalable labeling scheme for processing reachability queries. TF-label is constructed based on a novel topological folding (TF) that recursively folds an input graph into half so as to reduce the label size, thus improving query efficiency. We show that TF-label is efficient to construct and propose efficient algorithms and optimization schemes. Our experiments verify that TF-label is significantly more scalable and efficient than the state-of-the-art methods in both index construction and query processing.","James Cheng*, CUHK; Silu Huang, CUHK; Huanhuan Wu, Chinese University of HK; Ada Fu, Chinease University of Hong Kong",j.cheng@acm.org; slhuang@cse.cuhk.edu.hk; hhwu@cse.cuhk.edu.hk; adafu@cse.cuhk.edu.hk,"Graph Management, Social Networks*",,,654,Efficiently Computing k-Edge Connected Components via Graph Decomposition,"Efficiently computing k-edge connected components in a large graph, G = (V, E), where V is the vertex set and E is the edge set, is a long standing research problem. It is not only fundamental in graph analysis but also crucial in graph search optimization algorithms. Consider existing techniques for computing k-edge connected components are quite time consuming and are unlikely to be scalable for large scale graphs, in this paper we firstly propose a novel graph decomposition paradigm to iteratively decompose a graph G for computing its k-edge connected components such that the number of drilling-down iterations h is bounded by the “depth” of the k-edge connected components nested together to form G, where h usually is a small integer in practice. Secondly, we devise a novel, efficient threshold-based graph decomposition algorithm, with time complexity O(l _ |E|), to decompose a graph G at each iteration, where l usually is a small integer with l _ |V |. As a result, our algorithm for computing k-edge connected components significantly improves the time complexity of an existing state-of-the-art technique from O(|V||V|(|E|+|V| log |V|)) to O(h_l_|E|). Finally, we conduct extensive performance studies on large real and synthetic graphs. The performance studies demonstrate that our techniques significantly outperform the state-of-the-art solution by several orders of magnitude.","Lijun Chang*, CUHK; Jeffrey Yu, CUHK; Lu Qin, CUHK; Xuemin Lin, UNSW; Chengfei Liu, ; Weifa Liang,",ljchang@se.cuhk.edu.hk; yu@se.cuhk.edu.hk; lqin@se.cuhk.edu.hk; lxue@cse.unsw.edu.au; cliu@swin.edu.au; wliang@cs.anu.edu.au,"Graph Management, Social Networks*",Tuesday Afternoon II,,,,,,,100,7,Crowdsourcing,626,An Online Cost Sensitive Decision-Making Method in Crowdsourcing Systems,"Crowdsourcing has created a variety of opportunities for solving many challenging problems by leveraging human intelligence. For example, applications such as image tagging, natural language processing, and semantic-based information retrieval can exploit crowd-based human computation to supplement existing computational algorithms. Naturally, human workers in crowdsourcing solve problems based on their knowledge, experience, and perception. It is therefore not clear which problems can be better solved by crowdsourcing that yield better results than solving solely using traditional machine-based methods. In this paper, we design and implement a cost sensitive method of crowdsourcing. We online estimate the profit of crowdsourcing so that we know when individual questions should be terminated. We propose two models to estimate the profit of the crowdsourcing system, a linear profit model and a non-linear model. Using the models, the expected profit of taking on new answers for a specific question are computed based on the answers we have already received. The question is terminated in real time if the marginal expected profit of obtaining one more answer is not positive. We extends the method to publish a batch of questions in a HIT. We evaluate the effectiveness of our proposed method using Twitter sentiment analysis jobs on AMT. The experimental results show that our proposed method outperforms all the state-of-art methods.","Jinyang Gao, National Univ. of Singapore; Xuan Liu, National Univ. of Singapore; Beng Chin Ooi*, ; Haixun Wang, Microsoft",gaojinyang@comp.nus.edu.sg; liuxuan@comp.nus.edu.sg; ooibc@comp.nus.edu.sg; Haixun.Wang@microsoft.com,"Aggregation, Data Warehouses, OLAP, Analytics*",,,401,Leveraging Transitive Relations for Crowdsourced Joins,"The development of crowdsourced query processing system has recently attracted a significant attention in the database community. A variety of crowdsourced queries have been investigated. In this paper, we focus on the crowdsourced join query which aims to utilize humans to find all pairs of matching objects between two collections. As a human-only solution to the query is wasteful, we adopt a hybrid human-machine approach which first uses machines to generate a candidate set of matching pairs, and then asks humans to label the pairs in the candidate set as either matching or non-matching. Given the candidate pairs, existing works will publish all of them to a crowdsourcing platform. However, they neglect the fact that transitive relations hold among the pairs. As an example, if o_1 matches with o_2, and o_2 matches with o_3, then we can deduce that o_1 matches with o_3 without needing to crowdsource (o_1, o_3). To this end, we study how to leverage transitive relations for crowdsourced joins. We propose a hybrid transitive-relations and crowdsourcing labeling framework which aims to crowdsource the minimum number of pairs to label all the given pairs. We prove the optimal labeling order and devise a parallel labeling algorithm for that order. We evaluate our approaches in both simulated environment and a real crowdsourcing platform. The experimental results show that our approaches with transitive relations can save much more money and time than existing methods, with a little loss in the result quality.","Jiannan Wang*, Tsinghua University; Guoliang Li, Tsinghua University; Tim Kraska, Brown University; Michael Franklin, UC Berkeley; Jianhua Feng, Tsinghua University",wjn08@mails.tsinghua.edu.cn; liguoliang@tsinghua.edu.cn; kraskat@cs.brown.edu; franklin@cs.berkeley.edu; fengjh@tsinghua.edu.cn,None of the above*,,,647,Crowd Mining,"Harnessing a crowd of Web users for data collection has recently become a wide-spread phenomenon. A key challenge is that the human knowledge forms an open world and it is thus difficult to know what kind of information we should be looking for. Classic databases have addressed this problem by data mining techniques that identify interesting data patterns. These techniques, however, are not suitable for the crowd. This is mainly due to properties of the human memory, such as the tendency to remember simple trends and summaries rather than exact details. Following these observations, we develop here for the first time the foundations of crowd mining. We first define the formal settings. Based on these, we design a framework of generic components, used for choosing the best questions to ask the crowd and mining significant patterns from the answers. We suggest general implementations for these components, and test the resulting algorithm's performance on benchmarks that we designed for this purpose. Our algorithm consistently outperforms alternative baseline algorithms.","Yael Amsterdamer*, Tel Aviv University; Yael Grossman, Tel Aviv University; Tova Milo, Tel Aviv University; Pierre Senellart, Télécom ParisTech & HKU",yaelamst@post.tau.ac.il; yaelgros@post.tau.ac.il; milo@cs.tau.ac.il; pierre@senellart.com,"Knowledge Discovery, Clustering, Data Mining*"150,8,Social Media,644,Efficient Sentiment Correlation for Large-scale Demographics,"Aggregating sentiments for ad-hoc user groups is becoming necessary on the Social Web, where millions of users provide opinions on a wide variety of content. While several approaches exist for mining sentiments from product reviews or micro-blogs, little attention has been devoted to aggregating and comparing extracted sentiments for different demographic groups over time, such as “Students in Italy"" or “Teenagers in Europe."" This problem demands efficient and scalable methods for sentiment aggregation and correlation, which account for the evolution of sentiment values, sentiment bias, and other factors associated with the special characteristics of web data. We propose a scalable approach for sentiment indexing and aggregation that automatically detects the right time granularity for computing meaningful sentiment correlations among various demographic groups. Furthermore, we describe methods for compressing the top-k correlations, leading to improved performance without significantly affecting the quality of the results. In addition, the data structures we use are incrementally updateable, making our approach suitable online. We present an extensive experimental evaluation with both synthetic and real datasets. Our experiments show the efficiency of our sentiment aggregation and demonstrate the effectiveness of the proposed algorithms.","Mikalai Tsytsarau*, University of Trento; Sihem Amer-Yahia, CNRS LIG ; Themis Palpanas, University of Trento",tsytsarau@disi.unitn.it; Sihem.Amer-Yahia@imag.fr; themis@disi.unitn.eu,"Aggregation, Data Warehouses, OLAP, Analytics*",,,619,EBM - An Entropy-Based Model to Infer Social Strength from Spatiotemporal Data,"The ubiquity of mobile devices and the popularity of location-based-services have generated, for the first time, rich datasets of people's location information at a very high fidelity. These location datasets can be used to study people's behavior - for example, social studies have shown that people, who are seen together frequently at the same place and at the same time, are most probably socially related. In this paper, we are interested in inferring these social connections by analyzing people's location information, which is useful in a variety of application domains from sales and marketing to intelligence analysis. In particular, we propose an entropy-based model (EBM) that not only infers social connections but also estimates the strength and type of social connections by analyzing people's co-occurrences in space and time. We examine two independent ways: diversity and weighted frequency, through which co-occurrences contribute to social strength. In addition, we take the characteristics of each location into consideration in order to compensate for cases where limited location information is available. We conducted extensive sets of experiments with real-world datasets including both people's location data and their social connections, where we used the latter as the ground-truth to verify the results of applying our approach to the former. We show that our approach outperforms the competitors.","Huy Pham*, USC; Cyrus Shahabi, USC; Yan Liu,",huy.pham.sc@gmail.com; shahabi@usc.edu; yanliu.cs@usc.edu,"Spatial, Temporal, Multimedia and Scientific Databases*",,,663,Online Search of Overlapping Communities,"A great deal of research has been conducted on modeling and discovering communities in complex networks. In most real life networks, including social networks and bio-chemical networks, an object often participates in multiple overlapping communities. In view of this, recent research has focused on mining overlapping community structures in complex networks. The algorithms essentially materialize a snapshot of the overlapping communities in the network. This approach has three drawbacks, however. First, the mining algorithm uses the same global criterion to decide whether a subgraph qualifies as a community. In other words, the criterion is fixed and predetermined. But in reality, communities for different vertices may have very different characteristics. Second, it is costly, time consuming, and often unnecessary to find communities for an entire network. Third, the approach does not support dynamically evolving networks. In this paper, we focus on online search of overlapping communities, that is, given a query vertex, we find meaningful overlapping communities the vertex belongs to in an online manner. In doing so, each search can use community criterion tailored for the vertex in the search. To support this approach, we introduce a novel model for overlapping communities, and we provide theoretical guidelines for tuning the model. We present several algorithms for online overlapping community search and we conduct comprehensive experiments to demonstrate the effectiveness of the model and the algorithms. We also suggest many potential applications of our model and algorithms.","wanyun cui, ; Yanghua Xiao*, Fudan University; Haixun Wang, Microsoft Research Asia; yiqi Lu, ; wei Wang, Fudan University",zhenjiong@gmail.com; shawyh@fudan.edu.cn; haixunw@microsoft.com; yiqilu@gmail.com; weiwang1@fudan.edu.cn,"Graph Management, Social Networks*"200,9,"Systems, Performance I",651,BitWeaving: Fast Scans for Main Memory Data Processing,"A key design goal in main memory database is to run at the ""bare metal"" speed. Essentially, this means that the system must aim to process data at the speed of the processor (the fastest component in most system configurations). Scans are common in main memory databases, and a lot of attention has been paid to running scans fast, including using column stores and using SIMD-based parallelism. With the state-of-the-art techniques, it still takes many cycles per input tuple to apply simple predicates on a single column of a main memory table. In this paper, we propose a novel technique called BitWeaving that exploits the parallelism available at the bit level in modern processors, where by in a single cycle multiple bits of data can be operated on in a single cycle. We propose two flavors of BitWeaving. The first, BitWeaving/V, uses a combination of columnar data representation, but at the bit level, with horizontal bit packing to achieve scans that are always faster than the state-of-the-art, and often by more than 6X. We recommend using BitWeaving/V for attributes on which scan predicates are common. For other attributes, such as those that are used more frequently in projection lists, we recommend a dual bit organization approach called BitWeaving/H. Collectively, the methods that we propose in this paper present new and high-performance storage methods for main memory databases.","Yinan Li*, Univ. of Wisconsin-Madison; Jignesh Patel, University of Wisconsin-Madison",yinan@cs.wisc.edu; jignesh@cs.wisc.edu,"Aggregation, Data Warehouses, OLAP, Analytics*",,,568,Performance and Resource Modeling in Highly-Concurrent OLTP Workloads,"Online Transaction Processing (OLTP) is characterized by many short-lived transactions, running concurrently. Resource and performance analysis and prediction---answering questions like ``How much disk I/O will my system perform if the requests per second double?'' or ``What happens if the ratio of transactions in my system changes, e.g., as a result of a new web page coming online?'' or `What is the maximum throughput I can sustain with my current hardware?''--- are both important and difficult in this setting. The main challenge is due to the high degree of concurrency and complex interactions between transactions for resources, leading to non-linear performance impact. Nonetheless, such analysis is a key component in enabling database administrators and developers to understand how their systems will scale under load and which queries are using certain resources. In this paper, we address this problem by developing statistical models that provide resource and performance analysis and prediction for highly concurrent OLTP workloads. Our models are built on a small amount of training data from standard log information (i.e., query log, and OS resource monitoring) collected during normal system operations. These models are capable of providing accurate estimation of several performance metrics, including: resource consumption broken down by transaction type, resource bottlenecks, and throughput estimates at different load levels. We have validated these models on MySQL/Linux with numerous experiments on standard benchmarks (TPC-C) and real workloads (Wikipedia), observing very high accuracies (within a few percent error) when predicting all of the above metrics.","Barzan Mozafari*, MIT; Carlo Curino , Yahoo! Research; Alekh Jindal, CSAIL, MIT; Sam Madden, MIT",barzan@csail.mit.edu; carlo.curino@gmail.com; alekh@csail.mit.edu; madden@csail.mit.edu,"Systems, Performance, Transaction Processing*",,,641,An Approach to Building a Massively-Parallel Search Engine Using a DB-IR Tightly-Integrated Parallel DBMS for Higher-Level Functionality,"Recently, parallel search engines have been implemented based on scalable distributed file systems such as Google File System. However, we claim that building a massively-parallel search engine using a parallel DBMS can be an attractive alternative since it supports a higher-level (i.e., SQL-level) interface than that of a distributed file system for easy and less error-prone application development while providing scalability. Regarding higher-level functionality, we can draw a parallel with the traditional O/S file system vs. DBMS. In this paper, we propose a new approach of building a massively-parallel search engine using a DB-IR tightly-integrated parallel DBMS. To estimate the performance, we propose a hybrid (i.e., analytic and experimental) performance model for the parallel search engine. We argue that the model can accurately estimate the performance of a massively-parallel (e.g., 300-node) search engine using the experimental results obtained from a small-scale (e.g., 5-node) one. We show that the estimation error between the model and the actual experiment is less than 3.85% by observing that the bulk of the query processing time is spent at the slave (vs. at the master and network) and by estimating the time spent at the slave based on actual measurement. Using our model, we demonstrate a commercial-level scalability and performance of our architecture. Our proposed system ODYS is capable of handling 1 billion queries per day (81 queries/sec) for 30 billion Web pages by using only 43,472 nodes with an average query response time of 195 ms. By using twice as many (86,944) nodes, ODYS can provide an average query response time of 148 ms. These results show that building a massively-parallel search engine using a parallel DBMS is a viable approach with advantages of supporting the high-level (i.e., DBMS-level), SQL-like programming interface.","Kyu-Young Whang, KAIST; Tae-Seob Yun*, KAIST; Yeon-Mi Yeo, KAIST; Il-Yeol Song, Drexel University; Hyuk-Yoon Kwon, KAIST; In-Joong Kim, KAIST",kywhang@cs.kaist.ac.kr; tsyun@mozart.kaist.ac.kr; ymyeo@mozart.kaist.ac.kr; songiy@drexel.edu; hykwon@mozart.kaist.ac.kr; ijkim@mozart.kaist.ac.kr,"Systems, Performance, Transaction Processing*",Wednesday Morning,,,,,,,100,10,Graph Management,228,Massive Graph Triangulation (best paper),"This paper studies I/O-efficient algorithms for settling the classic {\em triangle listing} problem, whose solution is a basic operator in dealing with many other graph problems. Specifically, given an undirected graph $G$, the objective of triangle listing is to find all the cliques involving 3 vertices in $G$. The problem has been well studied in internal memory, but remains an urgent difficult challenge when $G$ does not fit in memory, rendering any algorithm to entail frequent I/O accesses. Although previous research has attempted to tackle the challenge, the state-of-the-art solutions rely on a set of crippling assumptions to guarantee good performance. Motivated by this, we develop a new algorithm that is provably I/O and CPU efficient at the same time, without making any assumption on the input $G$ at all. The algorithm uses ideas drastically different from all the previous approaches, and outperformed the existing competitors by a factor over an order of magnitude in our extensive experimentation.","Xiaocheng Hu, CUHK; Yufei Tao*, CUHK; Chin-Wan Chung, KAIST",xchu@cse.cuhk.edu.hk; taoyf@cse.cuhk.edu.hk; chungcw@kaist.edu,"Graph Management, Social Networks*",,,618,Turbo_{ISO}: Towards Ultrafast and Robust Subgraph Isomorphism Search in Large Graph Databases,"Given a query graph $q$ and a data graph $g$, the subgraph isomorphism search finds all occurrences of $q$ in $g$ and is considered one of the most fundamental query types for many real applications. While this problem belongs to NP-hard, many algorithms have been proposed to solve it in a reasonable time for real datasets. However, a recent study has shown through an extensive benchmark with various real datasets that all existing algorithms have serious problems in their matching order selection. Furthermore, all algorithms blindly permutate all possible mappings for query vertices, often leading to useless computations. In this paper, we present an efficient and robust subgraph search solution called {\gTurbo}, which is turbo-charged with two novel concepts, \emph{candidate region exploration} and the \emph{combine and permute} strategy (in short, COMB/PERM). The candidate region exploration on-the-fly identifies candidate subgraphs (i.e, candidate regions), which contain embeddings, and computes a robust matching order for each candidate region explored. The {\combperm} strategy exploits the novel concept of the \emph{neighborhood equivalence class} (NEC). Each query vertex in the same NEC has the same matching data vertices. During subgraph isomorphism search, COMB/PERM generates only combinations for each NEC instead of permutating all possible enumerations. Thus, if a chosen combination is determined to not contribute to a complete solution, all possible permutations for the combination will be safely pruned. Extensive experiments with many real datasets show that {\gTurbo} consistently and significantly outperforms all competitors by up to several orders of magnitude.","Wook-Shin Han*, Kyungpook National University; Jinsoo Lee, Kyungpook National University; Jeong Hoon Lee, Kyungpook National University",wshan@knu.ac.kr; jslee2@www-db.knu.ac.kr; jhlee@www-db.knu.ac.kr,"Graph Management, Social Networks*",,,638,Fast Exact Shortest-Path Distance Queries on Large Networks by Pruned Landmark Labeling,"We propose a new exact method for shortest-path distance queries on large-scale networks. Our method precomputes distance labels for vertices by performing a breadth-first search from every vertex. Seemingly too obvious and too inefficient at first glance, the key ingredient introduced here is pruning during breadth-first searches. While we can still answer the correct distance for any pair of vertices from the labels, it surprisingly reduces the search space and sizes of labels. Moreover, we show that we can perform 32 or 64 breadth-first searches simultaneously exploiting bitwise operations. We experimentally demonstrate that, the combination of these two techniques is efficient and robust on various kinds of large-scale real-world networks. In particular, our method can handle social networks and web graphs with hundreds of millions of edges, which are two orders of magnitude larger than the limits of previous exact methods, with comparable query time to those of previous methods.","Takuya Akiba*, University of Tokyo; Yoichi Iwata, The University of Tokyo; Yuichi Yoshida, National Institute of Informatics & Preferred Infrastructure, Inc.",t.akiba@is.s.u-tokyo.ac.jp; y.iwata@is.s.u-tokyo.ac.jp; yyoshida@nii.ac.jp,"Graph Management, Social Networks*"50,11,Text Databases,581,Improving Regular-Expression Matching on Strings Using Negative Factors,"The problem of finding matches of a regular expression (RE) on a string exists in many applications such as text editing, biosequence search, and shell commands. Existing techniques first identify candidates using substrings in the RE, then verify each of them using an automaton. These techniques become inefficient when there are many candidate occurrences that need to be verified. In this paper we propose a novel technique that prunes false negatives by utilizing {\em negative factors}, which are substrings that {\em cannot} appear in an answer. A main advantage of the technique is that it can be integrated with many existing algorithms to improve their efficiency significantly. We give a full specification of this technique. We develop an efficient algorithm that utilizes negative factors to prune candidates, then improve it by using bit operations to process negative factors in parallel. We show that negative factors, when used together with necessary factors (substrings that must appear in each answer), can achieve much better pruning power. We analyze the infinite space of negative factors, and develop an algorithm for finding a finite number of high-quality negative factors. We conducted a thorough experimental study of this technique on real data sets, including DNA sequencess, proteins, and text documents, and show the significant performance improvement when applying the technique in existing algorithms. For instance, it improved the search speed of the popular Gnu Grep tool by 3 to 63 times.","Xiaochun Yang*, Northeastern University; Bin Wang, Northeastern University; Tao Qiu, NEU; Yaoshu Wang, NEU; Chen Li, UCI",yangxc@mail.neu.edu.cn; binwang@mail.neu.edu.cn; yang.x.chun@gmail.com; wangbin@ise.neu.edu.cn; chenli@ics.uci.edu,"Text Databases, XML, Keyword Search*",,,636,String Similarity Measures and Joins with Synonyms,"A string similarity measure is to measure similarity between two text strings for approximate string matching or comparison. For example the strings ""Sam"" and ""Samuel"" can be considered to be similar. Most existing works that compute the similarity of two strings only consider syntactic similarities, e.g., number of common words or \qgrams. While this is indeed an indicator of similarity, there are many important cases where syntactically different strings can represent the same real-world object. For example, ``Bill'' is a short form of ``William''. Given a collection of predefined synonyms, the purpose of this paper is to explore such existing knowledge to support efficient string similarity measures and joins, thereby boosting the string-matching quality. In particular, we first present an expansion-based framework to perform efficient similarity measures. Because using synonyms in similarity measures is, while expressive, computationally expensive (NP-hard), we propose an efficient greedy algorithm, called {\it selective-expansion} to guarantee the optimality in most real scenarios. We then study a novel index structure called SI-tree for the purpose of string similarity join with synonyms, combining signature filtering and length filtering strategies together. We develop an approximate estimator to compute the size of candidates to enable an online selection of signatures in signature filters to improve the efficiency. This estimator provides strong low-error, high-confidence guarantees while requiring only logarithmic space and time costs, thus making our method become interesting in theory and in practice. Finally, the results from an empirical study of the proposed measures and join algorithms verify the effectiveness and efficiency of our approach.","Jiaheng Lu*, Renmin University of China ; Chunbin Lin, Renmin University of China; Wei Wang, UNSW; Chen Li, UCI; Haiyong Wang, RUC",jiahenglu@gmail.com; chunbinlin@ruc.edu.cn; weiw@cse.unsw.edu.au; chenli@ics.uci.edu; whyruc@163.com,"Text Databases, XML, Keyword Search*",,,655,Efficient Top-k Algorithms for Approximate Substring Matching,"There is a wide range of applications that require to query a large database of texts to search for similar strings or substrings. Traditional approximate substring matching requests a user to specify a similarity threshold. Without top-k approximate substring matching, users have to try repeatedly different maximum distance threshold values when the proper threshold is unknown in advance. In our paper, we first propose the efficient algorithms for finding the top-k approximate substring matches with a given query string in a set of data strings. To reduce the number of expensive distance computations, the proposed algorithms utilize our novel filtering techniques which take advantages of q-grams and inverted q-gram indexes available. We conduct extensive experiments with real-life data sets. Our experimental results confirm the effectiveness and scalability of our proposed algorithms.","Younghoon Kim, Seoul National University; Kyuseok Shim*, Seoul National University",yhkim@kdd.snu.ac.kr; shim@ee.snu.ac.kr,"Text Databases, XML, Keyword Search*"150,12,"Systems, Performance II",149,Towards High-Throughput Gibbs Sampling at Scale: A Study across Storage Managers,"Factor graphs and Gibbs sampling are a popular combination for Bayesian statistical methods that are used to solve diverse problems including insurance risk models, pricing models, and information extraction. Given a fixed sampling method and a fixed amount of time, an implementation of a sampler that achieves a higher throughput of samples will achieve a higher quality than a lower-throughput sampler. We study how (and whether) traditional data processing choices about materialization, page layout, and buffer-replacement policy need to be changed to achieve high-throughput Gibbs sampling for factor graphs that are larger than main memory. We find that both new theoretical and new algorithmic techniques are required to understand the tradeoff space for each choice. On both real and synthetic data, we demonstrate that traditional baseline approaches may achieve two orders of magnitude lower throughput than an optimal approach. For a handful of popular tasks across several storage backends, including HBase and traditional unix files, we show that our simple prototype achieves competitive (and sometimes better) throughput compared to specialized state-of-the-art approaches on factor graphs that are larger than main memory.","Ce Zhang*, University of Wisconsin-Madison; Feng Niu, Univ. of Wisconsin-Madison; Christopher Re, University of Wisconsin Madison",czhang@cs.wisc.edu; niufeng14@gmail.com; chrisre@cs.wisc.edu,"Systems, Performance, Transaction Processing*",,,652,"Latch-free Data Structures for DBMS - Design, Implementation, and Evaluation -","The fact that multi-core CPUs have become so common and that the number of CPU cores in one chip has continued to rise means that a server machine can easily contain an extremely high number of CPU cores. The CPU scalability of IT systems is thus attracting a considerable amount of research attention. Some systems, such as ACID-compliant DBMSs, are said to be difficult to scale, probably due to the mutual exclusion required to ensure data consistency. Possible countermeasures include latch-free (LF) data structures, an elemental technology to improve the CPU scalability by eliminating the need for mutual exclusion. This paper investigates these LF data structures with a particular focus on their applicability and effectiveness. Some existing LF data structures (such as LF hash tables) have been adapted to PostgreSQL, one of the most popular open-source DBMSs. The performance improvement was evaluated with a benchmark program simulating real-world transactions. Measurement results obtained from state-of-the-art 80-core machines demonstrated that the LF data structures were effective for performance improvement in a many-core situation in which DBT-1 throughput increased by about 2.5 times. Although the poor performance of the original DBMS was due to a severe latch-related bottleneck and can be improved by parameter tuning, it is of practical importance that LF data structures provided performance improvement without deep understanding of the target system behavior that is necessary for the parameter tuning.","Takashi Horikawa*, NEC Corporation",t-horikawa@aj.jp.nec.com,"Systems, Performance, Transaction Processing*",,,668,DBMS Metrology: Measuring Query Time,"It is surprisingly hard to obtain accurate and precise measurements of the time spent executing a query. We review relevant process and overall measures obtainable from the Linux kernel and introduce a structural causal model relating these measures. A thorough correlational analysis provides strong support for this model. Using this model, we developed a timing protocol, which (1) performs sanity checks to ensure validity of the data, (2) drops some query executions via clearly motivated predicates, (3) drops some entire queries at a cardinality, again via clearly motivated predicates, (4) for those that remain, for each computes a single measured time by a carefully justified formula over the underlying measures of the remaining query executions, and (5) performs post-analysis sanity checks. The resulting query time measurement appliesto proprietary and open-source DBMSes.","Sabah Currim, University of Arizona; Richard Snodgrass*, University of Arizona; Young-Kyoon Suh, University of Arizona; Rui Zhang, Teradata; Matthew Johnson, University of California, San Diego; Cheng Yi, University of Arizona",scurrim@email.arizona.edu; rts@cs.arizona.edu; yksuh@cs.arizona.edu; Rui.Zhang@teradata.com; mjw@email.arizona.edu; yic@cs.arizona.edu,"Systems, Performance, Transaction Processing*",Wednesday Afternoon,,,,,,,,,SIGMOD Panel,,Panel: We are Drowning in a Sea of Least Publishable Units (LPUs),,"Michael Stonebraker (Professor, MIT); David DeWitt (Technical Fellow, Microsoft); Jeff Naughton (Professor, Univ. of Wisconsin); Ihab Ilyas (Principal Scientist, QCRI and Associate Professor, Univ. of Waterloo)",,100,13,Information Extraction,648,Quality and Efficiency for Kernel Density Estimates in Large Data,"Kernel density estimates are important for a broad variety of applications including media databases, pattern recognition, computer vision, data mining, and the sciences. Their construction has been well-studied, but existing techniques are expensive on massive datasets and/or only provide heuristic approximations without theoretical guarantees. We propose randomized and deterministic algorithms with quality guarantees which are orders of magnitude more efficient than previous algorithms. Our algorithms do not require knowledge of the kernel or its bandwidth parameter and are easily parallelizable. We demonstrate how to implement our ideas in a centralized setting and in MapReduce, although our algorithms are applicable to any large-scale data processing framework. Extensive experiments on large real datasets demonstrate the quality, efficiency, and scalability of our techniques.","Yan Zheng, University of Utah; Jeffrey Jestes, University of Utah; Jeff Phillips, University of Utah; Feifei Li*, University of Utah",yanzheng@cs.utah.edu; jestes@cs.utah.edu; jeffp@cs.utah.edu; lifeifei@cs.utah.edu,"Knowledge Discovery, Clustering, Data Mining*",,,639,Efficient Ad-hoc Search for Personalized PageRank,"Personalized PageRank (PPR) has been successfully applied to various applications. In real applications, it is important to set PPR parameters in an ad-hoc manner when finding similar nodes because of dynamically changing nature of graphs. Through interactive actions, interactive similarity search supports users to enhance the efficacy of applications. Unfortunately, if the graph is large, interactive similarity search is infeasible due to its high computation cost. Previous PPR approaches cannot effectively handle interactive similarity search since they need precomputation or approximate computation of similarities. The goal of this paper is to efficiently find the top-k nodes with exact node ranking so as to effectively support interactive similarity search based on PPR. Our solution is Castanet. The key Castanet operations are (1) estimate upper/lower bounding similarities iteratively, and (2) prune unnecessary nodes dynamically to obtain top-k nodes in each iteration. Experiments show that our approach is much faster than existing approaches.","Yasuhiro Fujiwara*, NTT; Makoto Nakatsuji, NTT; Hiroaki Shiokawa, NTT; Takeshi Mishima, NTT; Makoto Onizuka, NTT",fujiwara.yasuhiro@lab.ntt.co.jp; nakatsuji.makoto@lab.ntt.co.jp; shiokawa.hiroaki@lab.ntt.co.jp; mishima.takeshi@lab.ntt.co.jp; onizuka.makoto@lab.ntt.co.jp,"Knowledge Discovery, Clustering, Data Mining*",,,510,Provenance-based Dictionary Refinement in Information Extraction,"Dictionaries of terms and phrases (e.g. common person or organization names) are integral to information extraction systems that extract structured information from unstructured text. Using noisy or unrefined dictionaries may lead to many incorrect results even when highly precise and sophisticated extraction rules are used. In general, the results of the system are dependent on dictionary entries in arbitrary complex ways, and removal of a set of entries can remove both correct and incorrect results. Further, any such refinement critically requires laborious manual labeling of the results. In this paper, we study the dictionary refinement problem and address the above challenges. Using provenance of the outputs in terms of the dictionary entries, we formalize an optimization problem of maximizing the quality of the system with respect to the refined dictionaries, study complexity of this problem, and give efficient algorithms. We also propose solutions to address incomplete labeling of the results where we estimate the missing labels assuming a statistical model. We conclude with a detailed experimental evaluation using several real-world extractors and competition datasets to validate our solutions. Beyond information extraction, our provenance-based techniques and solutions may find applications in view-maintenance in general relational settings.","Sudeepa Roy*, University of Washington; Laura Chiticariu, IBM Research, Almaden; Vitaly Feldman, IBM Research, Almaden; Frederick Reiss, IBM Research, Almaden; Huaiyu Zhu, IBM Research, Almaden",sudeepa@cs.washington.edu; chiti@us.ibm.com; vitaly@post.harvard.edu; frreiss@us.ibm.com; huaiyu@us.ibm.com,"Text Databases, XML, Keyword Search*"150,14,Query Processing and Optimization,140,CS2: A New Database Synopsis for Query Estimation,"Fast and accurate estimations for complex queries are profoundly beneficial for large databases with heavy workloads. In this research, we propose a statistical summary for a database, called CS2 (Correlated Sample Synopsis), to provide rapid and accurate result size estimations for all queries with joins and arbitrary selections. Unlike the state-of-the-art techniques, CS2 does not completely rely on simple random samples, but mainly consists of correlated sample tuples that retain join relationships with less storage. We introduce a statistical technique, called reverse sample, and design an innovative estimator, called reverse estimator, to fully utilize correlated sample tuples for query estimation. We prove both theoretically and empirically that the reverse estimator is unbiased and accurate using CS2. Extensive experiments on multiple datasets show that CS2 is fast to construct and derives more accurate estimations than existing methods with the same space budget.","Feng Yu*, SIUC; Wen-Chi Hou, SIUC",fyu@cs.siu.edu; hou@cs.siu.edu,Query Processing and Optimization*,,,371,Branch-and-Bound Algorithm for Reverse Top-k Queries,"Top-k queries return to the user only the k best objects based on the individual user preferences and comprise an essential tool for rank-aware query processing. Assuming a stored data set of user preferences, reverse top-k queries have been introduced for retrieving the users that deem a given database object as one of their top-k results. Reverse top-k queries have already attracted significant interest in research, due to numerous real-life applications such as market analysis and product placement. Currently, the most efficient algorithm for computing the reverse top-k set is RTA. RTA has two main drawbacks when processing a reverse top-k query: (i) it needs to access all stored user preferences, and (ii) it cannot avoid executing a top-k query for each user preference that belongs to the result set. To address these limitations, in this paper, we identify useful properties for processing reverse top-k queries without accessing each user's individual preferences nor executing the top-k query. We propose an intuitive branch-and-bound algorithm for processing reverse top-k queries efficiently and discuss novel optimizations to boost its performance. Our experimental evaluation demonstrates the efficiency of the proposed algorithm that outperforms RTA by a large margin.","Akrivi Vlachou*, IMIS, RC Athena; Christos Doulkeridis, UNIPI; Kjetil Nørvåg, NTNU; Yannis Kotidis, AUEB Greece",vlachou@idi.ntnu.no; cdoulk@unipi.gr; noervaag@idi.ntnu.no; kotidis@aueb.gr,Query Processing and Optimization*,,,637,On the Correct and Complete Enumeration of the Core Search Space,"Reordering more than traditional joins (e.g. outerjoins, antijoins) requires some care since not all reorderings are valid. Two approaches have been described in the literature to prevent invalid plans. We show that both approaches still produce invalid plans. We present three conflict detectors. All three of them are (1) correct, i.e., prevent invalid plans, (2) easier to understand and implement than the previous (buggy) approaches, (3) more flexible in the sense that the restriction that all predicates must reject nulls is no longer required, and (4) extensible in the sense that it is easy to add new operators. Further, the last of our three approaches is complete, i.e., it allows for the generation of all valid plans within the core search space.","Pit Fender*, University of Mannheim; Guido Moerkotte, University of Mannheim; Marius Eich, University of Mannheim",pit.fender@uni-mannheim.de; moer@uni-mannheim.de; marius.eich@uni-mannheim.de,Query Processing and Optimization*200,15,Cloud computing,537,Trinity: A Distributed Graph Engine on a Memory Cloud,"Large graph processing is data driven, and general purpose graph computation requires high degree of random data access. Despite great progress in disk technology, it still cannot provide the level of random access that is required by graph computation. On the other hand, memory-based approaches usually do not scale well due to the capacity limit of a single machine. In this paper, we introduce Trinity, a general purpose graph engine over a distributed memory cloud. Trinity supports online query processing and offline analytics on large graphs. For online query processing, it leverages its fast graph exploration capability provided by the memory-based storage infrastructure. For offline graph analytics, it leverages the parallelism provided by the underlying scale-out distributed architecture. Furthermore, Trinity leverages graph access patterns in both online and offline computation to optimize memory and communication in order to deliver the best performance. We demonstrate with Trinity we can perform low latency graph queries as well as high throughput graph analytics on web-scale, billion-node graphs using just a few commodity machines.","Bin Shao*, Microsoft Research; Haixun Wang, Microsoft Research Asia; yatao Li,",binshao@microsoft.com; haixunw@microsoft.com; v-yadli@microsoft.com,"Cloud Computing, Map Reduce, Parallel, Distributed, P2P Systems*",,,627,Characterizing Tenant Behavior for Placement and Crisis Mitigation in Multitenant DBMSs,"With the proliferation of small applications deployed in cloud platforms, multitenant database management systems (DBMS) have become an integral component for effective resource utilization. A multitenant DBMS in the cloud must continuously monitor the trade-off between efficient resource sharing among multiple application databases (tenants) and their performance. Considering the scale of hundreds to thousands of tenants in such multitenant DBMSs, manual approaches for continuous monitoring are not tenable. A self-managing controller of a multitenant DBMS faces several challenges. For instance, how to characterize a tenant given its variety of workloads, how to reduce the impact of tenant colocation, how to adapt to changes in behavior, and how to detect and mitigate a performance crisis where one or more tenants’ desired service level objective (SLO) is not achieved. We present Delphi, a self-managing system controller for a multitenant DBMS, and Pythia, a technique to learn behavior through observation and supervision using DBMS agnostic database-level performance measures. Pythia accurately learns tenant behavior even when multiple tenants share a database process, ensures performance SLOs, learns good and bad tenant consolidation plans (or packings), and maintains a per-tenant history to detect behavior changes. Pythia uses supervised learning techniques that require minimal human supervision only during initial training. Delphi detects performance crises, and leverages Pythia to suggests remedial actions using a hill-climbing search algorithm to identify a new tenant packing to mitigate violating SLOs. Our evaluation using a variety of tenant types and workloads shows that Pythia can learn a tenant’s behavior with more than 92% accuracy and learn the quality of packings with more than 86% accuracy. During a performance crisis, Pythia is able to reduce 99th percentile latencies by 80%, and can consolidate 45% more tenants than a greedy baseline.","Aaron Elmore*, UC Santa Barbara; Sudipto Das, Microsoft Research; Alexander Pucher, UCSB; Divyakant Agrawal, ; Amr El Abbadi, UCSB; Xifeng Yan, UCSB",aelmore@cs.ucsb.edu; sudiptod@microsoft.com; pucher@cs.ucsb.edu; agrawal@cs.ucsb.edu; amr@cs.ucsb.edu; xyan@cs.ucsb.edu,"Cloud Computing, Map Reduce, Parallel, Distributed, P2P Systems*",,,643,Minimal MapReduce Algorithms,"MapReduce has become a dominant parallel computing paradigm for {\em big data}, i.e., colossal datasets at the scale of tera-bytes or higher. Ideally, a MapReduce system should achieve a high degree of load balancing among the participating machines, and minimize the space usage, CPU and I/O time, and network transfer at each machine. Although these principles have guided the development of MapReduce algorithms, limited emphasis has been placed on enforcing serious constraints on the aforementioned metrics simultaneously. This paper presents the notion of {\em minimal algorithm}, that is, an algorithm that guarantees the best parallelization in multiple aspects at the same time, up to a small constant factor. We show the existence of elegant minimal algorithms for a set of fundamental database problems, and demonstrate their excellent performance with extensive experiments.","Yufei Tao*, CUHK; Wenqing Lin, NTU; Xiaokui Xiao, Nanyang Technological University",taoyf@cse.cuhk.edu.hk; wlin1@e.ntu.edu.sg; xkxiao@ntu.edu.sg,"Cloud Computing, Map Reduce, Parallel, Distributed, P2P Systems*",Thursday Morning,,,,,,,,,SIGMOD Keynote II,,"Keynote: The Complexity of Scale in Financial Servicesby Paul Yaron",,"Paul Yaron (Distinguished Engineer and Executive Director, J. P. Morgan Chase)",,50,16,Data Cleaning,661,Towards a Commodity Data Cleaning System,"Despite the increasing importance of data quality and the rich theoretical and practical contributions in all aspects of data cleaning, there is no single end-to-end off-the-shelf solution to (semi-)automate the detection and the repairing of violations w.r.t. a set of heterogeneous and ad-hoc quality constraints. In short, there is no commodity platform similar to general purpose DBMSs that can be easily customized and deployed to solve application-specific data quality problems. In this paper, we present XYZ, an extensible, generalized and easy-to-deploy data cleaning platform. XYZ distinguishes between a programming interface and a core to achieve generality and extensibility. The programming interface allows the users to specify multiple types of data quality rules, which uniformly defines what is wrong with the data and (possibly) how to repair it through writing code that implements predefined classes. We show that the programming interface can be used to express many types of data quality rules beyond the well known CFDs (FDs), MDs and ETL rules. Treating user implemented interfaces as black-boxes, the core provides algorithms to detect errors and to clean data. The core is designed in a way to allow cleaning algorithms to cope with multiple rules holistically, i.e., detecting and repairing data errors without differentiating between various types of rules. We showcase two implementations for core repairing algorithms. These two implementations demonstrate the extensibility of our core, which can also be replaced by other user-provided algorithms. Using real-life data, we experimentally verify the generality, extensibility, and effectiveness of our system.","Michele Dallachiesa, University of Trento; Amr Ebaid, Purdue Universify; Ahmed Eldawy, University of Minnesota; Ahmed Elmagarmid, QCRI; Ihab Ilyas, Qatar Computing Research Institute; Mourad Ouzzani, Qatar Computing Res. Inst.; Nan Tang*, QCRI",dallachiesa@disi.unitn.it; aebaid@cs.purdue.edu; eldawy@cs.umn.edu; aelmagarmid@qf.org.qa; ilyas@uwaterloo.ca; mouzzani@qf.org.qa; ntang@qf.org.qa,"Aggregation, Data Warehouses, OLAP, Analytics*",,,298,Don't be SCAREd: Use SCalable Automatic REpairing with Maximal Likelihood and Bounded Changes,"Various computational procedures or constraint-based methods for data repairing have been proposed over the last decades to identify errors and, when possible, correct them. However, these approaches have several limitations including the scalability and quality of the values to be used in replacement of the errors. In this paper, we propose a new data repairing approach that is based on maximizing the likelihood of replacement data given the data distribution, which can be modeled using statistical machine learning techniques. This is a novel approach combining machine learning and likelihood methods for cleaning dirty databases by value modification. We develop a quality measure of the repairing updates based on the likelihood benefit and the amount of changes applied to the database. We propose SCARE (SCalable Automatic REpairing), a systematic scalable framework that follows our approach. SCARE relies on a robust mechanism for horizontal data partitioning and a combination of machine learning techniques to predict the set of possible updates. Due to data partitioning, several updates can be predicted for a single record based on local views on each data partition. Therefore, we propose a mechanism to combine the local predictions and obtain accurate final predictions. Finally, we experimentally demonstrate the effectiveness, efficiency, and scalability of our approach on real-world datasets in comparison to recent data cleaning approaches.","Mohamed Yakout*, Microsoft Corp.; Ahmed Elmagarmid, QCRI; Laure Berti-Equille, IRD",myakout@microsoft.com; aelmagarmid@qf.org.qa; Laure.Berti@ird.fr,"Aggregation, Data Warehouses, OLAP, Analytics*",,,629,Determining the Relative Accuracy of Attributes,"The relative accuracy problem is to determine, given tuples t1 and t2 that refer to the same entity e, whether t1[A] is more accurate than t2[A], i.e., t1[A] is closer to the true value of the A attribute of e than t2[A]. This has been a longstanding issue for data quality, and is challenging when the true values of e are unknown. This paper proposes a model for determining relative accuracy. (1) We introduce a class of accuracy rules and an inference system with a chase procedure, to deduce relative accuracy. (2) We identify and study several fundamental problems for relative accuracy. Given a set Ie of tuples pertaining to the same entity e and a set of accuracy rules, these problems are to decide whether the chase process terminates, is Church-Rosser, and leads to a unique target tuple te composed of the most accurate values from Ie for all the attributes of e. (3) We propose a framework for inferring accurate values with user interaction. (4) We provide algorithms underlying the framework, to find the unique target tuple te whenever possible; when there is no enough information to decide a complete te, we compute top-k candidate targets based on a preference model. (5) Using real-life and synthetic data, we experimentally verify the effectiveness and efficiency of our method.","Yang Cao, Beihang University; Wenfei Fan, University of Edinburgh; Wenyuan Yu*, University of Edinburgh",caoyang@act.buaa.edu.cn; wenfei@inf.ed.ac.uk; wenyuan.yu@ed.ac.uk,"Database Models, Uncertainty, Schema Matching, Data Integration*"100,17,Complex Event Processing,73,Photon: Fault-tolerant and scalable joining of continuous data streams,"Photon is a large-scale stateful distributed system for joining multiple continuously flowing streams of data with very high scalability and low latency. Photon can withstand infrastructure degradation and datacenter-level outages without any manual intervention, providing significantly better fault-tolerance and lower maintenance overhead compared to typical single-datacenter systems. In addition, Photon guarantees at-most-once semantics at any point of time, near-exact semantics in real-time, and exactly-once semantics eventually. Photon is currently deployed in a real production system to join multiple data streams and generate joined logs which are critical for key business metrics. Our current production deployment processes millions of events per minute at peak with an average end-to-end latency of less than 10 seconds. In this paper, we describe the architecture of Photon and performance results from our live production deployment.","Manpreet Singh*, Google; Ashish Gupta, Google; Haifeng Jiang, Google; Sumit Das, Google; Tianhao Qiu, Google; Venkatesh Basker, Google; Alexey Reznichenko, Max Planck Institute for Software Systems; Shivakumar Venkataraman, Google; Deomid Ryabkov, Google; Ananth Ananthanarayanan,",manpreet@google.com; agupta@google.com; jianghf@google.com; sumitdas@google.com; tianhao@google.com; vbasker@google.com; areznich@mpi-sws.org; shivav@google.com; rojer@google.com; ananthr@google.com,"Streams, Sensor Networks, Complex Event Processing*",,,623,Utility-Maximizing Event Stream Suppression,"Complex Event Processing (CEP) has emerged as a technology for monitoring event streams in search of user specified event patterns. While there has been extensive research into efficient pattern detection for CEP, its privacy implications have largely been overlooked. In this paper we consider how to suppress events on a stream to prevent the disclosure of sensitive patterns, while ensuring that nonsensitive patterns are still reported by the CEP engine. We first formally define the problem of utility-maximizing event suppression for privacy preservation, and analyze its computational hardness. We then design a suite of real-time solutions that eliminate private pattern matches while maximizing the overall utility. Our first solution optimally solves the problem at the event-type level. The second solution, at event-instance level, further optimizes the event-type level solution by exploiting runtime event distributions using advanced pattern match cardinality estimation techniques. Despite of the fact that we show the problem to be intractable and even inapproximable in polynomial time, our experimental evaluation over both real-world and synthetic data demonstrates that our algorithms are surprisingly effective in preserving utility yet still efficient enough to be used in a real-time stream environment.","Di Wang*, Worcester Polytechnic Institut; Yeye He, Microsoft; Elke Rundensteiner, WPI; Jefferey Naughton,",wangdi@cs.wpi.edu; yeyehe@microsoft.com; rundenst@cs.wpi.edu; naughton@cs.wisc.edu,"Streams, Sensor Networks, Complex Event Processing*",,,633,E-Matching: Event Processing over Noisy Sequences in Real Time,"Regular expression matching over sequences in real time is a crucial task in complex event processing on data streams. Given that such data sequences are often noisy and errors have temporal and spatial correlations, performing regular expression matching effectively and efficiently is a challenging task. Instead of the traditional approach of learning a distribution of the stream first and then processing queries, we propose a new approach that efficiently does the matching based on an error model. In particular, our algorithms are based on the realistic Markov chain error model, and report all matching paths to trace relevant basic events that trigger the matching. This is much more informative than a single matching path. We also devise algorithms to efficiently return only top-k matching paths, and to handle negations in an extended regular expression. Finally, we conduct a comprehensive experimental study to evaluate our algorithms using real datasets.","Zheng Li, University of Massachusetts, Lowell; Tingjian Ge*, Univ. of Massachusetts, Lowell; Cindy Chen, University of Massachusetts, Lowell",zli@cs.uml.edu; ge@cs.uml.edu; cchen@cs.uml.edu,"Streams, Sensor Networks, Complex Event Processing*"100,18,"Systems, Performance III",673,Practical Query Pricing with QueryMarket,"In this paper, we present and evaluate a practical interactive query pricing system, QueryMarket. The design of QueryMarket is based on the pricing framework from~\cite{KUBHS12}, where the data sellers set explicit price points as selection queries and the system automatically prices the queries issued by data consumers. In order to build such a system, we first present how to use an Integer Linear Programming formulation of the pricing problem so as to price a large class of queries, even in the case where pricing is computationally hard. Further, we show how to leverage query history to avoid double charging when queries issued over time have overlapping information, or when the database is updated. We then proceed to show a practical technique to share the revenue when multiple sellers are involved in pricing. Finally, we evaluate out approach and discuss its performance over several query workloads.","Paraschos Koutris*, University of Washington; Dan Suciu, University of Washington; Magdalena Balazinska, U of Washington; Prasang Upadhyaya, ; Bill Howe, University of Washington",pkoutris@cs.washington.edu; suciu@cs.washington.edu; magda@cs.washington.edu; prasang@cs.washington.edu; billhowe@cs.washington.edu,None of the above*,,,671,Generalized Scale Independence Through Incremental Precomputation,"Developers of rapidly growing applications must be able to anticipate potential scalability problems before they cause performance issues in production environments. A new type of data independence, called {\em scale independence}, seeks to address this challenge by guaranteeing a bounded amount of work is required to execute all queries in an application, independent of the size of the underlying data. While optimization strategies have been developed to provide these guarantees for the class of queries that are scale-independent when executed using simple indexes, there are important queries for which such techniques are insufficient. These queries require precomputing results through the creation of incrementally-maintained materialized views. However, since this precomputation effectively shifts some of the query processing burden from execution time to insertion time, a scale-independent system must be careful to ensure that storage and maintenance costs do not threaten scalability. In this paper, we describe a scale-independent view selection and maintenance system, which uses novel static analysis techniques that ensure that created views do not themselves become scaling bottlenecks. Finally, we present an empirical analysis that includes all the queries from the TPC-W benchmark and validates our implementation's ability to maintain nearly constant high-quantile query and update latency even as an application scales to hundreds of machines.","Michael Armbrust*, UC Berkeley; Tim Kraska, Brown University; Eric Liang, UC Berkeley; Michael Franklin, UC Berkeley; Armando Fox, UC Berkeley; David Patterson, UC Berkeley",marmbrus@cs.berkeley.edu; kraskat@cs.brown.edu; ericliang@berkeley.edu; franklin@cs.berkeley.edu; fox@cs.berkeley.edu; pattrsn@cs.berkeley.edu,"Cloud Computing, Map Reduce, Parallel, Distributed, P2P Systems*",,,495,Simulation of DatabaseValued Markov Chains Using SimSQL,"This paper describes the SimSQL system, which allows for SQL-based specification, simulation, and querying of database-valued Markov chains, i.e., chains whose value at any time step comprises the contents of an entire database. SimSQL extends the earlier Monte Carlo database system (MCDB), which permitted Monte Carlo simulation of static database-valued random variables. Like MCDB, SimSQL uses user-specified “VG functions” to generate the simulated data values that are the building blocks of a simulated database. The enhanced functionality of SimSQL is enabled by the ability to parametrize VG functions using stochastic tables, so that one stochastic database can be used to parametrize the generation of another stochastic database, which can parametrize another, and so on. Other key extensions include the ability to explicitly define recursive versions of a stochastic table and the ability to execute the simulation in a MapReduce environment. We focus on using SimSQL to specify Bayesian machine learning models in a concise and natural manner, and then to analyze such models at scale using Markov Chain Monte Carlo methods.","Zhuhua Cai, Rice University; Foula Vagena, LogicBlox; Subi Arumugam, Rice University; Luis Perez, Rice University; Peter Haas, IBM Almaden; Chris Jermaine*, Rice",caizhua@gmail.com; foula@acm.org; propus@gmail.com; lp6@rice.edu; phaas@us.ibm.com; cmj4@rice.edu,"Database Models, Uncertainty, Schema Matching, Data Integration*",Thursday Afternoon I,,,,,,,50,19,Privacy,622,Recursive Mechanism: Towards Node Differential Privacy and Unrestricted Joins,"Existing studies on differential privacy mainly consider aggregation on data sets where each entry corresponds to a particular participant to be protected. In many situations, a user may pose a relational algebra query on a sensitive database, and desires differentially private aggregation on the result of the query. However, no known work is capable to release this kind of aggregation when the query contains unrestricted join operations. This severely limits the applications of existing differential privacy techniques because many data analysis tasks require unrestricted joins. One example is subgraph counting on a graph. Existing methods for differentially private subgraph counting address only edge differential privacy and are subject to very simple subgraphs. Before this work, whether any nontrivial graph statistics can be released with reasonable accuracy under node differential privacy is still an open problem. In this paper, we propose a novel differentially private mechanism to release an approximation to a linear statistic of the result of some positive relational algebra calculation over a sensitive database. Unrestricted joins are supported in our mechanism. The error bound of the approximate answer is roughly proportional to the \emph{empirical sensitivity} of the query --- a new notion that measures the maximum possible change to the query answer when a participant withdraws its data from the sensitive database. For subgraph counting, our mechanism provides the first solution to achieve node differential privacy, for any kind of subgraphs.","Shixi Chen*, Fudan University; Shuigeng Zhou, Fudan University",chensx@fudan.edu.cn; sgzhou@fudan.edu.cn,"Security, Privacy, Authenticated Query Processing*",,,665,PrivGene: Differentially Private Model Fitting Using Genetic Algorithms,"epsilon-differential privacy is rapidly emerging as the state-of-theart scheme for protecting individuals’ privacy in published analysis results over sensitive data. The main idea is to perform random perturbations on the analysis results, such that any individual’s presence in the data has negligible impact on the randomized results. This paper focuses on analysis tasks that involve model fitting, i.e., finding the parameters of a statistical model that best fit the dataset. For such tasks, the quality of the differentially private results depends upon both the effectiveness of the model fitting algorithm, and the amount of perturbations required to satisfy the privacy guarantees. Most previous studies start from a state-of-the-art, non-private model fitting algorithm, and develop a differentially private version. Unfortunately, many model fitting algorithms require intensive perturbations to satisfy epsilon-differential privacy, leading to poor overall result quality. Motivated by this, we propose PrivGene, a general-purpose differentially private model fitting solution based on genetic algorithms (GA). PrivGene needs significantly less perturbations than previous methods, and it achieves higher overall result quality, even for model fitting tasks where GA is not the first choice without privacy considerations. Further, PrivGene performs the random perturbations using a novel technique called the enhanced exponential mechanism, which improves over the exponential mechanism by exploiting the special properties of model fitting tasks. As case studies, we apply PrivGene to three common analysis tasks involving model fitting: logistic regression, SVM classification, and k-means clustering. Extensive experiments using real data confirm the high result quality of PrivGene, and its superiority over existing methods.","Jun Zhang*, Nanyang Technological Univ.; Yin Yang, ADSC Singapore; Xiaokui Xiao, Nanyang Technological University; Zhenjie Zhang, ADSC Singapore; Marianne Winslett, University of Illinois at Urbana-Champaign",jzhang027@ntu.edu.sg; yin.yang@adsc.com.sg; xkxiao@ntu.edu.sg; zhenjie@adsc.com.sg; winslett@uiuc.edu,"Security, Privacy, Authenticated Query Processing*",,,658,A First Principles Approach to Utility in Statistical Privacy,"In statistical privacy, “utility” is a consideration that arises from two important questions: (1) How should (or shouldn’t) we measure the utility of a sanitizing algorithm (i.e. what quantity should we try to maximize when we design, assess, and compare sanitizing algorithms)? (2) How should we process/analyze sanitized data? In this paper we present three contributions to the study of utility. First, we analyze some of the most popular types of utility measures that are currently in use. We show that they have the undesirable property of rewarding data sanitizers for throwing away information unnecessarily. Second, using three utility axioms, we show that the utility of a data sanitizer should be measured as the average (over possible outputs of the sanitizer) error of a Bayesian decision maker. This result is surprising because the axioms do not mention users, Bayesians, nor subjective probabilities. This result is important because it provides a complete justification for why a rarely used utility measure should be one of the main focuses of research into privacy-preserving algorithms. Finally, we present a new estimation algorithm for the sorted histogram problem in which accurate information must be extracted from privacy-enhanced sanitized data. Our algorithm is based on the insight that since utility of a sanitizing algorithm should be measured in terms of the error of a Bayesian decision maker, analyzing the output of such an algorithm should be done using Bayesian decision theory. In a thorough empirical evaluation, we show that our algorithm consistently and significantly outperforms previously proposed approaches.","Bing-Rong Lin, Penn State; Daniel Kifer *, Penn State",blin@cse.psu.edu; dkifer@cse.psu.edu,"Security, Privacy, Authenticated Query Processing*"100,20,Spatial Databases II,222,Collective Spatial Keyword Queries: A Distance Owner-Driven Approach,"Recently, spatial keyword queries become a hot topic in the literature. One example of these queries is the collective spatial keyword query (CoSKQ) which is to find a set of objects in the database such that it covers a set of given keywords collectively and has the smallest cost. Unfortunately, existing exact algorithms usually have severe scalability problems and existing approximate algorithms, though scalable, cannot return near-to-optimal solutions. In this paper, we study the CoSKQ problem and address the above two problems. Firstly, we consider the CoSKQ problem using an existing cost measurement called the maximum sum cost. This problem is called MaxSum-CoSKQ and is shown to be NP-hard in the literature. Interestingly, we observe that the maximum sum cost of a set of objects is dominated (or determined) by at most three objects which are called as the distance owners of the set. Motivated by this, we propose a distance owner-driven approach to solve MaxSum-CoSKQ. We develop two algorithms for MaxSum-CoSKQ: one is an exact algorithm which runs faster than the best-known existing algorithm by several orders of magnitude and the other is an approximate algorithm which improves the best-known constant approximation factor from 2 to 1.375. Secondly, we propose a new cost measurement called diameter and the CoSKQ with this new measurement is called Dia-CoSKQ. We prove that Dia-CoSKQ is NP-hard. Besides, we design two algorithms for Dia-CoSKQ: one is an exact algorithm which beats the adaption of an existing algorithm in terms of both efficiency and scalability and the other is an approximate algorithm which gives a $\sqrt{3}$-factor approximation. We conducted extensive experiments on real datasets which shows that the proposed exact algorithms are scalable and the proposed approximate algorithms return near-to-optimal solutions.","Cheng Long*, HKUST; Raymond Chi-Wing Wong, HKUST; Ke Wang, SFU; Ada Fu, Chinease University of Hong Kong",snobari@nus.edu.sg; farhan.tauheed@epfl.ch; thomas.heinis@epfl.ch; anastasia.ailamaki@epfl.ch; karras@business.rutgers.edu; steph@nus.edu.sg,"Spatial, Temporal, Multimedia and Scientific Databases*",,,132,TOUCH: In-Memory Spatial Join by Hierarchical Data-Oriented Partitioning,"Efficient spatial joins are pivotal for many applications and particularly important for geographical information systems or for the simulation sciences where scientists work with spatial models. Past research has primarily focused on disk-based spatial joins; efficient in-memory approaches, however, are important for two reasons: a) main memory has grown so large that many datasets fit in it and b) the in-memory join is a very time-consuming part of all disk-based spatial joins. In this paper we develop TOUCH, a novel in-memory spatial join algorithm that uses hierarchical data-oriented space partitioning, thereby keeping both its memory footprint and the number of comparisons low. Our results show that TOUCH outperforms known in-memory spatial-join algorithms as well as in-memory implementations of disk-based join approaches. In particular, it has an one-order-of-magnitude advantage over the memory-demanding state of the art in terms of number of comparisons (i.e., pairwise object comparisons), as well as execution time, while it is two orders of magnitude faster when compared to approaches with a similar memory footprint. Furthermore, TOUCH is more scalable than competing approaches as data density grows.","Sadegh Nobari, ; Farhan Tauheed, EPFL; Thomas Heinis*, EPFL; Anastasia Ailamaki, EPFL; Panagiotis Karras, Rutgers; Stephane Bressan,",,,,,555,Finding Time Period-Based Most Frequent Path in Big Trajectory Data,"The rise of GPS-equipped mobile devices has let to the emergence of big trajectory data. In this paper, we study a new path finding query which finds the most frequent path (MFP) during user specified time periods in large-scale historical trajectory data. We refer to this query as time period-based MFP (TPMFP). Specifically, given a time period T, a source Vs and a destination Vd, TPMFP searches the MFP from Vs to Vd during T. Though there exists several proposals on defining MFP, they only consider a fixed time period. Most importantly, we find that none of them can well reflect people’s common sense notion which can be described by three key properties, namely suffix-optimal (i.e., any suffix of an MFP is also an MFP), length-insensitive (i.e., MFP should not favor shorter or longer paths), and bottleneck-free (i.e., MFP should not contain infrequent edges). The TPMFP with the above properties will reveal not only common routing preferences of the past travelers, but also take the time effectiveness into the consideration. Therefore, our first task is to give a TPMFP definition which satisfies the above three properties. Then, given the comprehensive TPMFP definition, our next task is to find TPMFP over huge amount of trajectory data efficiently. Particularly, we propose efficient search algorithms together with novel indexes to speed up the processing of TPMFP. To demonstrate both the effectiveness and the efficiency of our approach, we conduct extensive experiments using a real dataset containing over 11 million trajectories.","Wuman Luo*, HKUST; Haoyu Tan, HKUST; Lei Chen, HKUST; Lionel M. Ni, HKUST",luowuman@cse.ust.hk; hytan@cse.ust.hk; leichen@cse.ust.hk; ni@cse.ust.hk,"Spatial, Temporal, Multimedia and Scientific Databases*"150,21,Data Streams,460,Integrating Scale Out and Fault Tolerance in Stream Processing using Operator State Management,"As users of ""big data"" applications expect fresh results, we witness a new breed of stream processing systems (SPS) that are designed to scale to large numbers of cloud-hosted machines. Such systems face new challenges: (i) to benefit from the ""pay-as-you-go"" model of cloud computing, they must scale out on demand, acquiring additional virtual machines (VMs) and parallelising operators when the workload increases; (ii) failures are common with deployments on hundreds of VMs --systems must be fault-tolerant with fast recovery times, yet low per-machine overheads. An open question is how to achieve these two goals when stream queries include stateful operators, which must be scaled out and recovered without affecting query results. Our key idea is to expose internal operator state explicitly to the SPS through a set of state management primitives. Based on them, we describe a new integrated approach for dynamic scale out and recovery of stateful operators. Externalised operator state is checkpointed periodically by the SPS and backed up to upstream VMs. The SPS then identifies individual operator bottlenecks and automatically scales them out by allocating new VMs and partitioning the checkpointed state from upstream VMs. At any point, failed operators are recovered by restoring checkpointed state on a new VM and replaying unprocessed tuples, not yet reflected in the state. We evaluate this approach with the Linear Road Benchmark on the Amazon EC2 cloud platform and show that it can scale automatically to a load factor of L= 350 with 50 VMs, while recovering quickly from failure.","Raul Castro Fernandez*, Imperial College London; Matteo Migliavacca, University of Kent; Evangelia Kalyvianaki, Imperial College London; Peter Pietzuch, Imperial College London",r.castro-fernandez11@imperial.ac.uk; mm53@kent.ac.uk; ekalyv@doc.ic.ac.uk; prp@doc.ic.ac.uk,"Streams, Sensor Networks, Complex Event Processing*",,,634,Quantiles over Data Streams: An Experimental Study,"A fundamental problem in data management and analysis is to generate descriptions of the distribution of data. It is most common to give such descriptions in terms of the cumulative distribution, which is characterized in terms of the quantiles of the data. The design and engineering of efficient methods to find these quantiles has attracted much study, especially in the case where the data is described incrementally, and we must compute the quantiles in an online, streaming fashion. Yet while such algorithms have proved to be tremendously useful in practice, there has been limited formal comparison of the competing methods, and no comprehensive study of their performance. In this paper, we remedy this deficit by providing a taxonomy of different methods, and describe efficient implementations. In doing so, we highlight variations that have not been explicitly studied before, yet which turn out to perform the best. To illustrate this, we provide detailed experimental comparisons demonstrating the tradeoffs between space, time, and accuracy for quantile computation.","Lu Wang, ; Ge Luo, ; Ke Yi*, HKUST; Graham Cormode, AT&T Research",luwang@cse.ust.hk; luoge@cse.ust.hk; yike@cse.ust.hk; graham@research.att.com,"Streams, Sensor Networks, Complex Event Processing*",,,662,An Efficient Indexing Mechanism for Filtering Geo-Textual Data over a Stream of Geo-Textual Queries,"Massive amount of data that are geo-tagged and associated with text information are being generated at an unprecedented scale. Users may want to be notified of interesting geo-textual objects during a period of time. For example, a user may want to be alerted when tweets containing terms ``food poisoning"" are posted within 1 {\it km} of a user specified location in the next 48 hours. In this paper, for the first time we study the problem of matching a stream of incoming Boolean Range Continuous queries over a stream of incoming geo-textual objects in real time. We develop a new system for addressing the problem. In particular, we propose a hybrid index, called IQ-tree, and novel cost models for managing a stream of incoming Boolean Range Continuous queries. We also propose algorithms for matching the queries with incoming geo-textual objects based on the index. Results of empirical studies with implementations of the proposed techniques demonstrate that the paper's proposals offer scalability and are capable of excellent performance.","Lisi Chen, NTU; Gao Cong*, Nanyang Technological University, Singapore; Xin Cao, NTU",LCHEN012@e.ntu.edu.sg; gaocong@ntu.edu.sg; xcao1@e.ntu.edu.sg,"Streams, Sensor Networks, Complex Event Processing*"200,22,Distributed Systems,377,Bolt-on Causal Consistency,"We consider the problem of separating consistency-related safety properties from liveness, replication, and durability in distributed data stores via the application of a ""bolt-on"" shim layer that upgrades the safety of an underlying general-purpose data store. This allows us to provide the same consistency guarantees atop a wide range of widely deployed but often inflexible stores. In light of recent work proving that causal consistency is the strongest consistency model that is available during system partitions, we develop a shim layer that upgrades eventually consistent stores to provide convergent causal consistency. Accordingly, we leverage widely deployed eventually consistent infrastructure as a common substrate for providing causal guarantees, which is especially useful given the current lack of a production-ready causally consistent store. We describe algorithms and shim implementations that are suitable for a large class of application-level causality relationships. We evaluate our techniques using an existing, production-ready data store and demonstrate their scalability under real-world explicit causality relationships.","Peter Bailis*, UC Berkeley; Ali Ghodsi, UC Berkeley; KTH; Joseph Hellerstein, UC Berkeley; Ion Stoica, UC Berkeley",pbailis@cs.berkeley.edu; alig@cs.berkeley.edu; hellerstein@cs.berkeley.edu; istoica@cs.berkeley.edu,"Cloud Computing, Map Reduce, Parallel, Distributed, P2P Systems*",,,620,RTP: Robust Tenant Placement for Elastic In-Memory Database Clusters,"In the cloud services industry, a key issue for cloud operators is to minimize operational costs. In this paper, we consider algorithms that elastically contract and expand a cluster of in-memory databases depending on tenants' behavior over time while maintaining response time guarantees. We evaluate our tenant placement algorithms using traces obtained from one of SAP's production on-demand applications. Our experiments reveal that our approach lowers operating costs for the database cluster of this application by a factor of 2.2 to 10, measured in Amazon EC2 hourly rates, in comparison to the state of the art approaches. In addition, we carefully study the trade-off between cost savings obtained by continuously migrating tenants and the robustness of servers towards load spikes and failures.","Jan Schaffner*, Hasso Plattner Institute; Tim Januschowski, SAP AG; Megan Kercher, SAP AG; Tim Kraska, Brown University; Hasso Plattner, Hasso Plattner Institute; Michael Franklin, UC Berkeley; Dean Jacobs, SAP AG",jan.schaffner@hpi.uni-potsdam.de; tim.januschowski@sap.com; megan.kercher@gmail.com; kraskat@cs.brown.edu; hasso.plattner@hpi.uni-potsdam.de; franklin@cs.berkeley.edu; dean.jacobs@sap.com,"Cloud Computing, Map Reduce, Parallel, Distributed, P2P Systems*",,,216,Inter-Media Hashing for Large-scale Retrieval from Heterogeneous Data Sources,"In this paper we present a new multimedia retrieval paradigm called inter-media retrieval to innovate large-scale multimedia search. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents, images, and videos from different data sources. This utilizes the widely available data from different sources and caters for the current users' demand of receiving a result list simultaneously containing text documents, images, and videos to obtain a comprehensive understanding of the query's results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with the state-of-the-art techniques. We have also released the datasets for public use.","Jingkuan Song, The University of Queensland; Yang Yang, The University of Queensland; Yi Yang, CMU; Zi Huang, The University of Queensland; Heng Tao Shen*, The University of Queensland",jk.song@itee.uq.edu.au; yang.yang@itee.uq.edu.au; yiyang@cs.cmu.edu; huang@itee.uq.edu.au; shenht@itee.uq.edu.au,"Spatial, Temporal, Multimedia and Scientific Databases*",Thursday Afternoon II,,,,,,,50,23,Data Mining,533,Mind the Gap: Large-Scale Frequent Sequence Mining,"Frequent sequence mining is one of the fundamental building blocks in data mining. While the problem has been extensively studied, few of the available techniques are sufficiently scalable to handle datasets with billions of sequences; such large-scale datasets arise, for instance, in text mining and session analysis. In this paper, we propose PFSM, a scalable algorithm for frequent sequence mining on MapReduce. PFSM can handle so-called ""gap constraints"", which can be used to limit the output to a controlled set of frequent sequences. At its heart, PFSM partitions the input database in a way that allows us to mine each partition independently using any existing frequent sequence mining algorithm. We introduce the notion of w-equivalency, which is a generalization of the notion of a projected database used by many frequent pattern mining algorithms. We also present a number of optimization techniques that minimize partition size, and therefore computational and communication costs, while still maintaining correctness. Our extensive experimental study in the context of text mining suggests that PFSM is significantly more efficient and scalable than alternative approaches.","Iris Miliaraki*, Max Planck Institute; Klaus Berberich, Max-Planck Institute ; Rainer Gemulla, ; Spyros Zoupanos,",miliaraki@mpi-inf.mpg.de; kberberi@mpi-inf.mpg.de; rgemulla@mpi-inf.mpg.de; zoupanos@mpi-inf.mpg.de,"Knowledge Discovery, Clustering, Data Mining*",,,649,Reverse Engineering Complex Join Queries,"e study the following problem: Given a database $\D$ with schema $\G$ and an output table $Out$, compute a join query $Q$ that generates $Out$ from $\D$. A simpler variant allows $Q$ to return a superset of $Out$. This problem has numerous applications, both by itself, and as a building block for other problems arising in data mining, keyword search and schema mapping. Related prior work imposes conditions on the structure of $Q$ which are not always consistent with the application, but are imposed for ease of computation. We discuss several natural SQL queries that do not observe these conditions and cannot be discovered by prior work. In this paper, we propose an efficient algorithm that discovers queries with arbitrary join graphs. A crucial insight is that any graph can be characterized by the combination of a simple structure, called a star, and a series of merge steps over the star. The merge steps define a lattice over graphs derived from the same star. This allows us to explore the set of candidate solutions in a principled way and quickly prune out a large number of infeasible graphs. We also design several optimizations that significantly reduce the running time (by orders of magnitude in some cases). Finally, we conduct an extensive experimental study over a benchmark database and show that our approach is scalable and accurately discovers complex queries.","Meihui Zhang, NUS; Hazem Elmeleegy, ; Cecilia Procopiuc, AT&T Research Labs; Divesh Srivastava*, AT&T Labs-Research",mhzhang@comp.nus.edu.sg; hazem.elmeleegy@turn.com; magda@research.att.com; divesh@research.att.com,"Knowledge Discovery, Clustering, Data Mining*",,,667,A Direct Mining Approach To Efficient Constrained Graph Pattern Discovery,"Despite the wealth of research on frequent graph pattern mining, how to efficiently mine the complete set of those with constraints still poses huge challenge to the existing algorithms mainly due to the inherent bottleneck in the mining paradigm. In essence, mining requests with explicitly-specified constraints cannot be handled in a way that is direct and precise. In this paper, we propose a direct mining framework to solve the problem and illustrate our ideas in the context of a particular type of constrained frequent patterns --- the ``skinny'' patterns, which are graph patterns with a long backbone from which short twigs branch out. These patterns, which we formally define as $l$-long $\delta$-skinny patterns, are able to reveal insightful spatial and temporal trajectory patterns in mobile data mining, information diffusion, adoption propagation and many others. Based on the key concept of a \emph{canonical diameter}, we develop \textsf{SkinnyMine}, an efficient algorithm to mine all the $l$-long $\delta$-skinny patterns guaranteeing both the completeness of our mining result as well as the unique generation of each target pattern. We also present a general direct mining framework together with two properties of \emph{reducibility} and \emph{continuity} for qualified constraints. Our experiments on both synthetic and real data demonstrate the effectiveness and efficiency of our approach.","Feida Zhu*, Singapore Management Universit; Zequn Zhang, Singapore Management University; Qiang Qu, Aarhus University",fdzhu@smu.edu.sg; zqzhang@smu.edu.sg; qu@cs.au.dk,"Knowledge Discovery, Clustering, Data Mining*"100,24,Road Networks and Trajectories,621,Calibrating Trajectory Data for Similarity-based Analysis,"Due to the prevalence of GPS-enabled devices, wireless communications and sensing technologies, spatial trajectories that describe the movement histories of moving objects are being generated and accumulated with an unprecedented pace. Given the fact that a trajectory is a discrete approximation of the original continuous path by sampling the locations periodically, the trajectories in a practical database could be derived by quite different sample strategies, resulting in a set of heterogeneous trajectory data. The heterogeneity of trajectory data has negative impact on the effectiveness of trajectory similarity measures, which are the basis of many crucial trajectory processing tasks. In this work, we pioneer a systematic approach to transform a heterogeneous trajectory dataset to the one with (almost) unified sample strategies, which is called trajectory calibration. Specifically, we propose an anchor-based calibration system that aligns trajectories to a set of anchor points, which are fixed regions independent of any trajectory source. As the first step to tackle this problem, we present different types of anchor points, which are suitable for building stable reference systems. On top of that, we first propose a geometric-based calibration approaches, which only consider the spatial relationship between anchor points and trajectories. Then a model-based calibration method is designed, which leverage the historical trajectory data to train an inference model, to further improve the calibration performance. Finally, we conduct extensive experiments based on real trajectory dataset to demonstrate the effectiveness and efficiency of the proposed calibration system.","Han Su*, University of Queensland; Kai Zheng, university of queensland; Haozhou Wang, university of queensland; Jiamin Huang, Nanjing University; Xiaofang Zhou, U. of Queensland",Diwen.Zhu@gmail.com; MaHui@ntu.edu.sg; xkxiao@ntu.edu.sg; sqluo@fudan.edu.cn; tangyouze@gmail.com; sgzhou@fudan.edu.cn,"Spatial, Temporal, Multimedia and Scientific Databases*",,,650,On Optimal Worst-Case Matching,"Bichromatic reverse nearest neighbor (BRNN) queries have been studied extensively in the literature of spatial databases. Given a set P of service-providers and a set O of customers, a BRNN query is to find which customers in O are ""interested"" in a given service-provider in P. Recently, it has been found that this kind of queries lacks the consideration of the capacities of service-providers and the demands of customers. In order to address this issue, some spatial matching problems have been proposed in the literature. However, these existing spatial matching problems do not consider some real-life applications where the maximum matching cost (or distance) should be minimized. One example of these applications is that hospitals are matched with residential estates where the greatest distance between a hospital and a matched residential estate should be minimized. In this paper, we propose a new problem called SPatial Matching for Minimizing Maximum matching distance (SPM-MM). Then, we design two algorithms for SPM-MM, Threshold-Adapt and Swap-Chain. Threshold-Adapt is simple and easy to understand but not scalable to large datasets due to its relatively high time complexity. Swap-Chain, which follows a fundamentally different idea from Threshold-Adapt, runs faster than Threshold-Adapt by several orders of magnitude. We conducted extensive empirical studies which verify the efficiency and scalability of Swap-Chain.","Cheng Long*, HKUST; Raymond Chi-Wing Wong, HKUST; Philip S. Yu, UIC",clong@cse.ust.hk; raywong@cse.ust.hk; psyu@uic.edu,"Spatial, Temporal, Multimedia and Scientific Databases*",,,317,Shortest Path and Distance Queries on Road Networks: Towards Bridging Theory and Practice,"Given two locations s and t in a road network, a distance query returns the minimum network distance from s to t, while a shortest path query computes the actual route that achieves the minimum distance. These two types of queries find important applications in practice, and a plethora of solutions have been proposed in past few decades. The existing solutions, however, are optimized for either practical or asymptotic performance, but not both. In particular, the techniques with enhanced practical efficiency are mostly heuristic-based, and they offer unattractive worst-case guarantees in terms of space and time. On the other hand, the methods that are worst-case efficient often entail prohibitive preprocessing or space overheads, which render them inapplicable for the large road networks (with millions of nodes) commonly used in modern map applications. This paper presents Arterial Hierarchy (AH), an index structure that narrows the gap between theory and practice in answering shortest path and distance queries on road networks. On the theoretical side, we show that, under a realistic assumption, AH answers any distance query in $\tilde{O}(\log \r)$ time, where $\r = d_{max}/d_{min}$, and $d_{max}$ (resp.\ $d_{min}$) is the largest (resp.\ smallest) $L_\infty$ distance between any two nodes in the road network. In addition, any shortest path query can be answered in $\tilde{O}(k + \log \r)$ time, where k is the number of nodes on the shortest path. On the practical side, we experimentally evaluate AH on a large set of real road networks with up to twenty million nodes, and we demonstrate that (i) AH outperforms the state of the art in terms of query time, and (ii) its space and pre-computation overheads are moderate.","Andy Diwen Zhu, Nanyang Technological University; Hui Ma, Nanyang Technological University; Xiaokui Xiao*, Nanyang Technological University; Siqiang Luo, Fudan University; Youze Tang, Nanyang Technological University; Shuigeng Zhou, Fudan University",,50,25,Security,231,Fine-Grained Disclosure Control for App Ecosystems,"The modern computing landscape contains an increasing number of app ecosystems, where users store personal data on platforms such as Facebook or smartphones. APIs enable third-party applications (apps) to utilize that data. A key concern associated with app ecosystems is the confidentiality of user data. In this paper, we develop a new model of disclosure in app ecosystems. In contrast with previous solutions, our model is data-derived and semantically meaningful. Information disclosure is modeled in terms of a set of distinguished security views. Each query is labeled with the precise set of security views that is needed to answer it, and these labels drive policy decisions. We explain how our disclosure model can be used in practice and provide algorithms for labeling conjunctive queries for the case of single-atom security views. We show that our approach is useful by demonstrating the scalability of our algorithms and by applying it to the real-world disclosure control system used by Facebook.","Gabriel Bender*, Cornell University; Lucja Kot, Cornell University; Johannes Gehrke, Cornell University; Christoph Koch, EPFL",gbender@cs.cornell.edu; lucja@cs.cornell.edu; johannes@cs.cornell.edu; christoph.koch@epfl.ch,"Security, Privacy, Authenticated Query Processing*",,,436,Lightweight Authentication of Linear Algebraic Queries on Data Streams,"We consider a stream outsourcing setting, where a data owner delegates the management of a set of disjoint data streams to an untrusted server. The owner authenticates his streams via signatures. The server processes continuous queries on the union of the streams for clients trusted by the owner. Along with the results, the server sends proofs of result correctness derived from the owner’s signatures, which are easily verifiable by the clients. We design novel constructions for a collection of fundamental problems over streams represented as linear algebraic queries. In particular, our basic schemes authenticate dynamic vector sums and dot products, as well as dynamic matrix products. These techniques can be adapted for authenticating a wide range of important operations in streaming environments, including group by queries, joins, in-network aggregation, similarity matching, and event processing. All our schemes are very lightweight, and offer strong cryptographic guarantees derived from formal definitions and proofs. We experimentally confirm the practicality of our schemes in the performance sensitive streaming setting.","Stavros Papadopoulos*, Hong Kong University of Science and Technology; Graham Cormode, AT&T Research; Antonios Deligiannakis, Technical University of Crete; Minos Garofalakis, Technical University of Crete",stavrosp@cse.ust.hk; adeli@softnet.tuc.gr; graham@research.att.com; minos@softnet.tuc.gr,"Security, Privacy, Authenticated Query Processing*"100,26,Indexing,624,Column Imprints: A Secondary Index Structure,"Large scale data warehouses rely heavily on secondary indexes, such as bitmaps and b-trees, to limit access to slow IO devices. However, with the advent of large main memory systems, cache conscious secondary indexes are needed to improve also the transfer bandwidth between memory and cpu. In this paper, we introduce column imprint, a novel and powerful, yet lightweight, cache conscious secondary index. A column imprint is a collection of many small bit vectors, each indexing the data points of a single cacheline. An imprint is used during query evaluation to limit data access and thus minimise memory traffic. The compression for imprints is cpu friendly and exploits the empirical observation that data often exhibits local clustering or partial ordering as a side-effect of the construction process. Most importantly, column imprint compression remains effective and robust even in the case of unclustered data, while other state-of-the-art solutions fail. We conducted an extensive experimental evaluation to assess the applicability and the performance impact of the column imprints. The storage overhead, when experimenting with real world datasets, is just a few percent over the size of the columns being indexed. The evaluation time for over 40000 range queries of varying selectivity revealed the efficiency of the proposed index compared to zonemaps and bitmaps with WAH compression.","Lefteris Sidirourgos*, CWI; Martin Kersten, cwi",lsidir@cwi.nl; mk@cwi.nl,"Storage, Indexing and Physical Database Design*",,,664,DeltaNI: An Efficient Labeling Scheme for Versioned Hierarchical Data,"Main-memory database systems are emerging as the new backbone of business applications. Besides flat relational data representations also hierarchical ones are essential for these modern applications; therefore we devise a new indexing and versioning approach for hierarchies that is deeply integrated into the relational kernel. We propose the DeltaNI index as a versioned pendant of the nested intervals (NI) labeling scheme. The index is efficient in space and time and yields a gapless, fixed-size integer NI labeling for each version while also supporting branching histories. In contrast to a naive NI labeling, the index facilitates even complex updates of the tree structure. As many query processing techniques that work on top of the NI labeling (e.g., the staircase join) have already been proposed, our index can be used as a building block for processing various kinds of queries. We evaluate the performance of the index on large inputs consisting of millions of nodes and thousands of versions. Thereby we show that DeltaNI scales well and is able to deliver satisfying performance for realistically large business scenarios.","Jan Finis*, Technische Universität München; Robert Brunel, SAP; Alfons Kemper, Technische Universität München; Thomas Neumann, Technische Universität München; Franz Faerber, SAP AG; Norman May, SAP AG",finis@in.tum.de; robert.brunel@sap.com; kemper@in.tum.de; neumann@in.tum.de; franz.faerber@sap.com; norman.may@sap.com,"Storage, Indexing and Physical Database Design*"