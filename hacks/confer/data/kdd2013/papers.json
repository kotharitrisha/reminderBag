{"769": {"abstract": "The competitive business climate and the complexity of IT environments dictate efficient and cost-effective service delivery and support of IT services. These are largely achieved by automating routine maintenance procedures, including problem detection, determination and resolution. System monitoring provides an effective and reliable means for problem detection. Coupled with automated ticket creation, it ensures that a degradation of the vital signs, defined by acceptable thresholds or monitoring conditions, is flagged as a problem candidate and sent to supporting personnel as an incident ticket. This paper describes an integrated framework for minimizing false positive tickets and maximizing coverage for the monitoring of all system faults (minimizing false negative tickets). In particular, the integrated framework defines monitoring conditions and the optimal corresponding delay times based on an off-line analysis of historical alerts and the matching incident tickets. Potential monitoring conditions are built on a set of predictive rules which are automatically generated by a rule-based learning algorithm with coverage, confidence and rule complexity criteria. These conditions and delay times are propagated as configurations into run-time monitoring systems. Moreover, a part of misconfigured monitoring conditions can be corrected according to false negative tickets which are discovered by another text classification algorithm. This paper also provides implementation details of a program product that uses this methodology and shows some illustrative examples of successful results.", "authors": [{"affiliation": " Florida International Univ", "name": "Liang Tang"}, {"affiliation": " Florida International University", "name": " Tao Li"}, {"affiliation": " IBM Research", "name": " Larisa Shwartz"}, {"affiliation": " IBM Research", "name": " Florian Pinel"}, {"affiliation": " St. John's University", "name": " Genady Grabarnik"}], "title": "An Integrated Framework for Optimizing Automatic Monitoring Systems in Large IT Infrastructures"}, "668": {"abstract": "Frequent episode mining (FEM) is an interesting research topic in data mining with wide range of applications. However, the traditional framework of FEM treats all events as having the same importance/utility and assumes that a same type of event appears at most once at a time point. These simplifying assumptions do not reflect the characteristics of scenarios in real applications and thus the useful information of episodes in terms of utilities such as profits is lost. Furthermore, most studies on FEM focused on mining episodes in simple event sequences and few considered the scenario of complex event sequences, where different events can occur simultaneously. To address these issues, in this paper, we incorporate the concept of utility into episode mining and address a new problem of mining high utility episodes from complex event sequences, which has not been explored so far. In the proposed framework, the importance/utility of different events is considered and multiple events can appear simultaneously. Several novel features are incorporated into the proposed framework to resolve the challenges raised by this new problem, such as the absence of anti-monotone property and the huge set of candidate episodes. Moreover, an efficient algorithm named UP-Span (Utility ePisodes mining by Spanning prefixes) is proposed for mining high utility episodes with several strategies incorporated for pruning the search space to achieve high efficiency. Experimental results on real and synthetic datasets show that UP-Span has excellent performance and serves as an effective solution to the new problem of mining high utility episodes from complex event sequences.", "authors": [{"affiliation": " National Cheng Kung University", "name": "Cheng-Wei Wu"}, {"affiliation": " National Cheng Kung University, Taiwan, ROC", "name": " Yu Feng Lin"}, {"affiliation": " University of Illinois", "name": " Philip Yu"}, {"affiliation": " National Cheng Kung University", "name": " Vincent Tseng"}], "title": "Mining High Utility Episodes in Complex Event Sequences"}, "1148": {"abstract": "Global-state networks provide a powerful mechanism to model the increasing heterogeneity in data generated by current systems. Such a network comprises of a series of network snapshots with dynamic local states at nodes, and a global network state indicating the occurrence of an event. Mining discriminative subgraphs from global-state networks allows us to identify the influential sub-networks that have maximum impact on the global state and unearth the complex relationships between the local entities of a network and their collective behavior. In this paper, we explore this problem and design a technique called MINDS to mine minimally discriminative subgraphs from large global-state networks. To combat the exponential subgraph search space, we derive the concept of an edit map and perform Metropolis Hastings sampling on the map to compute the answer set. Furthermore, we formulate the idea of network-constrained decision trees to learn prediction models on a subgraph without compromising on the underlying network structure. Extensive experiments on real datasets demonstrate excellent accuracy in terms of prediction quality. Additionally, MINDS achieves a speed-up of at least two orders of magnitude over baseline techniques.", "authors": [{"affiliation": " IBM", "name": "Sayan Ranu"}, {"affiliation": " UC Santa Barbara", "name": " Minh Hoang"}, {"affiliation": " UC Santa Barbara", "name": " Ambuj Singh"}], "title": "Mining Discriminative Subgraphs from Global-state Networks"}, "761": {"abstract": "This paper investigates two types of graph queries: single source distance (SSD) queries and single source shortest path (SSSP) queries. Given a node v in a graph G, an SSD query from v asks for the distance from v to any other node in G, while an SSSP query retrieves the shortest path from v to any other node. These two types of queries are fundamental building blocks for numerous graph algorithms, and they find important applications in graph analysis, especially in the computation of graph measures. Most of the existing solutions for SSD and SSSP queries, however, require that the input graph fits in the main memory, which renders them inapplicable for the massive disk-resident graphs commonly used in web and social applications. There are several techniques that are designed to be I/O efficient, but they all focus on undirected and/or unweighted graphs, and they provide rather sub-optimal query efficiency.  To address the deficiency of existing work, this paper presents Highways-on-Disk (HoD), a disk-based index that supports both SSD and SSSP queries on directed and weighted graphs. The key idea of HoD is to augment the input graph with a set of auxiliary edges, and exploit them during query processing to reduce I/O and computation costs. We experimentally evaluate HoD on a variety of real-world graphs with up to billions of nodes and edges, and we demonstrate that HoD significantly outperforms alternative solutions in terms of query efficiency.", "authors": [{"affiliation": " Nanyang Technological University ", "name": "Xiaokui Xiao"}, {"affiliation": " Nanyang Technological University", "name": " Andy Diwen Zhu"}, {"affiliation": " Nanyang Technological University", "name": " Sibo Wang"}, {"affiliation": " Nanyang Technological University", "name": " Wenqing Lin"}], "title": "Efficient Single-Source Shortest Path and Distance Queries on Large Graphs"}, "765": {"abstract": "Graph clustering, often addressed as  community detection, is a prominent task in the domain of graph data mining with dozens of algorithms proposed in recent years. In this paper, we focus on several popular community detection algorithms with low computational complexity and with decent performance on the artificial benchmarks, and we study their behaviour on real-world networks. Motivated by the observation that there is a class of networks for which the community detection methods fail do deliver good community structure, we examine the assortativity coefficient of ground-truth communities and show that assortativity of a community structure can be very different from the assortativity of the original network.  We then examine the possibility of exploiting the latter by weighting edges of a network with the aim to improve the community detection outputs for networks with assortative community structure. The evaluation shows that the proposed weighting can significantly improve the results of community detection methods on networks with assortative community structure.", "authors": [{"affiliation": " IISAS", "name": "Marek Ciglan"}, {"affiliation": " NTNU", "name": " Kjetil Norvag"}, {"affiliation": " IISAS", "name": " Michal Laclavik"}], "title": "On Community Detection in Real-World Networks and the Importance of Degree Assortativity"}, "132": {"abstract": "Total variation (TV) regularization has important applications in signal processing including image denoising, image deblurring, and image reconstruction. A significant challenge in the practical use of TV regularization lies in the nondifferentiable convex optimization, which is difficult to solve especially for large-scale problems. In this paper, we propose an efficient alternating augmented Lagrangian method (ADMM) to solve total variation regularization problems. The proposed algorithm is applicable for tensors, thus it can solve multidimensional total variation regularization problems. One appealing feature of the proposed algorithm is that it does not need to solve a linear system of equations, which is often the most computationally expensive part in previous ADMM-based methods. In addition, each step of the proposed algorithm involves a set of independent and smaller problems, which can be solved in parallel. Thus, the proposed algorithm scales to large size problems. Furthermore, the global convergence of the proposed algorithm is guaranteed, and the time complexity of the proposed algorithm is $O(dN/\\epsilon)$ on a $d$-mode tensor with $N$ entries for achieving an $\\epsilon$-optimal solution. Extensive experimental results demonstrate the superior performance of the proposed algorithm in comparison with current state-of-the-art methods.", "authors": [{"affiliation": " Arizona State University", "name": "Sen Yang"}, {"affiliation": " Arizona State University", "name": " Jie Wang"}, {"affiliation": " IBM Research", "name": " Wei Fan"}, {"affiliation": " Huawei Technologies Co., Ltd", "name": " Xiatian Zhang"}, {"affiliation": " Arizona State University", "name": " Peter Wonka"}, {"affiliation": " Arizona State University", "name": " Jieping Ye"}], "title": "An Efficient ADMM Algorithm for Multidimensional Anisotropic Total Variation Regularization Problems"}, "131": {"abstract": "A Cyber-Physical System (CPS) integrates physical (i.e., sensor) devices with cyber (i.e., informational) components to form a context sensitive system that responds intelligently to dynamic changes in real-world situations. The CPS has wide applications in scenarios such as environment monitoring, battlefield surveillance and traffic control. One key research problem of CPS is called \u00d2mining lines in the sand\u00d3. With a large number of sensors (sand) deployed in a designated area, the CPS is required to discover all the trajectories (lines) of passing intruders in real time. There are two crucial challenges that need to be addressed: (1) the collected sensor data are not trustworthy; (2) the intruders do not send out any identification information, the system needs to distinguish multiple intruders and track their movements. In this study, we propose a method called LiSM (Line-in-the-Sand Miner) to discover the trajectories from untrustworthy sensor data. LiSM constructs a watching network from sensor data and computes the locations of intruder appearances based on the link information of the network. The system retrieves a cone-model from the historical trajectories and tracks multiple intruders based on this model. Finally the system validates the mining results and updates the sensor's reliability in a feedback process. Extensive experiments on both real and synthetic datasets demonstrate the feasibility and applicability of the proposed methods.", "authors": [{"affiliation": " UIUC", "name": "Lu-An Tang"}, {"affiliation": " University of Illinois at Urbana-Champaign", "name": " Xiao Yu"}, {"affiliation": " CS, UIUC", "name": " Quanquan Gu"}, {"affiliation": " UIUC", "name": " Jiawei Han"}, {"affiliation": " BBN", "name": " Alice Leung"}, {"affiliation": " PSU", "name": " Thomas La Porta"}], "title": "Mining Lines in the Sand: On Trajectory Discovery From Untrustworthy Data in Cyber-Physical System"}, "137": {"abstract": "Slow convergence and poor initial accuracy are two problems that plague efforts to use very large feature sets in learning. This is especially true when only a few features are ``active'' in any training example, and the frequency of activations of different features is skewed. We show how these problems can be mitigated if a graph of relationships between features is known. We study this problem in a fully Bayesian setting, focusing on the problem of using Facebook user-IDs as features, with the social network giving the relationship structure. Our analysis uncovers significant problems with the obvious regularizations, and motivates a two-component mixture-model ``social prior'' that is provably better. Empirical results on large-scale click prediction problems show that our algorithm can learn as well as the baseline with $12M$ fewer training examples, and continuously outperforms it for over $60M$ examples. On a second problem using binned features, our model outperforms the baseline even after the latter sees 5x as many training examples. ", "authors": [{"affiliation": " Facebook", "name": "Deepayan Chakrabarti"}, {"affiliation": "", "name": " Ralf Herbrich"}], "title": "Speeding up Large-Scale Learning with a Social Prior"}, "93": {"abstract": "Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this paper, we propose a flexible and robust framework, CGC (Co-regularized Graph Clustering), based on non-negative matrix factorization (NMF), to tackle these challenges.  CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. Extensive experimental results on UCI benchmark data sets, newsgroup data sets and biological interaction networks demonstrate the effectiveness of our approach.", "authors": [{"affiliation": " UNC at Chapel Hill", "name": "Wei Cheng"}, {"affiliation": " Case Western Reserve University", "name": " xiang Zhang"}, {"affiliation": " UNC at Chapel Hill", "name": " Patrick Sullivan"}, {"affiliation": " University of California, Los Angeles", "name": " Wei Wang"}], "title": "Flexible and Robust Co-regularized Multi-Domain Graph Clustering"}, "1076": {"abstract": "Online reviews have been popularly adopted in many applications.  Since they can either promote or harm the reputation of a product or a service, buying and selling fake reviews becomes a profitable business and a big threat. In this paper, we introduce a very simple, but powerful review spamming technique that could fail the existing feature-based detection algorithms easily. It uses one truthful review as a template, and replaces its sentences with those from other reviews in a repository. Fake reviews generated by this mechanism are extremely hard to detect: Both the state-of-the-art computational approaches and human readers acquire an error rate of 35%-48%, just slightly better than a random guess. While it is challenging to detect such fake reviews, we have made solid progress in suppressing them. A novel defense method that leverages the difference of semantic flows between synthetic and truthful reviews is developed, which is able to reduce the detection error rate to approximately 22%, a significant improvement over the performance of existing approaches. Nevertheless, it is still a challenging research task to further decrease the error rate.", "authors": [{"affiliation": " ", "name": "Alex Morales"}, {"affiliation": " UCSB", "name": " Huan Sun"}, {"affiliation": " University of California at Santa Barbara", "name": " Xifeng Yan"}], "title": "Synthetic Review Spamming and Defense"}, "211": {"abstract": "We consider the problem of adaptively routing a fleet of cooperative vehicles within a road network in the presence of uncertain and dynamic congestion conditions.  To tackle this problem, we first propose a Gaussian Process Dynamic Congestion Model that can effectively characterize both the dynamics and the uncertainty of congestion conditions.  Our model is efficient and thus facilitates real-time adaptive routing in the face of uncertainty.  Leveraging this congestion model, we develop an efficient algorithm for non-myopic adaptive routing to minimize the collective travel time of all vehicles in the system.  A key property of our approach is the ability to efficiently reason about the long-term value of exploration, which enables collectively balancing the exploration/exploitation trade-off for entire fleets of vehicles.  We validate our approach based on traffic data from two large Asian cities.  We show that our congestion model is  effective in modeling dynamic congestion conditions.  We further  show that our routing algorithm  generates significantly faster routes compared to standard baselines, and achieves near-optimal performance compared to an omniscient routing algorithm. We also present the results from a preliminary field study, which showcases the efficacy of our approach. ", "authors": [{"affiliation": " CMU", "name": "Siyuan Liu"}, {"affiliation": " Carnegie Mellon University", "name": " Yisong Yue"}, {"affiliation": " Carnegie Mellon University", "name": " Ramayya Krishnan"}], "title": "Adaptive Collective Routing Using Gaussian Process Dynamic Congestion Models"}, "543": {"abstract": "Analyzing functional interactions between small compounds and proteins is indispensable in genomic drug discovery. Since rich information on various compound-protein interactions is available in recent molecular databases, strong demands for making best use of such databases require to invent powerful methods to help us find new functional compound- protein pairs on a large scale. We present the succinct interval-splitting tree algorithm (SITA) that efficiently performs similarity search in databases for compound-protein pairs with respect to both binary fingerprints and real-valued properties. SITA achieves both time and space efficiency by developing the data structure called interval-splitting trees, which enables to efficiently prune the useless portions of search space, and by incorporating the ideas behind wavelet tree, a succinct data structure to compactly represent trees. We experimentally test SITA on the ability to retrieve similar compound-protein pairs/substrate-product pairs for a query from large databases with over 200 million compound- protein pairs/substrate-product pairs and show that SITA performs better than other possible approaches.", "authors": [{"affiliation": " JST", "name": "Yasuo Tabei"}, {"affiliation": " IBM Research, Dublin", "name": " Akihiro Kishimoto"}, {"affiliation": " Kyoto University", "name": " Masaaki Kotera"}, {"affiliation": " Kyushu university", "name": " Yoshihiro Yamanishi"}], "title": "Succinct Interval-Splitting Tree for Scalable Similarity Search of Compound-Protein Pairs with Property Constraints"}, "347": {"abstract": "State-of-the-art link prediction utilizes combinations of complex features derived from network panel data.  We here show that computationally less expensive features can achieve the same performance in the common scenario in which the data is available as a sequence of interactions.  Our features are based on social vector clocks, an adaptation of the vector-clock concept introduced in distributed computing to social interaction networks.  In fact, our experiments suggest that by taking into account the order and spacing of interactions, social vector clocks exploit different aspects of link formation so that their combination with previous approaches yields the most accurate predictor to date.", "authors": [{"affiliation": " University College Dublin", "name": "Conrad Lee"}, {"affiliation": " Konstanz Universitat", "name": " Bobo Nick"}, {"affiliation": " Konstanz Universitat", "name": " Ulrik Brandes"}, {"affiliation": " University College Dublin", "name": " Padraig Cunningham"}], "title": "Link Prediction with Social Vector Clocks"}, "546": {"abstract": "User review is a crucial component of open mobile app markets such as the Google Play Store. How do we summarize hundreds or thousands of user reviews and identify the reasons of their likes/dislikes? Unfortunately, beyond simple summaries such as histograms of user ratings, there are few analytic tools that can provide insights into user reviews. In this paper, we propose WisCom, a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail. Our system is able to (a) discover inconsistencies in reviews; (b) identify reasons why users like or dislike a given app, and provide an interactive, zoomable view of how users' reviews evolve over time; and (c) provide valuable insights into the entire app market, identifying major user concerns and preferences of different types of apps. Results using our techniques are reported for a large dataset of 32 GB consisting of over 13 million user reviews of a total 171 thousand Android apps in the Google Play Store. We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google Play as well as individual app developers and end-users.", "authors": [{"affiliation": " Carnegie Mellon University", "name": "Bin Fu"}, {"affiliation": " Carnegie Mellon University", "name": " Jialiu Lin"}, {"affiliation": " UC Berkeley", "name": " Lei Li"}, {"affiliation": " CMU", "name": " Christos Faloutsos"}, {"affiliation": " Carnegie Mellon University", "name": " Jason Hong"}, {"affiliation": " Carnegie Mellon University", "name": " Norman Sadeh-Koniecpol"}], "title": "Why People Hate Your App -- Making Sense of User Feedback in a Mobile App Store"}, "341": {"abstract": "When analyzing data that originated from a dynamical system, a common practice is to encompass the problem in the well known frameworks of Markov Decision Processes (MDPs) and Reinforcement Learning (RL). The state space in these solutions is usually chosen in some heuristic fashion and the formed MDP can then be used to simulate and predict data, as well as indicate the best possible action in each state. The model chosen to characterize the data affects the complexity and accuracy of any further action we may wish to apply, yet few methods that rely on the dynamic structure to select such a model were suggested.  In this work we address the problem of how to use time series data to choose from a finite set of candidate discrete state spaces, where these spaces are constructed by a domain expert. We formalize the notion of model selection consistency in the proposed setup. We then discuss the difference between our proposed framework and the classical Maximum Likelihood (ML) framework, and give an example where ML fails. Afterwards, we suggest alternative selection criteria and show them to be weakly consistent. We then define weak consistency for a model construction algorithm and show a simple algorithm that is weakly consistent. Finally, we test the performance of the suggested criteria and algorithm on both simulated and real world data.", "authors": [{"affiliation": " The Technion", "name": "Assaf Hallak"}, {"affiliation": " Technion", "name": " Dotan Di-Castro"}, {"affiliation": " Technion", "name": " Shie Mannor"}], "title": "Model Selection in Markovian Processes"}, "281": {"abstract": "Suicide is a major concern in society. Despite of great attention paid by the community with very substantive medico-legal implications, there has been no satisfying method that can reliably predict the future attempted or completed suicide. We present an integrated machine learning framework to tackle this challenge. Our proposed framework consists of a novel feature extraction scheme, an embedded feature selection process, a set of risk classifiers and finally, a risk calibration procedure.. For temporal feature extraction, we cast the patient's clinical history into a temporal image to which a bank of one-side filters are applied. The responses are then partly transformed into mid-level features and then selected in \\ell_{1}+\\ell_{2}-norm framework under the extreme value theory. A set of probabilistic ordinal risk classifiers are then applied to compute the risk probabilities and further re-rank the features. Finally, the predicted risks are calibrated. Together with our Australian partner, we perform comprehensive study on data collected for the mental health cohort, and the experiments validate that our proposed framework outperforms risk assessment instruments by medical practitioners.", "authors": [{"affiliation": " Deakin University", "name": "Truyen Tran"}, {"affiliation": " Deakin University", "name": " Dinh Phung"}, {"affiliation": " Deakin University", "name": " Svetha Venkatesh"}, {"affiliation": " Deakin University", "name": " Wei Luo"}, {"affiliation": " Barwon Health", "name": " Richard Harvey"}, {"affiliation": " Deakin University", "name": " Michael Berk"}], "title": "An Integrated Framework for Suicide Risk Prediction"}, "811": {"abstract": "Quantitative notions of diversity are central to scientific disciplines ranging from conservation biology to the reflexive study of science. However, there has been relatively little work on measuring the diversity of text documents via their content. In this paper we present a text-based framework for quantifying the diversity of documents. We learn a topic model over a corpus of documents, and compute a distance matrix between pairs of topics based on measures such as topic co-occurrence. We then use these pairwise distance measures, combined with the distribution of topics in each document, to estimate each document's diversity relative to the rest of the corpus. Our approach provides several advantages over existing methods.  It is fully data-driven, requiring only the text from a corpus of documents as input, it produces human-readable explanations, and it can be generalized to score diversity of other entities such as authors, academic departments, or journals.  We describe experimental results on several large data sets which suggest that our approach does accurately capture document diversity.", "authors": [{"affiliation": " University of California, Irvine", "name": "Kevin Bache"}, {"affiliation": " UC Irvine", "name": " Padhraic Smyth"}, {"affiliation": " University of California, Irvine", "name": " David Newman"}], "title": "Text-Based Measures of Document Diversity"}, "629": {"abstract": "Online social networks are becoming increasingly popular. Modeling the dynamics of these networks over time not only helps us understand the evolution of network structures and user behaviors, but also improves the performance of other analysis tasks, such as link prediction and community detection. Nowadays, users engage in multiple networks and the common users serve as bridges to connect different social networks and form a ``composite social network''. State-of-the-art network dynamics analysis is performed in isolation for individual networks, but users' interactions in one network can influence their behaviors in other networks, and in an individual network, different types of user interactions also affect each other. Without considering the influences across networks, one may not be able to model the dynamics in a given network correctly due to the lack of information. In this paper, we study the problem of modeling the dynamics of composite networks, where the evolution processes of different networks are jointly considered. However, due to the difference in network properties, simply merging multiple networks into a single one is not ideal because individual evolution patterns may be ignored and network differences may bring negative impacts. The proposed solution is a nonparametric Bayesian model, which models each user's common latent features to extract the cross-network influences, and use network-specific factors to describe different networks' evolution patterns. Empirical studies on large-scale dynamic composite social networks demonstrate that the proposed approach improves the performance of link prediction over several state-of-the-art baselines significantly and unfolds the network evolution accurately.", "authors": [{"affiliation": " HKUST", "name": "Erheng Zhong"}, {"affiliation": " IBM Research", "name": " Wei Fan"}, {"affiliation": " ", "name": " Yin Zhu"}, {"affiliation": " Hong Kong University of Science and Technology", "name": " Qiang Yang"}], "title": "Modeling the Dynamics of Composite Social Networks"}, "409": {"abstract": "Arguably, the most effective technique to ensure wide adoption of a concept (or product) is by repeatedly exposing individuals to messages that reinforce the concept (or promote the product).  Recognizing the role of repeated exposure to a message, in this paper we propose and study a novel framework for the effective placement of content: Given the navigational patterns of users in a network, e.g., web graph, hyperlinked corpus, or road network, and given a model of the relationship between content-adoption and frequency of exposition, we define the repetition-aware content-placement (RACP) problem as that of identifying the set of B nodes on which content should be placed so that the expected number of users adopting that content (conversion rate) is maximized. The key contribution of our work is the introduction of memory into the navigation process, by making user conversion dependent on the number of her exposures to that content. This dependency is captured using a conversion model that is general enough to capture arbitrary dependencies. Our solution to this general problem builds upon the notion of absorbing random walks, which we extend appropriately in order to address the technicalities of our definitions. Although we show the RACP problem to be NP-hard, we propose a general and efficient algorithmic solution. We present experimental results which demonstrate the scalability of our approach and the usefulness of our framework by considering real-world datasets, including Web-graph data, road networks, and hyperlinked corpora from bibliographic datasets. ", "authors": [{"affiliation": " Boston University", "name": "Dora Erdos"}, {"affiliation": " Boston University", "name": " Vatche Ishakian"}, {"affiliation": " Boston University", "name": " Evimaria Terzi"}, {"affiliation": " Boston University", "name": " Azer Bestavros"}], "title": "Repetition-Aware Content Placement in Navigational Networks"}, "408": {"abstract": "Identifying genetic variation underlying a complex disease is important. Many complex diseases have heterogeneous phenotypes and are products of a variety of genetic and environmental factors acting in concert. Deriving highly heritable quantitative traits of a complex disease can improve the identification of genetic risk of the disease. The most sophisticated methods so far perform unsupervised cluster analysis on phenotypic features; and then a quantitative trait is derived based on each resultant cluster. Heritability is estimated to assess the validity of the derived quantitative traits. However, none of these methods explicitly maximize the heritability of the derived traits. We propose a quadratic optimization approach that directly utilizes heritability as an objective during the derivation of quantitative traits of a disease. This method maximizes an objective function that is formulated by decomposing the traditional maximum likelihood method for estimating heritability of a quantitative trait. We demonstrate the effectiveness of the proposed method in finding a trait with high heritability on synthetic data. We also apply our algorithm to identify highly heritable traits of complex human behavior disorders, such as opioid use disorder and cocaine use disorders. Our approach outperforms standard cluster analysis methods to identify highly heritable traits of opioid use and cocaine use.", "authors": [{"affiliation": " University of Connecticut", "name": "Jiangwen Sun"}, {"affiliation": " University of Connecticut", "name": " Jinbo Bi"}, {"affiliation": " University of Pennsylvania", "name": " Henry Kranzler"}], "title": "Quadratic Optimization to Identify Highly Heritable Quantitative Traits from Complex Disease Features"}, "1097": {"abstract": "When information is abundant, users need support to understand complex stories, such as presidential elections.  We propose a methodology for creating structured summaries of information, which we call metro maps. Just as cartographic maps have been relied upon for centuries to help us understand our surroundings, metro maps can help us understand the relationships between pieces of information.  Our proposed algorithm generates a concise structured set of documents that explicitly  captures story development. As different users might be interested in different granularities, the maps are zoomable, with each level of zoom showing finer details and interactions.  In this work we formalize characteristics of good maps and formulate their construction as an optimization problem. We provide efficient, scalable methods with theoretical guarantees for generating maps. Pilot user studies over real-world datasets demonstrate that the method is able to produce maps which help users acquire knowledge efficiently.      ", "authors": [{"affiliation": " Stanford", "name": "Dafna Shahaf"}, {"affiliation": " Stanford University", "name": " Jaewon Yang"}, {"affiliation": " ", "name": " Caroline Suen"}, {"affiliation": " ", "name": " Jeff Jacobs"}, {"affiliation": " ", "name": " Heidi Wang"}, {"affiliation": " Stanford University", "name": " Jure Leskovec"}], "title": "Information Cartography: Creating Zoomable, Large-Scale Maps of Information"}, "1024": {"abstract": "With the explosive growth of social networks, many applications are increasingly harnessing the pulse of online crowds for a variety of tasks such as marketing, advertising, and opinion mining. An important example is the wisdom of crowd effect that has been well studied for such tasks when the crowd is non-interacting. However, these studies don't explicitly address the network effects in social networks. A key difference in this setting is the presence of social influences that arise from these interactions and undermine the wisdom of the crowd~\\cite{LRS+11}.    Using a natural model of opinion formation, we analyze the effect of these interactions on an individual's opinion and estimate her propensity to conform. We then propose efficient sampling algorithms incorporating these conformity values to arrive at a debiased estimate of the wisdom of a crowd. We analyze the trade-off between the sample size and estimation error and validate our algorithms using both real data obtained from online user experiments and synthetic data.", "authors": [{"affiliation": " Microsoft", "name": "Abhimanyu Das"}, {"affiliation": " Microsoft Research", "name": " Sreenivas Gollapudi"}, {"affiliation": " Microsoft Research", "name": " Rina Panigrahy"}, {"affiliation": " Microsoft", "name": " Mahyar Salek"}], "title": "Debiasing Social Wisdom"}, "454": {"abstract": "Composed of several hundreds of processors, the Graphics Processing Unit(GPU) has become a very interesting platform for computationally demanding tasks on massive data. A special hierarchy of processors and fast memory units allows very powerful and efficient parallelization but also demands novel parallel algorithms. Expectation Maximization (EM) is a widely used technique for maximum likelihood estimation. In this paper, we propose an innovative EM clustering algorithm particularly suited for the GPU platform on NVIDIA's Fermi architecture. The central idea of our algorithm is to allow the parallel threads exchanging their local information in an asynchronous way and thus updating their cluster representatives on demand by a technique called Asynchronous Model Updates (Async-EM). Async-EM enables our algorithm not only to accelerate convergence but also to reduce the overhead induced by memory bandwidth limitations and synchronization requirements. We demonstrate (1) how to reformulate the EM algorithm to be able to exchange information using Async-EM and (2) how to exploit the special memory and processor architecture of a modern GPU in order to share this information among threads in an optimal way. As a perspective Async-EM is not limited to EM but can be applied to a variety of algorithms.", "authors": [{"affiliation": " University of Munich", "name": "Muzaffer Can Altinigneli"}, {"affiliation": " Helmholtz Center Munich", "name": " Claudia Plant"}, {"affiliation": " University of Munich", "name": " Christian Bohm"}], "title": "Massively Parallel Expectation Maximization Using Graphics Processing Units"}, "120": {"abstract": "Learning of the information diffusion model is a fundamental problem in the study of information diffusion in social networks. Existing approaches learn the diffusion models from events in social networks. However, events in social networks may have different underlying reasons. Some of them may be caused by the social influence inside the network, while others may reflect external trends in the \"real world\". Most existing work on the learning of diffusion models does not distinguish the events caused by the social influence from those caused by external trends.     In this paper, we extract social events from the data stream in social networks, and then use the extracted social events to improve the learning of information diffusion models. We propose a LADP (Latent Action Diffusion Path) model to incorporate the information diffusion model with the model of external trends, and then design an EM-based algorithm to infer the diffusion probabilities, the external trends and the sources of events efficiently ", "authors": [{"affiliation": " UIC", "name": "Shuyang Lin"}, {"affiliation": " University of Illinois at Chic", "name": " Fengjiao Wang"}, {"affiliation": " University of Illinois at Chicago", "name": " Qingbo Hu"}, {"affiliation": " University of Illinois at Chicago", "name": " Philip Yu"}], "title": "Extracting Social Events for Learning Better Information Diffusion Models"}, "261": {"abstract": "Active search is an increasingly important learning problem in which we use a limited budget of label queries to discover as many members of a certain class as possible.  Numerous real-world applications may be approached in this manner, including fraud detection, product recommendation, and drug discovery.  Active search has model learning and exploration/exploitation features similar to those encountered in active learning and bandit problems, but algorithms for those problems do not fit active search.  Previous work on the active search problem \\cite{garnett12} showed that the optimal algorithm requires a lookahead evaluation of expected utility that is exponential in the number of selections to be made and proposed a truncated lookahead heuristic.  Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs.  We suggest selecting points by maximizing a score considering the potential impact of selecting a node, meant to emulate lookahead while avoiding exponential search.  We test the proposed algorithm empirically on real-world graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps.", "authors": [{"affiliation": " Carnegie Mellon University", "name": "Xuezhi Wang"}, {"affiliation": " ", "name": " Jeff Schneider"}, {"affiliation": "", "name": " Roman Garnett"}], "title": "Active Search on Graphs"}, "269": {"abstract": "Nonnegative matrix factorization (NMF) has been successfully used as a clustering method especially for flat partitioning of documents. In this paper, we propose an efficient hierarchical document clustering method based on a new algorithm for rank-2 NMF. When the two block coordinate descent framework is applied to rank-2 NMF, each subproblem requires a solution for nonnegative least squares (NNLS) with only two columns. We design the algorithm for rank-2 NMF by exploiting the fact that an exhaustive search for the optimal active set can be performed extremely fast when solving these NNLS problems. In addition, we design a measure on the results of rank-2 NMF for determining which leaf node should be further split. On a number of text data sets, our proposed method produces high-quality tree structures in significantly less time compared to other methods such as hierarchical K-means, standard NMF, and latent Dirichlet allocation.", "authors": [{"affiliation": " Georgia Institute of Technology", "name": "Da Kuang"}, {"affiliation": " Georgia Institute of Technology", "name": " Haesun Park"}], "title": "Fast Rank-2 Nonnegative Matrix Factorization for Hierarchical Document Clustering"}, "425": {"abstract": "Many people share their activities with others through online communities. These shared activities have an impact on other users' activities. For example, users are likely to become interested in items that are adopted (e.g. liked, bought and shared) by their friends. In this paper, we propose a probabilistic model for discovering latent influence from sequences of item adoption events. An inhomogeneous Poisson process is used for modeling a sequence, in which adoption by a user triggers the subsequent adoption of the same item by other users. For modeling adoption of multiple items, we employ multiple inhomogeneous Poisson processes, which share parameters, such as influence for each user and relations between users. The proposed model can be used for finding influential users, discovering relations between users and predicting item popularity in the future. We present an efficient Bayesian inference procedure of the proposed model based on the stochastic EM algorithm. The effectiveness of the proposed model is demonstrated by using real data sets in a social bookmark sharing service.", "authors": [{"affiliation": " NTT Communication Science Laboratories", "name": "Tomoharu Iwata"}, {"affiliation": " University of Cambridge", "name": " Amar Shah"}, {"affiliation": " Cambridge University", "name": " Zoubin Ghahramani"}], "title": "Discovering Latent Influence in Online Social Activities via Shared Cascade Poisson Processes"}, "51": {"abstract": "Information about urban air quality, e.g., the concentration of NO2 and PM10, is of great importance to protect human health and control air pollution. While there are limited air-quality-monitor-stations in a city, air quality varies in urban spaces non-linearly, affected by multiple factors, such as meteorology, traffic volume, and land uses. In this paper, we infer the real-time and fine-granularity air quality information throughout a city, based on the (historical and real-time) air quality data reported by existing monitor stations and a variety of data sources we observed in the city, such as meteorology, traffic flow, human mobility, structure of road networks, and point of interests (POIs). To deal with the sparsity of labeled data (i.e., only a few stations), we propose a semi-supervised learning approach based on a co-training framework incorporating two separated classifiers. One is a spatial classifier based on an artificial neural network (ANN), which takes spatially-related features (e.g., the density of POIs and length of highways extracted from the observed data) as input to model the spatial correlation between air qualities of different locations. The other is a temporal classifier based on a linear-chain conditional random field (CRF) involving temporally-related features (e.g., traffic and meteorology) to model the temporal dependency of air quality in a location. We evaluated our approach with extensive experiments based on five real data sources obtained in two cities, Beijing and Shanghai. The results show the advantages of our method over four baselines, including linear interpolation, classical dispersion models, and well-known classification models like decision tree, CRF, and ANN.", "authors": [{"affiliation": " Microsoft", "name": "Yu Zheng"}, {"affiliation": " Microsoft Research Asia", "name": " Furui Liu"}, {"affiliation": " Microsoft Research Asia", "name": " Hsun-Ping Hsieh"}], "title": "U-Air: When Urban Air Quality Inference Meets Big Data"}, "52": {"abstract": "Graphs are used to model many real objects such as social networks, web graphs, chemical compounds,  and biological structures. Many real applications in various fields require efficient and effective management of large-scale graph structured data. Although distributed graph engines such as \\GBase and \\Pregel handle billion-scale graphs, the user must be skilled at managing and tuning a distributed system in a cluster, which is a nontrivial job for the ordinary user. Furthermore, these distributed systems need many machines in a cluster in order to provide reasonable performance. In order to address this problem, a disk-based parallel graph engine called \\GraphChi, has been recently proposed. Although \\GraphChi significantly outperforms all representative (disk-based) distributed graph engines, we observe that \\GraphChi still has serious performance problems for many important types of graph queries due to 1) limited parallelism and 2) separate steps for I/O processing and CPU processing. In this paper, we propose a general, disk-based graph engine called \\TurboGraph\\footnote {https://sites.google.com/site/turbographdb/} to process billion-scale graphs very efficiently by using modern hardware on a single PC. \\TurboGraph is the first truly parallel graph engine that exploits 1) \\emph{full parallelism} including multi-core parallelism and FlashSSD IO parallelism and 2) \\emph{full overlap} of CPU processing and I/O processing as much as possible. Specifically, we propose a novel parallel execution model, called \\PinSlide. \\TurboGraph also provides engine-level operators such as BFS which are implemented under the pin-and-slide model. Extensive experimental results with large real datasets show that \\TurboGraph consistently and significantly outperforms \\GraphChi by up to four orders of magnitude.", "authors": [{"affiliation": " POSTECH", "name": "Wook-Shin Han"}, {"affiliation": " POSTECH", "name": " Sangyeon Lee"}, {"affiliation": " POSTECH", "name": " Kyungyeol Park"}, {"affiliation": " POSTECH", "name": " Jeong-Hoon Lee"}, {"affiliation": " DGIST", "name": " Min-Soo Kim"}, {"affiliation": " POSTECH", "name": " Jinha Kim"}, {"affiliation": " POSTECH", "name": " Hwanjo Yu"}], "title": "TurboGraph: A Fast Parallel Graph Engine Handling Billion-scale Graphs in a Single PC"}, "928": {"abstract": "Spontaneous devaluation in preferences is ubiquitous, where yesterday's hit is today's affliction. Devaluation in preferences or boredom can be temporary such that one's perceived liking for objects are temporarily reordered to devalue familiarity or can be permanent causing a complete shift in one's interests. One of the psychological theories explaining spontaneous devaluation relates to stimulus satiation arising on repeated exposure causing disinterest in the activity. Despite technological advances facilitating access to a wide range of commodities, finding engaging content is a major enterprise. Systems tracking spontaneous devaluation in user preferences can allow prediction of the onset of boredom in users potentially catering to their changed needs. In this work, we study the music listening histories of Last.fm users focusing on the changes in their preferences based on their choices for different artists at different points in time. A hazard function, commonly used in statistics for survival analysis, is used to capture the rate at which a user returns to an artist as a function of exposure to the artist. The analysis provides the first evidence of spontaneous devaluation in preferences of music listeners. Better understanding of the temporal dynamics of this phenomenon can inform solutions to similarity-dissimilarity dilemma in recommender systems. ", "authors": [{"affiliation": " University of Minnesota Twin C", "name": "Komal Kapoor"}, {"affiliation": " ", "name": " Nisheeth Srivastava"}, {"affiliation": " University of Minnesota", "name": " Jaideep Srivastava"}, {"affiliation": " University of Minnesota Twin Cities", "name": " Paul Schrater"}], "title": "Measuring spontaneous devaluations in user preferences"}, "416": {"abstract": "Today's popular web search engines expand the search process beyond crawled web pages to specialized corpora (``verticals'') like images, videos, news, local, sports, finance, and shopping etc., each with its own specialized search engine. Search federation deals with problems of the selection of search engines to query and merging of their results into a single result set. Despite a few recent advances, the problem is still very challenging. First, due to the heterogeneous nature of different verticals, how the system merges the vertical results with the web documents to serve the user's information need is still an open problem. Moreover, the scale of the search engine and the increasing number of vertical properties requires a solution which is efficient and scaleable.       In this paper, we propose an unified framework for the search federation problem. We model the search federation as a contextual bandit problem. The system uses reward as a proxy for user satisfaction. Given a query, our system predicts the expected reward for each vertical, then organizes the search result page (SERP) in a way which maximizes the total reward. Instead of relying on human judges, our system leverages implicit user feedback to learn the model. The method is efficient to implement and can be applied to verticals of different nature. We have successfully deployed the system to three different markets, and it handles multiple verticals in each market. The system is now serving hundreds of millions of queries live each day, and has improved user metrics considerably.", "authors": [{"affiliation": " Yahoo! Labs", "name": "Jie Luo"}, {"affiliation": " Yahoo! Labs", "name": " Sudarshan Lamkhede"}], "title": "A Unified Search Federation System Based on Online User Feedback"}, "294": {"abstract": "Path prediction is very useful in a wide range of applications. Most of existing solutions, however, are based on eager learning methods where models and patterns are extracted from historical trajectories and used for future prediction. Since such approaches are already committed to a statistically significant sets of models or patterns, problems can arise in dynamic environments where the underlying models change quickly or in regions which are not covered by statistically significant models or patterns.  We propose a \u00d2semi-lazy\u00d3 approach to path prediction. This approach has several advantages. First, the target trajectory to be predicted is first known before the model/pattern is derived, compared to finding models/patterns in a pre-processing stage which might not be relevant for target trajectories provided later. Second, we can use slightly more sophisticated learning algorithms to derive more accurate local models/patterns without unacceptable delay. Finally, we can dynamically construct new models/patterns if the actual and predicted movements do not match, giving rise to self-correcting continuous prediction.  Instead of trying to predict the most accurate location, our model gives a probabilistically predicted path, whose probability is larger than a threshold and whose time interval is the longest. Users can control the confidence of the predicted path by setting a probability threshold. We conducted a comprehensive experimental study on two real-world datasets and four semi-real datasets to show the effectiveness and efficiency of our model. Experimental results show that our model significantly outperforms competitors by 2 to 5-fold.", "authors": [{"affiliation": " NUS", "name": "Jingbo Zhou"}, {"affiliation": " SoC,NUS", "name": " Anthony TUNG "}, {"affiliation": " I2R, Astar", "name": " Wei Wu"}, {"affiliation": " I2R, Astar", "name": " Wee Siong Ng"}], "title": "Probabilistic Path Prediction in Dynamic Environments"}, "295": {"abstract": "Networked data, extracted from social media, web pages, and bibliographic databases, can contain entities of multiple classes, interconnected through different types of links. In this paper, we focus on the problem of performing multi-label classification on networked data, where the instances in the network can be assigned multiple labels. In contrast to traditional content-only classification methods, relational learning succeeds in improving classification performance by leveraging the correlation of the labels between linked instances. However, instances in a network can be linked for various causal reasons, hence treating all links in a homogeneous way can limit the performance of relational classifiers.  In this paper, we propose a multi-label iterative relational neighbor classifier that employs social context features (SCRN). Our classifier incorporates a class propagation probability distribution obtained from instances\u00d5 social features, which are in turn extracted from the network topology. This class-propagation probability captures the node\u00d5s intrinsic likelihood of belonging to each class, and serves as a prior weight for each class when aggregating the neighbors\u00d5 class labels in the collective inference procedure. Experiments on different real-world datasets demonstrate that our proposed classifier can boost classification performance over several commonly used benchmarks on networked multi-label data.", "authors": [{"affiliation": " University of Central Florida", "name": "Xi Wang"}, {"affiliation": " University of Central Florida", "name": " Gita Sukthankar"}], "title": "Multi-Label Relational Neighbor Classification using Social Context Features"}, "291": {"abstract": "Every day millions of users are connected through online social networks, generating a rich trove of data that allows us to study the mechanisms behind human interactions. Triadic closure has been treated as the major mechanism for creating social links: if Alice follows Bob and Bob follows Charlie, Alice will follow Charlie. Here we present an analysis of longitudinal micro-blogging data, revealing a more nuanced view of the strategies employed by users when expanding their social circles. While the network structure affects the spread of information among users, the network is in turn shaped by this communication activity. This suggests a link creation mechanism whereby Alice is more likely to follow Charlie after seeing many messages by Charlie. We characterize users with a set of parameters associated with different link creation strategies, estimated by a Maximum-Likelihood approach. Triadic closure does have a strong effect on link formation, but shortcuts based on traffic are another key factor in interpreting network evolution. However, individual strategies for following other users are highly heterogeneous. Link creation behaviors can be summarized by classifying users in different categories with distinct structural and behavioral characteristics. Users who are popular, active, and influential tend to create traffic-based shortcuts, making the information diffusion process more efficient in the network. ", "authors": [{"affiliation": " Indiana University", "name": "Lilian Weng"}, {"affiliation": " Google Inc.", "name": " Jacob Ratkiewicz"}, {"affiliation": " Northeastern University", "name": " Nicola Perra"}, {"affiliation": " Aix-Marseille Universite", "name": " Bruno Goncalves"}, {"affiliation": " Qatar Computing Research Institute", "name": " Carlos Castillo"}, {"affiliation": " Yahoo! Research", "name": " Francesco Bonchi"}, {"affiliation": " Universita degli Studi di Torino, Italy", "name": " Rossano Schifanella"}, {"affiliation": " Indiana University", "name": " Filippo Menczer"}, {"affiliation": " Indiana University", "name": " Alessandro Flammini"}], "title": "The Role of Information Diffusion in the Evolution of Social Networks"}, "590": {"abstract": "Road surface skid resistance has been shown to have a strong relationship to road crash risk; however applying the current method of investigatory levels to identify crash prone roads is problematic as they fail in identifying risky roads outside of the norm. The proposed method analyses a complex and formerly impenetrable volume of data from roads and crashes using data mining, providing a new tool for road asset management. This method allows identification of roads with elevated crash rate due to a skid resistance deficit. A regression tree algorithm was used to develop a model in the complex, multi class road crash data environment.  A hypothetical skid resistance/crash risk curve was developed for each road segment, driven by the resulting model deployed in a novel regression tree extrapolation method. The method solves the problem of missing values which occurs during  network-wide crash analysis, and allows risk assessment of the major proportion of roads that are without skid resistance values. ", "authors": [{"affiliation": " Queensland University of Technology", "name": "Daniel Emerson"}, {"affiliation": " Queensland University of Technology", "name": " Richi Nayak"}, {"affiliation": " Road asset management consultant", "name": " Justin Weligamage"}], "title": "A data mining driven risk profiling method for road asset management"}, "1083": {"abstract": "Recent research efforts have made notable progress in improving the performance of (exhaustive) maximal clique enumeration (MCE). However, exiting algorithms still suffer from exploring the huge search space of MCE. Furthermore, their results are often undesirable as many of the returned maximal cliques have large overlapping parts. This redundancy leads to problems in both computational efficiency and usefulness of MCE.  In this paper, we aim at providing a concise and complete summarization of the set of maximal cliques, which is useful to many applications. We propose the notion of \\tau-visible MCE to achieve this goal and design algorithms to realize the notion. Based on the refined output space, we further consider an efficient computation of the top-$k$ results with guaranteed quality and diversity. Our experimental results demonstrate that our approach is capable of producing output of high usability and our algorithms achieve superior efficiency over classic MCE algorithms.", "authors": [{"affiliation": " Chinese University of Hong Kong", "name": "Jia Wang"}, {"affiliation": " Chinese University of Hong Kong", "name": " James Cheng"}, {"affiliation": " Chinese University of Hong Kong", "name": " Ada Wai-Chee Fu"}], "title": "Redundancy-Aware Maximal Cliques"}, "198": {"abstract": "From Twitter to Facebook to Reddit, users have become accustomed to sharing the articles they read with friends or followers on their social networks. While previous work has modeled what these shared stories say about the user who shares them, the converse question remains unexplored: what can we learn about an article from the identities of its likely readers?   To address this question, we model the content of news articles and blog posts by attributes of the people who are likely to share them. For example, many Twitter users describe themselves in a short profile, labeling themselves with phrases such as \"vegetarian\" or \"liberal.\" By assuming that a user's labels correspond to topics in the articles he shares, we can learn a labeled dictionary from a training corpus of articles shared on Twitter. Thereafter, we can code any new document as a sparse non-negative linear combination of user labels, where we encourage correlated labels to appear together in the output via a structured sparsity penalty.   Finally, we show that our approach yields a novel document representation that can be effectively used in many problem settings, from recommendation to modeling news dynamics. For example, while the top politics stories will change drastically from one month to the next, the \"politics\" label will still be there to describe them. We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012, demonstrating that our approach is effective.", "authors": [{"affiliation": " Carnegie Mellon University", "name": "Khalid El-Arini"}, {"affiliation": " Carnegie Mellon University", "name": " Min Xu"}, {"affiliation": " University of Washington", "name": " Emily Fox"}, {"affiliation": " University of Washington", "name": " Carlos Guestrin"}], "title": "Representing Documents Through Their Readers"}, "596": {"abstract": "Malicious Uniform Resource Locator (URL) detection is an important problem in web search and mining, which plays a critical role in network security. In literature, most existing studies have attempted to formulate the problem as a regular supervised classification task which typically aims to optimize the prediction accuracy. However, in a real-world malicious URL detection task, the ratio between the number of malicious URLs and legitimate URLs is highly imbalanced, making it very inappropriate for simply optimizing the prediction accuracy. Besides, another key limitation of the existing work is to assume a large amount of training data is available, which is impractical as the human labeling cost could be potentially quite expensive. To overcome these limitations, in this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL), which only queries a small fraction of training data for labeling and directly optimizes two cost-sensitive measures to address the class-imbalance issue. In particular, we propose two CSOAL algorithms and analyze their theoretical performance in terms of cost-sensitive bounds. We conduct an extensive set of experiments to examine the empirical performance of the proposed algorithms for a large-scale challenging malicious URL detection task, in which the encouraging results showed that the proposed technique by querying an extremely small-sized labeled data (about 0.5\\% out of 1-million instances) can achieve better or highly comparable classification performance in comparison to the state-of-the-art regular and cost-sensitive online classification algorithms using a huge amount of labeled data. ", "authors": [{"affiliation": " Nanyang Technological University", "name": "Peilin ZHAO"}, {"affiliation": " Nanyang Technological University, Singapore.", "name": " Steven Hoi"}], "title": "Cost-Sensitive Online Active Learning with Application to Malicious URL Detection"}, "194": {"abstract": "Worker quality control is a crucial aspect of crowdsourcing systems; typically occupying a large fraction of the time and money invested on crowdsourcing. In this work, we devise techniques to generate confidence intervals for worker error rate estimates, thereby enabling a better evaluation of worker quality. We show that our techniques generate correct confidence intervals on a range of real-world datasets, and demonstrate wide applicability by using them to evict poorly performing workers, and provide confidence intervals on the accuracy of the answers.", "authors": [{"affiliation": " Stanford University", "name": "Manas Joglekar"}, {"affiliation": " Stanford University", "name": " Hector Garcia-Molina"}, {"affiliation": " Stanford University", "name": " Aditya Parameswaran"}], "title": "Evaluating the Crowd with Confidence"}, "980": {"abstract": "New challenges have been presented to classical topic models when applied to social media, as user-generated content suffers from significant problems of data sparseness. A variety of heuristic adjustments to these models have been proposed, with many based on the use of context information to improve the performance of topic modeling. Existing contextualized topic models rely on arbitrary manipulation of the model structure, by incorporating various context variables into the generative process of classical topic models in an ad hoc manner. Such manipulations usually result in much more sophisticated model structures and inference procedures, and substantial difficulty to generalize any of them to accommodate arbitrary types or combinations of contexts. In this paper we explore a different direction. We propose a general solution that is able to exploit multiple types of contexts without arbitrary manipulation of the structure of classical topic models. We formulate different types of contexts as multiple views of the partition of the corpus. A co-regularization framework is proposed to let the views collaborate with each other, vote for the consensus topics, and distinguish them from view-specific topics. Experiments with real world datasets prove that the proposed method is both effective and flexible to utilize arbitrary types of contexts.", "authors": [{"affiliation": " Peking University", "name": "Jian Tang"}, {"affiliation": " ", "name": " Ming Zhang"}, {"affiliation": " University of Michigan", "name": " Qiaozhu Mei"}], "title": "One Theme in All Views: Modeling Consensus Topics in Multiple Contexts"}, "707": {"abstract": "Influence-driven diffusion of information is a fundamental process in social networks. Learning the latent variables of such process, i.e., the influence strength along each link, is a central question towards understanding the structure and function of complex networks, modeling information cascades, and developing applications such as viral marketing.  Motivated by modern microblogging platforms, such as twitter, in this paper we study the problem of learning influence probabilities in a data-stream scenario, in which the network topology is relatively stable and the challenge of a learning algorithm is to keep up with a continuous stream of tweets. Our contribution is a number of randomized approximation algorithms, categorized according to the available space (superlinear, linear, and sublinear in the number of nodes $n$) and according to different models (landmark and sliding window). Among several results, we show that we can learn influence probabilities with one pass over the data, using O(n log n) space, in both the landmark model and the sliding-window model, showing that our algorithm is within a logarithmic factor of optimal.  For truly large graphs, when one needs to operate with sublinear space, we show that we can still learning influence probabilities in one pass, assuming that we restrict our attention to the most active users.  Our thorough experimental evaluation on large social graph demonstrates that the empirical performance of our algorithms agrees with that predicted by the theory.", "authors": [{"affiliation": " University of Copenhagen", "name": "Konstantin Kutzkov"}, {"affiliation": " Yahoo! Research", "name": " Albert Bifet"}, {"affiliation": " Yahoo! Research", "name": " Francesco Bonchi"}, {"affiliation": " Aalto University", "name": " Aristides Gionis"}], "title": "STRIP: Stream Learning of Influence Probabilities"}, "191": {"abstract": "With the advances and increasing sophistication in data collection techniques, we are facing with large amounts of data collected from multiple heterogeneous sources in many applications. For example, in the study of Alzheimer's Disease (AD), different types of measurements such as neuroimages, gene/protein expression data, genetic data etc. are often collected and analyzed together for improved predictive power. It is believed that a joint learning of multiple data sources is beneficial as different data sources may contain complementary information, and feature-pruning and data source selection are critical for learning interpretable models from high-dimensional data. Very often the collected data comes with block-wise missing entries; for example, a patient without the MRI scan will have no information in the MRI data block, making his/her overall record incomplete. There has been a growing interest in the data mining community on expanding traditional techniques for single-source complete data analysis to the study of multi-source incomplete data. The key challenge is how to effectively integrate information from multiple heterogeneous sources in the presence of block-wise missing data. In this paper we first investigate the situation of complete data and present a unified ``bi-level\" learning model for multi-source data. Then we give a natural extension of this model to the more challenging case with incomplete data. Our major contributions are threefold: (1) the proposed models handle both feature-level and source-level analysis in a unified formulation and include several existing feature learning approaches as special cases; (2) the model for incomplete data avoids direct imputation of the missing elements and thus provides superior performances. Moreover, it can be easily generalized to other applications with block-wise missing data sources; (3) efficient optimization algorithms are presented for both of the complete and incomplete model. ", "authors": [{"affiliation": " Arizona State University", "name": "Shuo Xiang"}, {"affiliation": " Arizona State University", "name": " Lei Yuan"}, {"affiliation": " IBM Research", "name": " Wei Fan"}, {"affiliation": " ", "name": " Yalin Wang"}, {"affiliation": " ", "name": " Paul Thompson"}, {"affiliation": " Arizona State University", "name": " Jieping Ye"}], "title": "Multi-Source Learning with Block-wise Missing Data For Alzheimer's Disease Prediction"}, "190": {"abstract": "We describe methods for continual prediction of manufactured product quality prior to final testing. In our most expansive modeling approach, an estimated final characteristic of a product is updated after each manufacturing operation. Our initial application is for the manufacture of microprocessors, and we predict final microprocessor speed. Using these predictions, early corrective manufacturing actions may be taken to increase the speed of expected slow wafers (a collection of microprocessors) or reduce the speed of fast wafers. Such predictions may also be used to initiate corrective supply chain management actions. Developing statistical learning models for this task has many complicating factors: (a) a temporally unstable population (b) missing data that is a result of sparsely sampled measurements and (c) relatively few available measurements prior to corrective action opportunities. In a real manufacturing pilot application, our automated models selected 125 fast wafers in real-time. As predicted, those wafers were significantly faster than average. During manufacture, downstream corrective processing restored 25 nominally unacceptable wafers to normal operation. ", "authors": [{"affiliation": " IBM Research", "name": "Sholom Weiss"}, {"affiliation": " IBM TJ Watson", "name": " Amit Dhurandhar"}, {"affiliation": " IBM Research", "name": " Robert Baseman"}], "title": "Improving Quality Control by Early Prediction of Manufacturing Outcomes"}, "702": {"abstract": "The Electronic Road Pricing (ERP) system was implemented by the Land Transport Authority of Singapore to control traffic by road pricing since 1998. To better understand the traffic condition and improve the pricing scheme, the government initiated the next generation ERP (ERP 2) project, which aims to use the Global Navigation Satellite System (GNSS) collecting positional data from vehicles to analyze traffic condition. However, most drivers fear of being monitored once the government installs the devices in their vehicles to collect GPS data. The existing data stream management systems (DSMS) centralize both data management and privacy control at server site. This framework assumes DSMS server is secure and trustable, and protects providers' data from illegal access by data users. In ERP 2, the DSMS server is maintained by the government, i.e., data user. Thus, the existing framework is not adoptable. In this paper, we propose a novel framework for streaming data management with privacy preservation. We push the privacy protection to data provider site. By doing this, the system could be safer and more efficient. Our framework can be used for the situations such as ERP 2, i.e., data providers would like to control their own privacy policies, the workload of DSMS server needs to be reduced, and/or data providers need to be more energy efficient.", "authors": [{"affiliation": " Institute for Infocomm Research", "name": "Huayu Wu"}, {"affiliation": " Institute for Infocomm Research", "name": " Wee Siong Ng"}, {"affiliation": " National University of Singapore", "name": " Kian-Lee Tan"}, {"affiliation": " Institute for Infocomm Research", "name": " Wei Wu "}, {"affiliation": " Institute for Infocomm Research", "name": " Shili Xiang"}, {"affiliation": " Institute for Infocomm Research", "name": " Mingqiang Xue"}], "title": "A Privacy Preserving Framework for Managing Vehicle Data in Road Pricing Systems"}, "392": {"abstract": "Mining probabilistic frequent patterns from uncertain data has received a great deal of attention in recent years due to the wide applications. However, probabilistic frequent pattern mining suffers from the problem that an exponential number of result patterns are generated, which seriously hinders further evaluation and analysis. In this paper, we focus on the problem of mining probabilistic representative frequent patterns (P-RFP), which is the minimal set of patterns with adequately high probability to represent all frequent patterns.  Observing the bottleneck in checking whether a pattern can probabilistically represent another, which involves the computation of a joint probability of the supports of two patterns, we introduce a novel approximation of the joint probability with both theoretical and empirical proofs.  Based on the approximation, we propose an Approximate P-RFP Mining (APM) algorithm, which effectively and efficiently compresses the set of probabilistic frequent patterns.  To our knowledge, this is the first attempt to analyze the relationship between two probabilistic frequent patterns through an approximate approach. Our experiments on both synthetic and real-world datasets demonstrate that the APM algorithm accelerates P-RFP mining dramatically, orders of magnitudes faster than an exact solution. Moreover, the error rate of APM is guaranteed to be very small when the database contains hundreds transactions, which further affirms APM is a practical solution for summarizing probabilistic frequent patterns.", "authors": [{"affiliation": " UTS", "name": "Chunyang Liu"}, {"affiliation": " ", "name": " Ling Chen"}, {"affiliation": " QCIS, University of Technology, Sydney", "name": " Chengqi Zhang"}], "title": "Summarizing Probabilistic Frequent Patterns: A Fast Approach"}, "395": {"abstract": "We pose the problem of network discovery which involves simplifying spatio-temporal data into nodes and edges. Such problems naturally exist in fMRI scans of human subjects. These scans consist of activations of thousands of voxels over time with the aim to simplify them into the underlying cognitive network being used. We propose supervised and semi-supervised variations of this problem and postulate a constrained tensor decomposition formulation and a corresponding alternating least squares solver that is easily implementable. We show this formulation works well in controlled experiments where supervision is incomplete, superfluous and noisy and is able to recover the underlying ground truth network. We then show that for real fMRI data our approach can reproduce well known results in neurology regarding the default mode network in resting state healthy and Alzheimer affected individuals. Finally, we show that the reconstruction error of the decomposition provides a useful measure of the network strength and is useful at predicting key cognitive scores both by itself and with additional clinical information.", "authors": [{"affiliation": " University of California -- Davis", "name": "Ian Davidson"}], "title": "Network Discovery via Constrained Tensor Analysis of fMRI Data"}, "399": {"abstract": "Role discovery in graphs is an emerging area that allows analysis of complex graphs in an intuitive way. In contrast to community discovery, which finds groups of highly connected nodes, the role discovery problem finds groups of nodes that share similar topological structure in the graph, and hence a common role or function such as being a broker or a periphery node. However, existing work so far is completely unsupervised, which is undesirable for a number of reasons. We provide an alternating least squares framework that allows convex constraints to be placed on the role discovery problem, which can provide useful supervision. In particular we explore supervision to enforce i) sparsity, ii) diversity, and iii) alternativeness in the roles. We illustrate the usefulness of this supervision on various data sets and applications.", "authors": [{"affiliation": " U.C. Davis", "name": "Sean Gilpin"}, {"affiliation": " ", "name": " Tina Eliassi-Rad"}, {"affiliation": " University of California -- Davis", "name": " Ian Davidson"}], "title": "Guided Learning for Role Discovery (GLRD): Framework, Algorithms, and Applications"}, "569": {"abstract": "Selective sampling is an active variant of online learning in which the learner is allowed to adaptively query the label of an observed example. The goal of selective sampling is to achieve a good trade-off between prediction performance and the number of queried labels. Existing selective sampling algorithms are designed for vector-based data. In this paper, motivated by the ubiquity of graph representations in real-world applications, we propose to study selective sampling on graphs. We first present an online version of the well-known Learning with Local and Global Consistency method (OLLGC). It is essentially a second-order online learning algorithm, and can be seen as an online ridge regression in the Hilbert space of functions defined on graphs. We prove its regret bound in terms of the structural property (cut size) of a graph, or the spectral property of graph Laplaican. Based on OLLGC, we present a selective sampling algorithm, namely Selective Sampling with Local and Global Consistency (SSLGC), which queries the label of each node based on the confidence of the linear function on graphs. Its bound on the label complexity is also derived. We analyze the low-rank approximation of graph kernels, which enables the online algorithms scale to large graphs. Experiments on benchmark graph datasets show that OLLGC outperforms the state-of-the-art first-order algorithm significantly, and SSLGC achieves comparable or even better results than OLLGC while querying substantially fewer nodes. Moreover, SSLGC is overwhelmingly better than random sampling.", "authors": [{"affiliation": " CS, UIUC", "name": "Quanquan Gu"}, {"affiliation": " IBM Research", "name": " Charu Aggarwal"}, {"affiliation": " UIUC", "name": " Jialu Liu"}, {"affiliation": " University of Illinois at Urbana-Champaign", "name": " Jiawei Han"}], "title": "Selective Sampling on Graphs for Classification"}, "797": {"abstract": "We address the problem of predicting new drug-target interactions from three inputs: known interactions, similarities over drugs and those over targets. This setting has been considered by many methods, which however have a common problem of allowing to have only one similarity matrix over drugs and that over targets. The key idea of our approach is to use more than one similarity matrices over drugs as well as those over targets, where weights over the multiple similarity matrices are estimated from data to automatically select similarities, which are effective for improving the performance of predicting drug-target interactions. We propose a factor model, named Multiple Similarities Collaborative Matrix Factorization~(MSCMF), which projects drugs and targets into a common low-rank feature space, which is further consistent with weighted similarity matrices over drugs and those over targets. These two low-rank matrices and weights over similarity matrices are estimated by an alternating least squares algorithm. Our approach allows to predict drug-target interactions by the two low-rank matrices collaboratively and to detect similarities which are important for predicting drug-target interactions. This approach is general and applicable to any binary relations with similarities over elements, being found in many applications, such as recommender systems. In fact, MSCMF is an extension of weighted low-rank approximation for one-class collaborative filtering. We extensively evaluated the performance of MSCMF by using both synthetic and real datasets. Experimental results showed nice properties of MSCMF on selecting similarities useful in improving the predictive performance and the performance advantage of MSCMF over six state-of-the-art methods for predicting drug-target interactions.", "authors": [{"affiliation": " Fudan University", "name": "Xiaodong Zheng"}, {"affiliation": " Fudan University", "name": " Hao Ding"}, {"affiliation": " Kyoto University", "name": " Hiroshi Mamitsuka"}, {"affiliation": " Fudan University", "name": " Shanfeng Zhu"}], "title": "Collaborative Matrix Factorization with Multiple Similarities for Predictin Drug-Target Interactions"}, "795": {"abstract": "In this paper, we introduce a trial-and-error model to study information diffusion in a social network. Specifically, in every discrete period, all individuals in the network concurrently try a new technology or product with certain respective probabilities. If it turns out that an individual observes a better utility, he will then adopt the trial; otherwise, the individual continues to choose his prior selection.  We first demonstrate that the trial and error behavior of individuals characterizes certain global community structures of a social network, from which we are able to detect macro-communities through the observation of micro-behavior of individuals. We run simulations on classic benchmark testing graphs, and quite surprisingly, the results show that the trial and error dynamics even outperforms the Louvain method (a popular modularity maximization approach) if individuals have dense connections within communities. This gives a solid justification of the model.  We then study the influence maximization problem in the trial-and-error dynamics. We give a heuristic algorithm based on community detection and provide experiments on both testing and large scale collaboration networks. Simulation results show that our algorithm significantly outperforms several well-studied heuristics including degree centrality and distance centrality in almost all of the scenarios. Our results reveal the relation between the budget that an advertiser invests and marketing strategies, and indicate that the mixing parameter, a benchmark evaluating network community structures, plays a critical role for information diffusion.", "authors": [{"affiliation": " Nanyang Technological University", "name": "Xiaohui Bei"}, {"affiliation": " Nanyang Technological Univ", "name": " Ning Chen"}, {"affiliation": " Nanyang Technological University", "name": " Liyu Dou"}, {"affiliation": " Shanghai Jiao Tong University", "name": " Xiangru Huang"}, {"affiliation": " Shanghai Jiao Tong University", "name": " Ruixin Qiang"}], "title": "Trial and Error in Influential Social Networks"}, "141": {"abstract": "Sales representatives must have access to meaningful and actionable intelligence about potential customers to be effective in their roles. Historically, GE Capital Americas sales reps identified leads by manually searching through news reports and financial statements either in print or online. Here we describe a system built to automate the collection and aggregation of information on companies, which is then mined to identify actionable sales leads. The Financing Lead Triggers system is comprised of three core components that perform information fusion, knowledge discovery and information visualization. Together these components extract raw data from disparate sources, fuse that data into information, and then automatically mine that information for actionable sales leads driven by a combination of expert-defined and statistically derived triggers. A web-based interface provides sales reps access to the company information and sales leads in a single location. The use of the Lead Triggers system has significantly improved the performance of the sales reps, providing them with actionable intelligence that has improved their productivity by 30-50%. In 2010, Lead Triggers provided leads on opportunities that represented over $44B in new deal commitments for GE Capital.", "authors": [{"affiliation": " GE Global Research", "name": "Kareem Aggour"}, {"affiliation": " GE Global Research", "name": " Bethany Hoogs"}], "title": "Financing Lead Triggers: Empowering Sales Reps Through Knowledge Discovery and Fusion"}, "526": {"abstract": "The least squares problem is one of the most important regression problems in statistics, machine learning and data mining. In this paper, we present the Constrained Stochastic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by imposing a provable constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound $O(\\log{T})$, and fastest convergence speed among all first order approaches. Empirical studies justify the effectiveness of CSGD by comparing it with SGD and other state-of-the-art approaches. An example is also given to show how to use CSGD to optimize SGD based least squares problems to achieve a better performance.", "authors": [{"affiliation": " UMass Boston", "name": "Yang Mu"}, {"affiliation": " University of Massachusetts Boston", "name": " Wei Ding"}, {"affiliation": " University of Technology Sydney", "name": " Tianyi Zhou"}, {"affiliation": " University of Technology Sydney", "name": " Dacheng Tao"}], "title": "Constrained Stochastic Gradient Descent for Large-scale Least Squares Problem"}, "914": {"abstract": "This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost and power. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported *cluster* implementations for common benchmark problems. A key design criterion is *rapid exploration* of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computational kernels (iii), BIDMach, a machine learning system that includes efficient model optimizers, (iv) Butterfly mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative update algorithms. We present several benchmark problems as case studies to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem.", "authors": [{"affiliation": " UC Berkeley", "name": "John Canny"}, {"affiliation": " UC Berkeley", "name": " Huasha Zhao"}], "title": "Big Data Analytics with Small Footprint: Squaring the Cloud"}, "528": {"abstract": "Aggregator websites typically present documents in the form of representative clusters.  In order for users to get a broader perspective, it is important to deliver a  diversified set of representative documents in those clusters.  One approach to diversification is to maximize  the average dissimilarity among documents.  Another way to capture diversity is to  avoid showing several documents from the same category (e.g. from the same news channel).  We model the latter approach as a (partition) matroid constraint, and  study diversity maximization problems under matroid constraints.  We present the first constant-factor approximation algorithm for this problem,  using a new technique.  Our local search $0.5$-approximation algorithm  is also the first constant-factor approximation for the max-dispersion problem under matroid constraints. Our combinatorial proof technique for maximizing diversity under matroid constraints  uses the existence of a family of Latin squares which may also be of independent interest.   In order to apply these diversity maximization algorithms in the context of aggregator websites and as a preprocessing step for our diversity maximization tool, we develop  greedy clustering algorithms that maximize weighted coverage of a predefined set of topics. Our algorithms are based on computing a set of cluster centers, where clusters are formed around them.  We show the better performance of our algorithms for diversity  and coverage maximization by running experiments on real (Twitter) and synthetic data in the context of real-time search over micro-posts.  Finally we perform a user study validating our algorithms and diversity metrics.", "authors": [{"affiliation": " Columbia University", "name": "Zeinab Abbassi"}, {"affiliation": " Google", "name": " Vahab Mirrokni"}, {"affiliation": " Google", "name": " Mayur Thakur"}], "title": "Diversity Maximization Under Matroid Constraints"}, "427": {"abstract": "Given a set of entities, the all-pairs similarity search aims at identifying all pairs of entities that have similarity greater than (or distance smaller than) some user-defined threshold. In this article, we propose a MapReduce-based framework for solving this problem in metric spaces. Novel elements of our solution include: i) flexible support for multiple metrics of interest; ii) an autonomic approach to partition the in- put dataset with minimal redundancy to achieve good load- balance in the presence of limited computing resources; iii) an on-the-fly lossless compression strategy to reduce both the run time and the final output. We validate the utility, scalability and the effectiveness of the approach on hundreds of machines using real and synthetic datasets. ", "authors": [{"affiliation": " OSU", "name": "Ye Wang"}, {"affiliation": " Google Inc.", "name": " Ahmed Metwally"}, {"affiliation": " The Ohio State University", "name": " Srinivasan Parthasarathy"}], "title": "Metric All-Pairs Similarity Search on MapReduce"}, "583": {"abstract": "Finding dense subgraphs is an important graph-mining task with many applications. Given that the direct optimization of edge density is not meaningful, as even a single edge achieves maximum density, research has focused on optimizing alternative density functions. A very popular among such functions is the average degree, whose maximization leads to the well-known \\emph{densest-subgraph} notion.  Surprisingly enough, however, densest subgraphs are typically large graphs, with small edge density, and large diameter.  In this paper, we define a novel density function, which gives subgraphs of much higher quality than densest subgraphs: the graphs found by our method are compact, dense, and with smaller diameter. We show that the proposed function can be derived from a general framework, which includes other important density functions as subcases and for which we show interesting general theoretical properties.  We evaluate our algorithms on real and synthetic datasets, and we also devise several application studies as variants of our original problem. When compared with the method that finds the subgraph of the largest average degree, our algorithms return denser subgraphs with smaller diameter. Finally, our work opens new research directions which we discuss in detail.", "authors": [{"affiliation": " Carnegie Mellon University", "name": "Charalampos Tsourakakis"}, {"affiliation": " Yahoo! Research", "name": " Francesco Bonchi"}, {"affiliation": " Aalto University", "name": " Aristides Gionis"}, {"affiliation": " Yahoo! Research", "name": " Francesco Gullo"}, {"affiliation": " University of Pittsburgh", "name": " Maria Tsiarli"}], "title": "Denser than the densest subgraph: extracting optimal quasi-cliques with quality guarantees"}, "301": {"abstract": "The junction tree approach, with applications in artificial intelligence, computer vision, machine learning, and statistics, is often used for computing posterior distributions in probabilistic graphical models. One of the key challenges associated with junction trees is computational, and several parallel computing technologies - including many-core processors - have been investigated to meet this challenge. Many-core processors (including GPUs) are now programmable, unfortunately their complexities make it hard to manually tune their parameters in order to optimize software performance.  In this paper, we investigate a machine learning approach to minimize the execution time of parallel junction tree algorithms implemented on a GPU. By carefully allocating a GPU\u00d5s threads to different parallel computing opportunities in a junction tree, and treating this thread allocation problem as a machine learning problem, we find in experiments that regression - specifically support vector regression - can substantially outperform manual optimization. ", "authors": [{"affiliation": " Carnegie Mellon University", "name": "Lu Zheng"}, {"affiliation": " Carnegie Mellon University", "name": " Ole Mengshoel"}], "title": "Optimizing Parallel Belief Propagation in Junction Trees: Using Regression"}, "444": {"abstract": "Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Low-rank matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. Using variational approximations or Markov chain Monte Carlo sampling to approximate the posterior distribution over models, we can choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many \"positive\" data points as possible. We evaluate our methods on simulated data, and also show their applicability to movie ratings prediction and the discovery of drug-target interactions.", "authors": [{"affiliation": " Carnegie Mellon University", "name": "Dougal Sutherland"}, {"affiliation": " Carnegie Mellon University", "name": " Barnabas Poczos"}, {"affiliation": "", "name": " Jeff Schneider"}], "title": "Active Learning and Search on Low-Rank Matrices"}, "245": {"abstract": "Discovering frequent graph patterns in a graph database offers valuable information in a variety of applications. However, if the graph dataset contains sensitive data of individuals such as mobile phone-call graphs and web-click graphs, releasing discovered frequent patterns may present a threat to the privacy of individuals. {\\em Differential privacy} has recently emerged as the {\\em de facto} standard for private data analysis due to its provable privacy guarantee. In this paper we propose the first differentially private algorithm for mining frequent graph patterns.  We first show that previous techniques on differentially private discovery of frequent {\\em itemsets} cannot apply in mining frequent graph patterns due to the inherent complexity of handling structural information in graphs. We then address this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling based algorithm. Unlike previous work on frequent itemset mining, our techniques do not rely on the output of a non-private mining algorithm. Instead, we observe that both frequent graph pattern mining and the guarantee of differential privacy can be unified into an MCMC sampling framework. In addition, we establish the privacy and utility guarantee of our algorithm and propose an efficient neighboring pattern counting technique as well. Experimental results show that the proposed algorithm is able to output frequent patterns with good precision.", "authors": [{"affiliation": " North Carolina State Univ", "name": "Entong Shen"}, {"affiliation": " North Carolina State University", "name": " Ting Yu"}], "title": "Mining Frequent Graph Patterns with Differential Privacy"}, "240": {"abstract": "One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control, which is to expect high-quality results from crowd workers who are neither necessarily very capable nor motivated. A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks. For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been proposed. However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing,  which occupy the majority on most crowdsourcing marketplaces. In this paper, we propose a statistical quality estimation method for such general crowdsourcing tasks. Our method is based on the two-stage procedure;  multiple workers are first requested to work on the same tasks in the creation stage, and then another set of workers review and grade each artifact in the review stage. We model the ability of each author and the bias of each reviewer, and propose a two-stage probabilistic generative model using the graded response model in the item response theory. Experiments using several general crowdsourcing tasks show that our method outperforms popular vote aggregation methods, especially when the number of reviewers for each work is small, which implies that our method can deliver high quality results with lower costs.", "authors": [{"affiliation": " The University of Tokyo", "name": "Yukino Baba"}, {"affiliation": " The University of Tokyo", "name": " Hisashi Kashima"}], "title": "Statistical Quality Estimation for General Crowdsourcing Tasks"}, "242": {"abstract": "We present our experience of using machine learning techniques over data originating from advanced meter infrastructure (AMI) systems for water consumption in a medium-size city. We focus on two new use cases that are of utmost importance to city authorities. One use case is the automatic identification of malfunctioning meters, with a focus on distinguishing them from legitimate non-consumption such as during periods when the household residents are on vacation. The other use case is the identification of leaks or theft in the unmetered common areas of apartment buildings. These two use cases are highly important to city authorities both because of the lost revenue they imply and because of the hassle to the residents in cases of delayed identification. Both cases are inherently complex to analyze and require advanced data mining techniques in order to achieve high levels of correct identification. Our results provide tangible value to the authorities in terms of faster and more accurate detection of malfunctioning meters as well as leaks in the common areas. This results in a significant increase in technician efficiency and a decrease in the amount of wasted, non-revenue, water.", "authors": [{"affiliation": " IBM", "name": "Einat Kermany"}, {"affiliation": " Arad Technologies", "name": " Hagai Michaelis"}, {"affiliation": " IBM", "name": " Dorit Baras"}, {"affiliation": " IBM", "name": " Hanna Mazzawi"}, {"affiliation": " IBM", "name": " Yehuda Naveh"}], "title": "Analysis of Advanced Meter Infrastructure Data of Water Consumption in Apartment Buildings"}, "388": {"abstract": "Location-based services have become widely available on mobile devices. Existing methods employ a pull model or user-initiated model, where a user issues a query to a server which replies with location-aware answers. To provide users with instant replies, a push model or server-initiated model is becoming an inevitable computing model in the next-generation location-based services. In the push model, subscribers register spatio-textual subscriptions to capture their interests, and publishers post spatio-textual messages. This calls for a high-performance location-aware publish/subscribe system to deliver publishers' messages to relevant subscribers.  In this paper, we address the research challenges that arise in designing a location-aware publish/subscribe system. We propose an \\rtree based index structure by integrating textual descriptions into \\rtree nodes. We devise efficient filtering algorithms and develop effective pruning techniques to improve filtering efficiency. Experimental results show that our method achieves high performance. For  example, our method can filter 500 tweets in a second for 10 million registered subscriptions on a commodity computer.", "authors": [{"affiliation": " Tsinghua Univeristy", "name": "Guoliang Li"}], "title": "Location-Aware Publish/Subscribe"}, "589": {"abstract": "Groups play an essential role in many social websites which promote users' interactions and accelerate the diffusion of information. Recommending groups that users are really interested to join is significant for both users and social media. While traditional group recommendation problem has been extensively studied, we focus on a new type of the problem, i.e., event-based group recommendation. Unlike the other forms of groups, users join this type of groups mainly for participating offline events organized by group members or inviting other users to attend events sponsored by them. These characteristics determine that previously proposed approaches for group recommendation cannot adapt to the new problem easily as they ignore the geographical influence and other explicit features of groups and users.  In this paper, we propose a method called Pairwise Tag-enhAnced and featuRe-based Matrix factorIzation for Group recommendAtioN (PTARMIGAN), which considers location features, social features, and implicit patterns simultaneously in a unified model. More specifically, we exploit matrix factorization to model interactions between users and groups. Meanwhile, we incorporate their profile information into pairwise enhanced latent factors respectively. We also utilize the linear model to capture explicit features. Due to the reinforcement between explicit features and implicit patterns, our approach can provide better group recommendations. We conducted a comprehensive performance evaluation on real word data sets and the experimental results demonstrate the effectiveness of our method.", "authors": [{"affiliation": " Department of Computer Science", "name": "Wei Zhang"}, {"affiliation": " Tsinghua University", "name": " Jianyong Wang"}], "title": "Combining Latent Factor Model with Location Features for Event-based Group Recommendation"}, "249": {"abstract": "   Many real-world graphs have complex labels on the nodes and edges.    Mining only exact patterns yields limited insights, since it may be    hard to find exact matches. However, in many domains it is relatively    easy to compute some cost (or distance) between different labels.    Using this information, it becomes possible to mine a much richer set    of approximate subgraph patterns, which preserve the topology but allow    bounded label mismatches. We present novel and scalable methods to    efficiently solve the approximate isomorphism problem. We show that    the mined approximate patterns yield interesting patterns in several    real-world graphs ranging from IT and protein interaction networks to    protein structures.", "authors": [{"affiliation": " RPI", "name": "Pranay Anchuri"}, {"affiliation": " Rensselaer Polytechnic Institute", "name": " Mohammed Zaki"}, {"affiliation": " HP Labs", "name": " Omer Barkol"}, {"affiliation": " HP Labs", "name": " Shahar Golan"}, {"affiliation": " HP Labs", "name": " Moshe Shamy"}], "title": "Approximate Graph Mining with Label Costs"}, "760": {"abstract": "The automated targeting of online display ads at scale requires the simultaneous evaluation of a single prospect against many independent models.  When deciding which ad to show to a user, one must calculate likelihood-to-convert scores for that user across all potential advertisers in the system.  For modern machine-learning-based targeting, as conducted by Media6Degrees (M6D), this can mean scoring against thousands of models in a large, sparse feature space.  Dimensionality reduction within this space is useful, as it decreases scoring time and model storage requirements.  To meet this need, we develop a novel algorithm for scalable supervised dimensionality reduction across hundreds of simultaneous classification tasks.  The algorithm performs hierarchical clustering in the space of model parameters from historical models in order to collapse related features into a single dimension.  This allows us to implicitly incorporate feature and label data across all tasks without operating directly in a massive space.  We present experimental results showing that for this task our algorithm outperforms other popular dimensionality-reduction algorithms across a wide variety of ad campaigns, as well as production results that showcase its performance in practice.", "authors": [{"affiliation": " Media6Degrees", "name": "Troy Raeder"}, {"affiliation": " M6D", "name": " Claudia Perlich"}, {"affiliation": " M6D", "name": " Brian Dalessandro"}, {"affiliation": " M6D", "name": " Ori Stitelman"}, {"affiliation": " NYU Stern School of Business", "name": " Foster Provost"}], "title": "Scalable Supervised Dimensionality Reduction Using Clustering"}, "780": {"abstract": "In this paper we present a computational framework based on multiple instance learning for mapping informal settlements (slums) using very high-resolution remote sensing imagery. From remote sensing perspective, informal settlements share unique spatial characteristics that distinguish them from other urban structures like industrial, commercial, and formal residential settlements. However, regular pattern recognition and machine learning methods, which are predominantly single-instance or per-pixel classifiers,  often fail to accurately map the informal settlements as they do not capture the complex spatial patterns. To overcome these limitations we employed a multiple instance based machine learning approach, where groups of contiguous pixels (image patches) are modelled as generated by a Gaussian distribution.    We have conducted several experiments on high-resolutions satellite imagery, representing four unique geographic regions across the world. Our method showed consistent improvement in accurately identifying informal settlements.", "authors": [{"affiliation": " Oak Ridge National Labs", "name": "Ranga Vatsavai"}], "title": "Gaussian Multiple Instance Learning Approach for Mapping the Slums of the World Using Very High Resolution Imagery"}, "511": {"abstract": "Classification of time series data is an important problem with applications in virtually every scientific endeavor. The large research community working on time series classification has typically used the UCR Archive to test their algorithms. In this work we argue that the availability of this resource has isolated much of the research community from the following reality, labeled time series data is often very difficult to obtain.  The obvious solution to this problem is the application of semi-supervised learning; however, as we shall show, direct applications of off-the-shelf semi-supervised learning algorithms do not typically work well for time series. In this work we explain why semi-supervised learning algorithms typically fail for time series problems, and we introduce a simple but very effective fix. We demonstrate our ideas on diverse real word problems.  ", "authors": [{"affiliation": " UCR", "name": "Yanping Chen"}, {"affiliation": " ", "name": " Bing Hu"}, {"affiliation": " University of California -- Riverside", "name": " Eamonn Keogh"}, {"affiliation": "", "name": " Gustavo Batista"}], "title": "DTW-D: Time Series Semi-Supervised Learning from a Single Example"}, "1006": {"abstract": "Identifying the k most influential individuals in a social network is a well-studied problem. The objective is to detect k individuals in a (social) network who will influence the maximum number of people, if they are independently convinced of adopting a new strategy (product, idea, etc). There are cases  in real life, however, where we aim to instigate groups instead of individuals to trigger network diffusion. Such cases abound, e.g.,  billboards, TV commercials and  newspaper ads are utilized extensively to boost the popularity and raise awareness.  In this paper, we generalize the ``influential nodes'' problem. Namely we are interested to locate the most ``influential groups'' in a network. As the first paper to address this problem: we (1) propose a fine-grained model of information diffusion for the group-based problem, (2) show that the process is submodular and present an algorithm to determine the influential groups under this model (with a precise approximation bound), (3) propose a coarse-grained model that inspects the network at group level (not individuals) significantly speeding up calculations for large networks, (4) show that the diffusion function we design here is submodular in general case, and propose an approximation algorithm for this coarse-grained model, and finally by conducting experiments on real datasets, (5) demonstrate that seeding members of selected groups to be the first adopters can broaden diffusion (when compared to the influential individuals case). Moreover, we can identify these influential groups much faster (up to 12 million times speedup), delivering a practical solution to this problem.", "authors": [{"affiliation": " University of Toronto", "name": "Milad Eftekhar"}, {"affiliation": " University of Toronto", "name": " Yashar Ganjali"}, {"affiliation": " University of Toronto", "name": " Nick Koudas"}], "title": "Information Cascade at Group Scale"}, "434": {"abstract": "Linear Classification has achieved complexity linear to the data size. However, in many applications, data contain large amount of samples that does not help improve the quality of model, but still cost much I/O and memory to process. In this paper, we show how a Block Coordinate Descent method based on Nearest-Neighbor Index can significantly reduce such cost when learning a dual-sparse model. In particular, we employ truncated loss function to induce a series of convex programs with superior dual sparsity, and solve each dual using Indexed Block Coordinate Descent, which makes use of Approximate Nearest Neighbor (ANN) search to select active dual variables without I/O cost on irrelevant samples. We prove that, despite the bias and weak guarantee from ANN query, the proposed algorithm has global convergence to the solution defined on entire dataset, with sublinear complexity in each iteration. Experiments in both sufficient and limited memory conditions show that the proposed approach learns times faster than other state-of-the-art solvers without sacrificing accuracy.", "authors": [{"affiliation": " National Taiwan University", "name": "En-Hsu Yen"}, {"affiliation": " National Taiwan University", "name": " Chun-Fu Chang"}, {"affiliation": " National Taiwan University", "name": " Ting-Wei Lin"}, {"affiliation": " National Taiwan University", "name": " Shan-Wei Lin"}, {"affiliation": " National Taiwan University", "name": " Shou-De Lin"}], "title": "Indexed Block Coordinate Descent for Large-Scale Linear Classification with Limited Memory"}, "516": {"abstract": "Time series classification has been an active area of research in the data mining community for over a decade, and significant progress has been made in the tractability and accuracy of learning.  However, virtually all work assumes a one-time training session in which labeled examples of all the concepts to be learned are provided. This assumption may be valid in a handful of situations, but it does not hold in most medical and scientific applications where we initially may have only the vaguest understanding of what concepts can be learned. Based on this observation, we propose a never-ending learning framework for time series in which an agent examines an unbounded stream of data and occasionally asks a teacher (which may be a human or an algorithm) for a label. We demonstrate the utility of our ideas with experiments that consider real world problems in domains as diverse as medicine, entomology, wildlife monitoring and human behavior analyses.       ", "authors": [{"affiliation": " Univ of California, Riverside", "name": "Yuan Hao"}, {"affiliation": " UCR", "name": " Yanping Chen"}, {"affiliation": " ", "name": " Jesin Zakaria"}, {"affiliation": " ", "name": " Bing Hu"}, {"affiliation": " ", "name": " Thanawin Rakthanmanon"}, {"affiliation": " University of California -- Riverside", "name": " Eamonn Keogh"}], "title": "Towards Never-Ending Learning from Time Series Streams"}, "623": {"abstract": "As datasets become larger, more complex, and more available to diverse groups of analysts, it would be quite useful to be able to automatically and generically assess the quality of estimates, much as we are able to automatically train and evaluate predictive models such as classifiers. However, despite the fundamental importance of estimator quality assessment in data analysis, this task has eluded highly automatic solutions.  While the bootstrap provides perhaps the most promising step in this direction, its level of automation is limited by the difficulty of evaluating its finite sample performance and even its asymptotic consistency.  Thus, we present here a general diagnostic procedure which directly and automatically evaluates the accuracy of the bootstrap's outputs, determining whether or not the bootstrap is performing satisfactorily when applied to a given dataset and estimator.  We show via an extensive empirical evaluation on a variety of estimators and simulated and real datasets that our proposed diagnostic is effective. Our experiments on a real-world query workload from Conviva Inc. involving 1.7TB of data (n=500M) show that the diagnostic correctly identifies that the bootstrap is applicable on 219 out of the 268 SQL-like queries, with 7 false positives and 12 false negatives.", "authors": [{"affiliation": " ", "name": "Ariel Kleiner"}, {"affiliation": " UC Berkeley", "name": " Ameet Talwalkar"}, {"affiliation": " ", "name": " Sameer Agarwal"}, {"affiliation": " ", "name": " Michael Jordan"}, {"affiliation": "", "name": " Ion Stoica"}], "title": "A General Bootstrap Performance Diagnostic"}, "1075": {"abstract": "Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Inuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft\u00d5s Bing, the use of controlled experiments has grown exponentially over time, with over 100 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are millions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders\u00d5 early excitement, saving us similar large amounts.", "authors": [{"affiliation": " Microsoft", "name": "Ron Kohavi"}, {"affiliation": " Microsoft", "name": " Alex Deng"}, {"affiliation": " Microsoft", "name": " Brian Frasca"}, {"affiliation": " Microsoft", "name": " Toby Walker"}, {"affiliation": " Microsoft", "name": " Ya Xu"}, {"affiliation": " Microsoft", "name": " Nils Pohlmann"}], "title": "Online Controlled Experiments at Large Scale"}, "627": {"abstract": "In Multiple Instance Learning (MIL), each entity  is normally expressed as a set of instances.    Most of the current MIL methods only deal with the case when each instance is represented by one type of features. However, in many real world applications, entities are often described from several different information sources (views). For example, when applying MIL to image categorization, the characteristics of each image can be derived from both its RGB features and SIFT features. Previous research work has shown that, in traditional learning methods, by considering the consistencies between different information sources, the classification performance can be improved.   To incorporate  the consistencies between different  information sources into MIL, we propose a novel research framework --  Multi-Instance Learning from Multiple Information Sources (MI$^2$LS).  Based on this framework, an algorithm -- Fast MI$^2$LS (FMI$^2$LS) is designed for this research framework, which combines Concave-Convex Constraint Programming (CCCP) method and an adapted Stoachastic Gradient Descent (SGD) method. Some theoretical analysis on time complexity and generalization error bound are given based on the proposed method. Experimental results on three datasets, i.e., Reuters21578, WebKB,  and a novel application -- insider threat detection dataset, demonstrate the superior performance of the proposed method against several  state-of-the-art MIL techniques on both efficiency and effectiveness.", "authors": [{"affiliation": " Purdue University", "name": "Dan Zhang"}, {"affiliation": " Stevens Institute of Technolog", "name": " Jingrui He"}, {"affiliation": " IBM Research", "name": " Richard Lawrence"}], "title": "MILS: Multi-Instance Learning from Multiple Information Sources"}, "271": {"abstract": "We present an effective, multifaceted system for exploratory analyses of highly heterogeneous and \u00d2messy\u00d3 document collections. Our system is based on intelligently tagging individual documents in a purely automated fashion and exploiting these tags in a powerful faceted browsing framework.  Tagging strategies employed include both unsupervised and supervised approaches based on machine learning and natural language processing (e.g., LDA, SVM, and NER). As one of our key tagging strategies, we introduce the KERA algorithm (Keyword Extraction for Reports and Articles).  KERA is able to extract topic-representative terms from individual documents in a completely unsupervised fashion and is revealed to be significantly more effective than state-of-the-art methods. Finally, we evaluate our system in its ability to help users locate documents pertaining to military critical technologies buried deep in a sea of largely unimportant files. ", "authors": [{"affiliation": " Institute for Defense Analyses", "name": "Arun Maiya"}], "title": "Exploratory Analysis of Highly Heterogeneous Document Collections"}, "572": {"abstract": "Empirical risk minimization (ERM) provides a principal guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label and these data may have different distribution. In this paper, we generalize the empirical risk minimization principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound we develop a practical batch mode active learning method. The proposed formulation involves a non-convex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying most uncertain and representative queries. Experiments on benchmark data sets and real-world applications demonstrate the superior performance of our proposed method in comparison with the state-of-the-art methods.", "authors": [{"affiliation": " Arizona State University", "name": "Zheng Wang"}, {"affiliation": " Arizona State University", "name": " Jieping Ye"}], "title": "Querying Discriminative and Representative Samples for Batch Mode Active Learning"}, "571": {"abstract": "The benefits of crowdsourcing are well-recognized today for an increasingly broad range of problems. Meanwhile, the rapid development of social media makes it possible to seek the wisdom of a crowd in a broader sphere. However, it is not trivial to implement crowdsourcing platform on social media, specifically to make social media users as workers, we need to address the following two challenges: 1) how to motivate users to participate in tasks, and 2) how to aggregate their opinions. In this paper, we present Wise Market as an effective institution of crowds on social media to motivate users to participate in a task with care and correctly aggregate their opinions on pairwise choice problems. The Wise Market consists of a set of investors each with an associated individual confidence in their prediction, and after investment, only the ones whose choices are the same as the whole market are granted rewards. Therefore, the social media user has to give his/her \u00d2best\u00d3 answer in order to get reward, discouraging careless answers from sloppy users.Using this framework, we define the Effective Market Problem (EMP) as an optimization problem to minimize expected cost of paying out rewards while guaranteeing aminimum confidence level. We propose exact algorithms for calculating the market confidence and the expected cost with O(n log2 n) time cost in a Wise Market with n investors. Especially in the real situations of social media where the numbers of users are enormous, we design a Central Limit Theorem-based approximation algorithm for calculating the market confidence with O(n) time cost, as well as a bounded approximation algorithm for calculating the expected cost with O(n) time cost. Finally, we conducted empirical studies to validate effectiveness of the proposed algorithms on real and synthetic data.", "authors": [{"affiliation": " HKUST", "name": "Chen Cao"}, {"affiliation": " HKUST", "name": " Yongxin Tong"}, {"affiliation": " HKUST", "name": " Lei Chen"}, {"affiliation": " University of Michigan", "name": " H.V. Jagadish"}], "title": "WiseMarket: A New Paradigm for Managing Wisdom of Online Social Users"}, "577": {"abstract": "The two key challenges in hierarchical classification are to leverage the hierarchical dependencies between the class-labels for improving performance, and, at the same time maintaining scalability across large hierarchies. In this paper we propose a regularization framework for large-scale hierarchical classification that addresses both the problems. Specifically, we incorporate the hierarchical dependencies between the class-labels into the regularization structure of the parameters thereby encouraging classes nearby in the hierarchy to share similar model parameters. Furthermore, we extend our approach to scenarios where the dependencies between the class-labels are encoded in the form of a graph rather than a hierarchy. To enable large-scale training, we develop a parallel-iterative optimization scheme that can comfortably handle the largest datasets with hundreds of thousands of classes and millions of instances and learning terabytes of parameters. Our experiments showed consistent and significant performance improvements over other competing approaches and achieved state-of-the-art results on leading benchmark datasets.", "authors": [{"affiliation": " CMU", "name": "Siddharth Gopal"}, {"affiliation": " CMU", "name": " Yiming Yang"}], "title": "Recursive Regularization for Large-scale Classification with Hierarchical and Graphical Dependencies"}, "743": {"abstract": "Matching entities from different information sources is a very important problem in data analysis and data integration. It is however challenging due to the number and diversity of information sources involved, and the significant editorial efforts required to collect large-scale training data. In this paper, we show how user click behavior on Web search can be leveraged for automatically generating training data for entity matching. The basic intuition is that Web pages clicked for a given query are likely to be about the same entity. We use random walk with restart to reduce data sparseness, rely on co-clustering to group queries and Web pages, and exploit page similarity to improve matching precision. Experimental results show that: (i) With 360K pages from 6 major travel websites, we obtain 84K matchings (of 179K pages) that refer to the same entities, with an average precision of 0.826; (ii) The quality of matching obtained from a classifier trained on the resulted seed data is promising: the performance matches that of editorial data at small size and improves with size.", "authors": [{"affiliation": " Yahoo! Research Barcelona", "name": "Xiao Bai"}, {"affiliation": " Komli Labs", "name": " Srinivasan Sengamedu"}, {"affiliation": " Microsoft Research", "name": " Flavio Junqueira"}], "title": "Exploiting User Clicks for Automatic Seed Set Generation for Entity Matching"}, "179": {"abstract": "Crowdsourcing is an effective method to collect labeled data for various data mining tasks. It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable. Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in individual task. In this case, every single task all needs amounts of data to provide accurate estimation. However, in practice, budget provided by customers for a given target task may be limited, and hence each question can be presented to only a few users, and each user can answer only a few questions. This data sparsity problem can make previous approaches perform poorly due to the overfitting problem on rare data and eventually damaging the data veracity. In this paper, we employ transfer learning, which borrows knowledge from auxiliary history tasks to improve the data veracity in a given target task. The motivation is that users have stable characteristics across different crowdsourcing tasks, and thus data from different tasks can be exploited collectively to estimate users' abilities in the target task. We propose a hierarchical Bayesian model, TLC(Transfer Learning for Crowdsourcing), to implement this idea by considering the overlapping users as a bridge. In addition, to avoid possible negative impacts, TLC introduces task-specific factors to model task differences. The experimental results show that TLC improves the accuracy over several state-of-the-art non-transfer learning approaches significantly under very limited budget in given target domains.", "authors": [{"affiliation": " HKUST", "name": "Kaixiang Mo"}, {"affiliation": " HKUST", "name": " Erheng Zhong"}, {"affiliation": " Hong Kong University of Science and Technology", "name": " Qiang Yang"}], "title": "Cross-Task Crowdsourcing"}, "177": {"abstract": "The \"Moneyball revolution\" coincided with a shift in way professional sporting organizations handle and utilize data in terms of decision making processes. Due to the demand for better sports analytics and the improvement in sensor technology, there has been a plethora of ball and player tracking information generated within professional sports for analytical purposes. However, due to the continuous nature of the data and the lack of associated high-level labels to describe it - this rich set of information has had very limited use especially in the analysis of a team's tactics and strategy. In this paper, we give an overview of the types of analysis currently performed mostly with hand-labeled event data and highlight the problems associated with the influx of spatiotemporal data. By way of example, we present an approach which uses an entire season of ball tracking data from the English Premier League (2010-2011 season) to reinforce the common held belief that teams should aim to \"win home games and draw away ones\". We do this by: i) forming a representation of team behavior by  chunking the incoming spatiotemporal signal into a series of quantized bins, and ii) generate an expectation model of team behavior based on a code-book of past performances.  We show that home advantage in soccer is partly due to the conservative strategy of the away team. We also show that our approach can flag anomalous team behavior which has many potential applications.", "authors": [{"affiliation": " Disney Research Pittsburgh", "name": "Patrick Lucey"}, {"affiliation": " ESPN", "name": " Dean Oliver"}, {"affiliation": " Disney Research", "name": " Iain Matthews"}, {"affiliation": " Disney Research", "name": " Peter Carr"}, {"affiliation": " Disney Research", "name": " Joe Roth"}], "title": "Assessing Team Strategy using Spatiotemporal Data"}, "250": {"abstract": "Understanding topic hierarchies in text streams and their evolution patterns over time is very important in many applications. In this paper, we propose the method of evolutionary multi-branch tree clustering for streaming text data. We build evolutionary trees in a Bayesian online filtering framework. The tree construction is formulated as an online posterior estimation problem, which considers both the likelihood of the current tree and conditional prior given the previous tree. We also introduce a constraint model to compute the conditional prior of a tree hierarchy in the multi-branch setting. Experiments on real world news data demonstrate that our algorithm can better incorporate historical tree information and is more efficient and effective than the traditional evolutionary hierarchical clustering algorithm.", "authors": [{"affiliation": " Tsinghua University", "name": "Xiting Wang"}, {"affiliation": " HKUST", "name": " Yangqiu Song"}, {"affiliation": " Microsoft Research", "name": " Shixia Liu"}, {"affiliation": " Microsoft Research Asai", "name": " Baining Guo"}], "title": "Mining Evolutionary Multi-Branch Trees from Text Streams"}, "251": {"abstract": "In many applications such as image and video processing, the data matrix often possesses a low-rank structure capturing the global information and a sparse component capturing the local information simultaneously. How to accurately extract the low-rank and sparse components is a major challenge. Robust Principal Component Analysis (RPCA) is a general framework to extract such structures. It is well studied that under certain assumptions, convex optimization using trace norm and $\\ell_1$-norm can be an effective computation surrogate of the difficult RPCA problem. However, it is based on a strong assumption which may not hold in real-world applications, and the approximation error in these convex relaxations often cannot be neglected. In this paper, we present a novel non-convex formulation for the RPCA problem using the capped trace norm and the capped $\\ell_1$-norm. In addition, we present two algorithms to solve the non-convex optimization: one is based on the Difference of Convex functions (DC) framework and the other attempts to solve the sub-problems via a greedy approach. Compared to existing convex formulations, our approach gives proper interpretation and both of the proposed algorithms achieve better accuracy for the RPCA problem which can be verified by empirical results. Furthermore, between the two proposed algorithms, the greedy algorithm is more efficient than the DC programming, while they achieve comparable accuracy.", "authors": [{"affiliation": " Arizona State University", "name": "Qian Sun"}, {"affiliation": " Arizona State University", "name": " Shuo Xiang"}, {"affiliation": " Arizona State University", "name": " Jieping Ye"}], "title": "Robust Principal Component Analysis via Capped Norms"}, "172": {"abstract": "The Internet has enabled the creation of a growing number of large-scale knowledge bases in a variety of domains containing complementary information. Tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of large-scale knowledge bases still poses a considerable challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algorithm for aligning knowledge bases with millions of entities and facts. SiGMa is an iterative propagation algorithm which leverages both the structural information from the relationship graph as well as flexible similarity measures between entity properties in a greedy local search, thus making it scalable. Despite its greedy nature, our experiments indicate that SiGMa can efficiently match some of the world's largest knowledge bases with high accuracy. We provide additional experiments on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art approaches both in accuracy and efficiency. ", "authors": [{"affiliation": " INRIA / ENS", "name": "Simon Lacoste-Julien"}, {"affiliation": " University of Cambridge", "name": " Konstantina Palla"}, {"affiliation": " University of Cambridge", "name": " Alex Davies"}, {"affiliation": " Microsoft Research", "name": " Gjergji Kasneci"}, {"affiliation": " Microsoft Research", "name": " Thore Graepel"}, {"affiliation": " Cambridge University", "name": " Zoubin Ghahramani"}], "title": "SiGMa: Simple Greedy Matching for Aligning Large Knowledge Bases"}, "854": {"abstract": "The problem of point of interest (POI) recommendation is to provide personalized recommendations of places of interests, such as restaurants, for mobile users. Due to its complexity and its connection to location based social networks (LBSNs), the decision process of a user choose a POI is complex and can be influenced by various factors, such as user preferences, geographical influences, and user mobility behaviors. While there are some studies on POI recommendations, it lacks of integrated analysis of the joint effect of multiple factors. To this end, in this paper, we propose a novel geographical probabilistic factor analysis framework which strategically takes various factors into consideration. Specifically, this framework allows to capture the geographical influences on a user's check-in behavior. Also, the user mobility behaviors can be effectively exploited in the recommendation model. Moreover, the recommendation model can effectively make use of user check-in count data as implicity user feedback for modeling user preferences. Finally, experimental results on real-world LBSNs data show that the proposed recommendation method outperforms state-of-the-art latent factor models with a significant margin.", "authors": [{"affiliation": " Rutgers Univ", "name": "Bin Liu"}, {"affiliation": " Rutgers University", "name": " Yanjie Fu"}, {"affiliation": " Rutgers Univ", "name": " ZIjun Yao"}, {"affiliation": " Rutgers, the State University of New Jersey", "name": " Hui Xiong"}], "title": "Learning Geographical Preferences for Point-of-Interest Recommendation"}, "345": {"abstract": "The concern of privacy has become an important issue for online social networks. In services such as Foursquare.com, whether a person likes an article is considered private and cannot be disclosed; only the aggregative statistics of articles (i.e., how many people like this article) is revealed. This paper tries to answer a question: can we predict the opinion holder in a heterogeneous social network without any labeled data? This question can be generalized to an unseen-type link prediction with aggregative statistics problem. This paper devises a novel unsupervised framework to solve this problem, including three main components: (1) a three-layer factor graph model and three types of potential functions; (2) a ranked-margin learning algorithm for parameter tuning; and (3) a two-stage inference algorithm for link prediction. Finally, we evaluate our method on four diverse scenarios using four datasets: preference prediction (Foursquare), repost prediction (Twitter), response prediction (Plurk), and citation prediction (DBLP). We further exploit nine unsupervised models to solve this problem as baseline. Our approach not only wins out in all scenarios, but on the average achieves 6.24% AUC and 9.24% NDCG improvement over the best competitors. The source code and datasets are available at http://www.csie.ntu.edu.tw/~d97944007/aggregative/", "authors": [{"affiliation": " National Taiwan University", "name": "Tsung-Ting Kuo"}, {"affiliation": " Peking University", "name": " Rui Yan"}, {"affiliation": " National Taiwan University", "name": " Yu-Yang Huang"}, {"affiliation": " National Taiwan University", "name": " Perng-Hwa Kung"}, {"affiliation": " National Taiwan University", "name": " Shou-De Lin"}], "title": "Unsupervised Link Prediction Using Aggregative Statistics on Heterogeneous Social Networks"}, "884": {"abstract": "There has recently been a great deal of work focused on developing statistical models of graph structure\u00d1with the goal of modeling probability distributions over graphs from which new, similar graphs can be generated by sampling from the estimated distributions. Although current graph models can capture several important characteristics of social network graphs (e.g., degree, path lengths), many of them do not generate graphs with sufficient variation to reflect the natural variability in real world graph domains. One exception is the Mixed Kronecker Product Graph Model (mKPGM), a generalization of the Kronecker Product Graph Model, which uses parameter tying to capture variance in the underlying distribution. The enhanced representation of mKPGMs enables them to match both the mean graph statistics and their spread as observed in real network populations, but unfortunately to date, the only method to estimate mKPGMs involves an exhaustive search over the parameters. In this work, we present the first learning algorithm for mKPGMs. The O(|E|) algorithm searches over the continuous parameter space using constrained line search and is based on simulated method of moments, where the objective function minimizes the distance between the observed moments in the training graph and the empirically estimated moments of the model. We evaluate the mKPGM learning algorithm by comparing to several different graph models, including KPGMs. We use multidimensional KS distance to compare the generated graphs to the observed graphs and the results show mKPGMs are able to produce a closer match to real-world graphs (10-90% reduction in KS distance), while still providing natural variation in the generated graphs.", "authors": [{"affiliation": " Purdue University", "name": "Sebastian Moreno"}, {"affiliation": " Purdue University", "name": " Jennifer Neville"}, {"affiliation": " Purdue University", "name": " Sergey Kirshner"}], "title": "Learning Mixed Kronecker Product Graph Models with Simulated Method of Moments"}, "730": {"abstract": "Users' daily activities, such as dining and shopping, inherently reflect their habits, intents and preferences, thus provide invaluable information for services such as personalized information recommendation and targeted advertising. Users' activity information, although ubiquitous on social media, has largely been unexploited. This paper addresses the task of user activity classification in microblogs, where users can publish short messages and maintain social networks online. We identify the importance of modeling a user's individuality, and that of exploiting opinions of the user's friends for accurate activity classification. In this light, we propose a novel collaborative boosting framework comprising a text-to-activity classifier for each user, and a mechanism for collaboration between classifiers of users having social connections. The collaboration between two classifiers includes exchanging their own training instances and their dynamically changing labeling decisions. We propose an iterative learning procedure that is formulated as gradient descent in learning function space, while opinion exchange between classifiers is implemented with a weighted voting in each learning iteration. We show through experiments that on real-world data from Sina Weibo, our method outperforms existing off-the-shelf algorithms that do not take users' individuality or social connections into account.", "authors": [{"affiliation": " HKUST", "name": "Yangqiu Song"}, {"affiliation": " Huawei", "name": " Zhengdong Lu"}, {"affiliation": " Huawei", "name": " Cane Wing-Ki Leung"}, {"affiliation": " Hong Kong University of Science and Technology", "name": " Qiang Yang"}], "title": "Collaborative Boosting for Activity Classification in Microblogs"}, "503": {"abstract": "A high quality hierarchical organization of the concepts in a dataset at different levels of granularity has many valuable applications such as search, summarization, and content browsing. In this paper we propose an algorithm for recursively constructing a hierarchy of topics from a collection of content-representative documents. We characterize each topic in the hierarchy by an integrated ranked list of mixed-length phrases. Our mining framework is based on a phrase-centric view for clustering, extracting, and ranking topical phrases. Experiments with datasets from three different domains illustrate our ability to generate hierarchies of high quality topics represented by meaningful phrases.", "authors": [{"affiliation": " University of Illinois", "name": "Chi Wang"}, {"affiliation": " University of Illinois", "name": " Marina Danilevsky"}, {"affiliation": " University of Illinois at Urbana-Champaign", "name": " Nihit Desai"}, {"affiliation": " University of Illinois at Urbana-Champaign", "name": " Yinan Zhang"}, {"affiliation": " University of Illinois at Urbana-Champaign", "name": " Phuong Nguyen"}, {"affiliation": " University of Illinois at Urbana-Champaign", "name": " Thrivikrama Taula"}, {"affiliation": " University of Illinois at Urbana-Champaign", "name": " Jiawei Han"}], "title": "A Phrase Mining Framework for Recursive Construction of a Topical Hierarchy"}, "547": {"abstract": "Recent advances in smart metering technology enable utility companies to have access to tremendous amount of smart meter data, from which the utility companies are eager to gain more insight about their customers. In this paper, we aim to detect electric heat pump from coarse grained smart meter data for a heat pump marketing campaign. However, appliance detection is a challenging task, especially given a very low granularity and partial labeled even unlabeled data. Traditional methods install either sensors at every device or high granularity smart meter, which is either too expensive or requires technical expertise. In this paper, we propose a novel approach to detect heat pump that utilizes low granularity smart meter data, prior sales data and weather data. In particular, motivated by the characteristics of heat pump consumption pattern, we extract novel features that are highly relevant to heat pump usage from smart meter data and weather data. Under the constrain that only a subset of heat pump users are available, we formalize the problem into a positive and unlabeled data classification and apply biased Support Vector Machine (BSVM) to our extracted features. Our empirical study on a real-world data set demonstrates the effectiveness of our method. Furthermore, our method has been deployed in a real-life setting where the partner electric company runs a targeted campaign for 292,496 customers. Based on the initial feedback, our detection algorithm can successfully detect substantial number of heat pump users who were identified non-heat pump users with the prior algorithm the company had used.", "authors": [{"affiliation": " IBM T.J. Watson Research", "name": "Hongliang Fei"}, {"affiliation": " IBM Research", "name": " Younghun Kim"}, {"affiliation": " IBM Research", "name": " Sambit Sahu"}, {"affiliation": " IBM Research", "name": " Millind Naphade"}], "title": "Heat Pump Detection from Coarse Grained Smart Meter Data with Positive and Unlabeled Learning"}, "631": {"abstract": "We present ShoppingAdvisor, a novel recommender system that helps users in shopping technical products in an e-commerce website. ShoppingAdvisor leverages both user preferences and technical product attributes in order to generate its suggestions. The system elicits user preferences via a binary-tree-shaped flowchart, where each node is a question to the user. At each node, ShoppingAdvisor suggests a ranked list of products that match the preferences of the user, and that gets progressively re fined along the path from the root of the tree to one of its leafs. In this paper we show (i) how to learn the structure of the tree, i.e., which questions to ask at each node, and (ii) how to produce a suitable ranking at each node. First, we adapt the classical top-down strategy for building decision trees in order to find the best user attribute to ask at each node. Differently than decision trees, ShoppingAdvisor partitions the user space rather than the product space. Second, we show how to employ a learning-to-rank approach in order to learn, at each node of the tree, a ranking of products appropriate to the users who reach at that node. We experiment with two real-world datasets for cars and cameras, and a synthetic one. We use mean reciprocal rank to evaluate ShoppingAdvisor, and show how the performance increases by more than 50% along the path from root to leaf. We also show how collaborative recommendation algorithms such as k-nearest neighbor benefits from feature selection done by the ShoppingAdvisor tree. Our experiments show that ShoppingAdvisor produces good quality interpretable recommendations, while requiring less input from users and being able to handle the cold-start problem. ", "authors": [{"affiliation": " Univ of Texas at Arlington", "name": "Mahashweta Das"}, {"affiliation": " Yahoo! Research", "name": " Gianmarco De Francisci Morales"}, {"affiliation": " Aalto University", "name": " Aristides Gionis"}, {"affiliation": " Qatar Computing Research Institute", "name": " Ingmar Weber"}], "title": "Learning to question: Leveraging user preferences for shopping advice"}, "756": {"abstract": "Data generated by observing the actions of web browsers across the internet is being used at an ever increasing rate for both building models and making decisions. In fact, a quarter of the industry track papers for KDD in 2012 were based on data generated by online actions. The models, analytics and decisions they inform all stem from the assumption that observed data captures the intent of users. However, a large portion of these observed actions are not intentional, and are effectively polluting the models. Much of this observed activity is either generated by robots traversing the internet or is the result of unintended actions of real users. These non-intentional actions observed in the web logs severely bias both analytics and the models created from the data. In this paper, we will show examples of how non-intentional traffic adversely affects both general analytics and predictive models, and propose an approach using co-visitation networks to identify sites that have large amounts of non-intentional traffic. We will then show how this approach, along with a second stage classifier that identifies non-intentional traffic at the browser level, is deployed in production at Media6Degrees (m6d), a targeting technology company for display advertising. This deployed product acts to both filter out the spurious traffic from the input data and to insure that we don't serve ads during unintended website visits. ", "authors": [{"affiliation": " M6d", "name": "Ori Stitelman"}, {"affiliation": " M6D", "name": " Claudia Perlich"}, {"affiliation": " m6d", "name": " Brian Dalessandro"}, {"affiliation": " m6d", "name": " Rod Hook"}, {"affiliation": " Media6Degrees", "name": " Troy Raeder"}, {"affiliation": " NYU", "name": " Foster Provost"}], "title": "Using Co-visitation Networks For Classifying Non-Intentional Traffic"}, "700": {"abstract": " The role of big data in addressing the needs of the present healthcare system in US and rest of the world has been echoed by government, private, and academic sectors. Recently, the Center of Medicare & Medicaid Services (CMS) and the Oak Ridge National Laboratory (ORNL) have partnered to explore the promise of big data analytics in tapping the potential of the massive healthcare data emanating from CMS and other players in the health insurance field. While the domain implications of such collaboration are well known, this type of data has been explored to a limited extent in the data mining community. The objective of this paper is two fold: first, we introduce the emerging domain of ``big'' healthcare claims data to the KDD community, and second, we describe the success and challenges that we encountered in analyzing this data using state of art analytics for massive data. Specifically, we translate the problem of analyzing healthcare data into some of the most well-known analysis problems in the data mining community, social network analysis, text mining, and temporal analysis and higher order feature construction, and describe how advances within each of these areas can be leveraged to understand the domain of healthcare. Each case study illustrates a unique intersection of data mining and healthcare with a common objective of improving the cost-care ratio by mining for opportunities to improve healthcare operations and reducing what seems to fall under fraud, waste, and abuse.", "authors": [{"affiliation": " Oak Ridge National Laboratory", "name": "Varun Chandola"}], "title": "Knowledge Discovery from Massive Healthcare Claims Data"}, "1041": {"abstract": "Online social networks have become important channels for users to share content with their connections and diffuse information. Although much work has been done to identify socially influential users, the problem of finding ``reputable'' sharers, {\\em who share good content}, has received relatively little attention.  Availability of such reputation scores can be useful for various applications like recommending people to follow, procuring high quality content in a scalable way, creating a content reputation economy to incentivize high quality sharing, and many more. To estimate sharer reputation, it is intuitive to leverage data that records how recipients respond (through clicking, liking, etc.) to content items shared by a sharer.  However, such data is usually biased --- it has a {\\em selection bias} since the shared items can only be seen and responded to by users connected to the sharer in most social networks, and it has a {\\em response bias} since the response is usually influenced by the relationship between the sharer and the recipient (which may not indicate whether the shared content is good).  To correct for such biases, we propose to utilize an additional data source that provides {\\em unbiased} goodness estimates for a small set of shared items, and calibrate biased social data through a novel multi-level hierarchical model that describes how the unbiased data and biased data are jointly generated according to sharer reputation scores. The unbiased data also provides the ground truth for quantitative evaluation of different methods.  Experiments based on such ground-truth data show that our proposed model significantly outperforms existing methods that estimate social influence using biased social data. ", "authors": [{"affiliation": " Stanford University", "name": "Jaewon Yang"}, {"affiliation": " LinkedIn", "name": " Bee-Chung Chen"}, {"affiliation": " LinkedIn", "name": " Deepak Agarwal"}], "title": "Estimating Unbiased Sharer Reputation via Social Data Calibration"}, "165": {"abstract": "An important problem in the non-contractual marketing domain is discovering the customer lifetime and assessing the impact of customer's characteristic variables on the lifetime. Unfortunately, the conventional hierarchical Bayes model cannot discern the impact of customer's characteristic variables for each customer. To overcome this problem, we present a new survival model using a non-parametric Bayes paradigm with MCMC. The assumption of a conventional model, logarithm of purchase rate and dropout rate with linear regression, is extended to include our assumption of the Dirichlet Process Mixture of regression. The extension assumes that each customer belongs probabilistically to different mixtures of regression, thereby permitting us to estimate a different impact of customer characteristic variables for each customer. Our model creates several customer groups to mirror the structure of the target data set. The effectiveness of our proposal is confirmed by a comparison involving a real e-commerce transaction dataset and an artificial dataset; it generally achieves higher predictive performance. In addition, we show that preselecting the actual number of customer groups does not always lead to higher predictive performance.", "authors": [{"affiliation": " NTT", "name": "Shouichi Nagano"}, {"affiliation": " NTT", "name": " Yusuke Ichikawa"}, {"affiliation": " NTT", "name": " Noriko Takaya"}, {"affiliation": " NTT", "name": " Tadasu Uchiyama"}, {"affiliation": " Tokyo University", "name": " Makoto Abe"}], "title": "Nonparametric Hierarchal Bayesian Modeling in Non-contractual Heterogeneous Survival Data"}, "166": {"abstract": "Approximation of non-linear kernels using random feature mapping has been successfully employed in large-scale data analysis applications, accelerating the training of kernel machines. While previous random feature mappings run in $O(ndD)$ time for $n$ training samples in $d$-dimensional space and $D$ random feature maps, we propose a novel randomized tensor product technique, called \\textit{Tensor Sketching}, for approximating any polynomial kernel in $O(n(d+D \\log{D}))$ time. Also, we introduce both \\textit{absolute} and \\textit{relative} error bounds for our approximation to guarantee the reliability of our estimation algorithm. Empirically, Tensor Sketching achieves higher accuracy and often runs orders of magnitude faster than the state-of-the-art approach for large-scale real-world datasets.", "authors": [{"affiliation": " IT University of Copenhagen", "name": "Ninh Pham"}, {"affiliation": " IT University of Copenhagen", "name": " Rasmus Pagh"}], "title": "Fast and Scalable Polynomial Kernels via Explicit Feature Maps"}, "95": {"abstract": "This paper introduces a nonlinear logistic regression model for classification. The main idea is to map the data to a feature space based on kernel density estimation. A discriminative model is then learned to optimize the feature weights as well as the bandwidth of a Nadaraya-Watson kernel density estimator. We then propose a hierarchical optimization algorithm for learning the coefficients and kernel bandwidths in an integrated way. Compared to other nonlinear models such as kernel logistic regression (KLR) and SVM, our approach is far more efficient since it solves an optimization problem with a much smaller size. Two other major advantages are that it can cope with categorical attributes in a unified fashion and naturally handle multi-class problems. Moveover, our approach inherits from logistic regression good interpretability of the model, which is important for clinical applications but not offered by KLR and SVM. Extensive results on real datasets, including a clinical prediction application currently under deployment in a major hospital, show that our approach not only achieves superior classification accuracy, but also drastically reduces the computing time as compared to other leading methods.", "authors": [{"affiliation": " Yixin Chen, Washington University in St Louis", "name": "Wenlin Chen"}, {"affiliation": " Baolong Guo, Xidian University", "name": " Yi Mao"}], "title": "Density-Based Logistic Regression"}, "10": {"abstract": "Multi-label classification is prevalent in many real-world applications, where each example can be associated with a set of multiple labels simultaneously. The key challenge of multi-label classification comes from the large space of all possible label sets, which is exponential to the number of candidate labels. Most previous work focuses on exploiting correlations among different labels to facilitate the learning process. It is usually assumed that the label correlations are either given beforehand or can be derived directly from data samples by counting their label co-occurrences. However, in many real-world tasks, the label correlations are neither given, nor feasible to learn directly from data samples of moderate sizes. Heterogeneous information networks can provide abundant knowledge about relationships among different types of entities including data samples and label concepts. In this paper, we propose to use heterogeneous information networks to facilitate the multi-label classification process. By mining the linkage structure of heterogeneous information networks, multiple types of relationships among different label concepts and the data samples can be extracted. Then we can use these relationships to effectively induce the correlations among different class labels in general, as well as the dependencies among the label sets of inter-connected data examples. Empirical studies on real-world tasks demonstrate that the performance of multi-label classification can be effectively boosted using heterogeneous information networks.", "authors": [{"affiliation": " Univ. of Illinois at Chicago", "name": "Xiangnan Kong"}, {"affiliation": " Renmin University of China", "name": " Bokai Cao"}, {"affiliation": " University of Illinois", "name": " Philip Yu"}], "title": "Multi-Label Classification by Mining Label and Instance Correlations from Heterogeneous Information Networks"}, "960": {"abstract": "This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations\u00d5 information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users\u00d5 computers to detect independently developed red team inserts of malicious insider activities. Our system and project are called PRODIGAL, for PROactive Detection of Insider Threats with Graph Analysis and Learning. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, on indicators of unusual activities, on high-dimensional statistical patterns, on temporal sequences, and on normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect possible instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5M actions per day from ~5500 users over a calendar month of activity. We have achieved AUCs of up to 0.979 and lift values of 65 on the top 50 user-days identified.", "authors": [{"affiliation": " SAIC", "name": "Ted Senator"}, {"affiliation": " SAIC", "name": " Henry Goldberg"}, {"affiliation": "", "name": " Alex Memory"}, {"affiliation": " SAIC", "name": " William Young"}, {"affiliation": "", "name": " Bradley Rees"}, {"affiliation": " SAIC", "name": " Robert Pierce"}, {"affiliation": " SAIC", "name": " Danel Huang"}, {"affiliation": " SAIC", "name": " Matthew Reardon"}, {"affiliation": " Ga Tech", "name": " David Bader"}, {"affiliation": " Ga Tech", "name": " Edmond Chow"}, {"affiliation": " Georgia Institute of Technology", "name": " Irfan Essa"}, {"affiliation": " Ga Tech", "name": " Joshua Jones"}, {"affiliation": " Georgia Institute of Technology", "name": " Vinay Bettadapura"}, {"affiliation": " Georgia Institute of Technology", "name": " Duen Horng Chau"}, {"affiliation": " Ga Tech", "name": " Oded Green"}, {"affiliation": " Ga Tech", "name": " Oguz Kaya"}, {"affiliation": " Ga Tech", "name": " Anita Zakrzewska"}, {"affiliation": " GTRI", "name": " Erica Briscoe"}, {"affiliation": " GTRI", "name": " Rudolph Mappus IV"}, {"affiliation": " GTRI", "name": " Robert McColl"}, {"affiliation": " GTRI", "name": " Lora Weiss"}, {"affiliation": " Oregon State University", "name": " Thomas Dietterich"}, {"affiliation": " Oregon State University", "name": " Alan Fern"}, {"affiliation": " Oregon State University", "name": " Weng-Keen Wong"}, {"affiliation": " Oregon State University", "name": " Shubhomoy Das"}, {"affiliation": " Oregon State University", "name": " Andrew Emmott"}, {"affiliation": " Oregon State University", "name": " Jed Irvine"}, {"affiliation": " CMU", "name": " Jay-Yoon Lee"}, {"affiliation": " Carnegie Mellon University", "name": " Danai Koutra"}, {"affiliation": " CMU", "name": " Christos Faloutsos"}, {"affiliation": " University of Massachusetts", "name": " Daniel Corkill"}, {"affiliation": " University of Massachusetts", "name": " Lisa Friedland"}, {"affiliation": " University of Massachusetts", "name": " Amanda Gentzel"}, {"affiliation": " Univ of Massachusetts Amherst", "name": " David Jensen"}], "title": "Detecting Insider Threats in a Real Corporate Database of Computer Usage Activities"}, "963": {"abstract": "Named entity disambiguation is the task of disambiguating named entity mentions in natural language text to their corresponding entries in a knowledge base such as Wikipedia. Such disambiguation can help enhance readability and add semantics to plain text.  It is also a central step in constructing high-quality information network or knowledge graph from unstructured text.  Previous research has tackled this problem by making use of various textual and structural features from a knowledge base.  Most of the proposed algorithms assume that a knowledge base can provide enough explicit and useful information to help disambiguate a mention to the right entity.  However, the existing knowledge bases are rarely complete (likely will never be), thus leading to poor performance on short queries with not well-known contexts.  In such cases, we need to collect additional evidences scattered in internal and external corpus to augment the knowledge bases and enhance their disambiguation power.  In this work, we propose a generative model and an incremental algorithm to automatically mine useful evidences across documents.  With a specific modeling of \"background topic\" and \"unknown entities\", our model is able to harvest useful evidences without introducing noisy information. Experimental results show that our proposed method outperforms the state-of-the-art approaches significantly: boosting the disambiguation accuracy from 43% (baseline) to 86% on short queries derived from tweets.", "authors": [{"affiliation": " University of California Santa Barbara", "name": "Yang Li"}, {"affiliation": " University of Illinois", "name": " Chi Wang"}, {"affiliation": " University of California Santa Barbara", "name": " Fangqiu Han"}, {"affiliation": " University of Illinois at Urbana-Champaign", "name": " Jiawei Han"}, {"affiliation": " UIUC", "name": " Dan Roth"}, {"affiliation": " University of California at Santa", "name": " Xifeng Yan"}], "title": "Mining Evidences for Named Entity Disambiguation"}, "1186": {"abstract": "A/B testing is a standard approach for evaluating the effect of online experiments; the goal is to estimate the `average treatment effect' of a new feature or condition by exposing a sample of the overall population to it. A drawback with A/B testing is that it is poorly suited for experiments involving social interference, when the treatment of individuals spills over to neighboring individuals along an underlying social network. In this work, we propose a novel methodology for using graph clustering to analyze average treatment effects under social interference. To begin, we characterize graph-theoretic conditions under which individuals can be considered to be `network exposed' to an experiment. We then show how clustered graph randomization admits an efficient exact algorithm to compute the probabilities for each vertex being network exposed under several of these exposure conditions. Using these probabilities as inverse weights, a Horvitz-Thompson estimator can then provide an effect estimate that is unbiased, provided that the exposure model has been properly specified.   Given an estimator that is unbiased, we focus on minimizing the variance. First, we develop simple sufficient conditions for the variance of the estimator to be asymptotically small in n, the size of the graph. However, for general randomization schemes, this variance can be lower bounded by an exponential function of the degrees of a graph. In contrast, we show that if a graph satisfies a condition on the growth rate of neighborhoods, then there exists a natural clustering algorithm, based on vertex neighborhoods, for which the variance of the estimator can be upper bounded by a linear function of the degrees. Thus we show that proper cluster randomization can lead to exponentially lower estimator variance when experimentally measuring average treatment effects under interference.", "authors": [{"affiliation": " Cornell University", "name": "Johan Ugander"}, {"affiliation": " Facebook", "name": " Brian Karrer"}, {"affiliation": " Facebook", "name": " Lars Backstrom"}, {"affiliation": " Cornell", "name": " Jon Kleinberg"}], "title": "Clustered Graph Randomization: Network Exposure to Multiple Universes"}, "18": {"abstract": "Understanding and quantifying the impact of unobserved processes in the multivariate time series data is a major challenge in analysis of multivariate stochastic processes. In this paper, we analyze a flexible stochastic process model, the generalized linear auto-regressive process (GLARP) and identify the conditions with which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood. To illustrate the power of this framework, we examine three examples, including two popular models for count data, i.e, Poisson and Conwey-Maxwell Poisson vector auto-regressive processes, and one powerful but less studies model for continuous data, i.e., Pareto vector auto-regressive processes for heavy-tailed time series data. We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it. Experiment results on two synthetic datasets, one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models.", "authors": [{"affiliation": " University of Southern Califor", "name": "Mohammad Taha Bahadori"}, {"affiliation": " University of Southern California", "name": " Yan Liu"}, {"affiliation": " CMU", "name": " Eric Xing"}], "title": "Fast Structure Learning in Generalized Stochastic Processes with Latent Factors"}, "863": {"abstract": "The area under the ROC curve (AUC) is a well known performance measure in machine learning and data mining. In an increasing number of applications, however, ranging from ranking applications to a variety of important bioinformatics applications, performance is measured in terms of the partial area under the ROC curve between two specified false positive rates. In recent work, we proposed a structural SVM based approach for optimizing this performance measure (Narasimhan and Agarwal, 2013). In this paper, we develop a new support vector method, SVM_{pAUC}^{tight}, that optimizes a tighter convex upper bound on the partial AUC loss, which leads to both improved accuracy and reduced computational complexity. In particular, by rewriting the empirical partial AUC risk as a maximum over subsets of negative instances, we derive a new formulation, where a modified form of the earlier optimization objective is evaluated on each of these subsets, leading to a tighter hinge relaxation on the partial AUC loss. As with our previous method, the resulting optimization problem can be solved using a cutting-plane algorithm, but the new method has better run time guarantees. We also discuss a projected subgradient method for solving this problem, which offers additional computational savings in certain settings. We demonstrate on a wide variety of bioinformatics tasks, ranging from protein-protein interaction prediction to drug discovery tasks, that the proposed method does, in many cases, perform significantly better on the partial AUC measure than the previous structural SVM approach. In addition, we also develop extensions of our method to learn sparse and group sparse models, often of interest in biological applications.", "authors": [{"affiliation": " Indian Institute of Science", "name": "Harikrishna Narasimhan"}, {"affiliation": " Indian Institute of Science, Bangalore", "name": " Shivani Agarwal"}], "title": "SVM_{pAUC}^{tight}: A New Support Vector Method for Optimizing Partial AUC Based on a Tight Convex Upper Bound"}, "724": {"abstract": "Professional sports is a roughly $500 billion dollar industry that is increasingly data-driven.  In this paper we show how machine learning can be applied to generate a model that would lead to better on-field decisions by managers of professional baseball teams. Specifically we show how to use regularized linear regression to learn pitcher-specific predictive models that can be used to decide when a starting pitcher should be replaced. A key step in the process is our method of converting categorical variables (e.g., the venue in which a game is played) into continuous variables suitable for the regression. Another key step is dealing with situations in which there is an insufficient amount of data to compute measures such as the effectiveness of a pitcher against specific batters.  For each season we trained on the first 80% of the games, and tested on the rest. The results suggest that using our model would have led to better decisions than those made by major league managers. Applying our model would have led to a different decision 48% of the time.  For those games in which a manager left a pitcher in that our model would have removed, the pitcher ended up performing poorly 60% of the time.", "authors": [{"affiliation": " Massachusetts Institute of Tec", "name": "Gartheeban Ganeshapillai"}, {"affiliation": " Massachusetts Institute of Technology", "name": " John Guttag"}], "title": "A Data-driven Method for In-game Decision Making in MLB"}, "722": {"abstract": "Social media responses to news have increasingly gained in importance as they can enhance a consumer's news reading experience, promote information sharing and aid journalists in assessing their readership's response to a story. Given that the number of responses to an online news article may be huge, a common challenge is that of selecting only the most interesting responses for display.  This paper addresses this challenge by casting message selection as an optimization problem. We define an objective function which jointly models the messages' utility scores and their entropy. We propose a near-optimal solution to the underlying optimization problem which  leverages the submodularity property of the objective function. Our solution first learns the utility of individual messages in isolation and then produces a diverse selection of interesting messages by maximizing the defined objective function.  The intuition behind our work is that an interesting selection of messages contains diverse, informative, opinionated and popular messages referring to the news article, written mostly by users that have authority on the topic. Our intuitions are embodied by a rich set of content, social and user features capturing the aforementioned aspects. We evaluate our approach through both human and automatic experiments, and demonstrate it outperforms the state of the art.  Additionally, we perform an in-depth analysis of the annotated ``interesting'' responses, shedding light on the subjectivity around the selection process and the perception of interestingness.", "authors": [{"affiliation": " Jo_ef Stefan Institute", "name": "Tadej _tajner"}, {"affiliation": " Yahoo! Research", "name": " Bart Thomee"}, {"affiliation": " Yahoo! Labs", "name": " Ana Maria Popescu"}, {"affiliation": " eBay Inc.", "name": " Marco Pennacchiotti"}, {"affiliation": " Yahoo!", "name": " Alejandro Jaimes"}], "title": "Automatic selection of social media responses to news"}, "720": {"abstract": "Latent topic models have played a pivotal role in analyzing large collections of complex data ranging from text documents to social networks. Besides discovering latent semantic structures, supervised topic models can make predictions on unseen test data. By marrying with advanced machine learning techniques, the predictive strengths of supervised topic models have been dramatically enhanced, such as max-margin supervised topic models, a state-of-the-art supervised topic model that integrates max-margin learning with probabilistic topic models. Though powerful, max-margin supervised topic models have a hard non-smooth learning problem. Existing algorithms rely on solving multiple latent SVM subproblems in an EM-type procedure, which can be too slow to be applicable to large-scale text categorization tasks.  In this paper, we present a highly scalable approach to building max-margin supervised topic models. Our approach builds on three key innovations: 1) {\\it a new formulation of  Gibbs max-margin supervised topic models} for both multi-class and multi-label classification; 2) {\\it a simple ``augment-and-collapse\" Gibbs sampling algorithm} without making restricting assumptions on the posterior distributions; 3) {\\it an efficient parallel implementation} that can easily tackle data sets with hundreds of categories and millions of documents. Furthermore, our algorithm does not need to solve SVM subproblems. Though performing the two tasks of topic discovery and learning predictive models jointly, which significantly improves the classification performance, our methods have comparable scalability as the state-of-the-art parallel algorithms for the standard LDA topic models which perform the single task of topic discovery only. Finally, an open-source implementation is also provided.", "authors": [{"affiliation": " Tsinghua University", "name": "Jun Zhu"}, {"affiliation": " Beihang University", "name": " Xun Zheng"}, {"affiliation": " Tsinghua University", "name": " Li Zhou"}, {"affiliation": " Tsinghua University", "name": " Zhang Bo"}], "title": "Scalable Inference in Max-margin Supervised Topic Models"}, "1108": {"abstract": "Even with massive experiments, estimators of important quantities, such as a difference in means between conditions, can have substantial variance that in part arises from dependence in observations. For example, in online advertising, ad impressions that have a user or an ad in common are likely associated. Previous mathematical and simulation results demonstrate that not accounting for this dependence structure can result in anti-conservative tests and confidence intervals.  We examine how bootstrap methods that account for differing levels of dependence structure perform in practice. We use multiple real data sets describing user behaviors on Facebook to generate data for experiments in which there is no effect of the treatment on average and then estimate empirical Type I error rates for each method. We supplement results from these real data sets with realistic simulations. Accounting for dependence within a single type of unit (within-user dependence) is often sufficient to get reasonable error rates. But when the sharp null hypothesis is false (as one might expect from experiments with effects), accounting for multiple units with a multiway bootstrap can be necessary to get close to the advertised Type I error rates. This work provides guidance to experimenters on calibrating large-scale evaluation systems, and highlights the importance of analysis of inferential methods under conditions other than the sharp null hypothesis.", "authors": [{"affiliation": " Facebook", "name": "Eytan Bakshy"}, {"affiliation": " Facebook", "name": " Dean Eckles"}], "title": "Dependence and Uncertainty in Large Experiments: An Evaluation of Bootstrap Methods"}, "748": {"abstract": " Social media is a platform for people to share and vote content. From the analysis of the social media data we found that users are quite inactive in rating/voting. For example, a user on average only votes 2 out of 100 accessed items. Traditional recommendation methods are mostly based on users' votes and thus can not cope with this situation. Based on the observation that the dwell time on an item may reflect the opinion of a user, we aim to enrich the user-vote matrix by converting the dwell time on items into users' ``pseudo votes'' and then help improve recommendation performance. However, it is challenging to correctly interpret the dwell time since many subjective human factors, e.g. user expectation, sensitivity to various item qualities, reading speed, are involved into the casual behavior of online reading. In psychology, it is assumed that people have choice threshold in decision making. The time spent on making decision reflects the decision maker's threshold. This idea inspires us to develop a View-Voting model, which can estimate how much the user likes the viewed item according to her dwell time, and thus make recommendations even if there is no voting data available. Finally, our experimental evaluation shows that the traditional rate-based recommendation's performance is greatly improved with the support of VV model.", "authors": [{"affiliation": " Pennsylvania State University", "name": "Peifeng Yin"}, {"affiliation": " HP Lab", "name": " Ping Luo"}, {"affiliation": " ", "name": " Wang-Chien Lee"}, {"affiliation": " Google Research", "name": " Min Wang"}], "title": "Silence is also evidence: Interpreting dwell time for recommendation from Psychological Perspective"}, "1167": {"abstract": "Many applications concern sparse signals, for example, detecting anomalies from the differences between consecutive images taken by surveillance cameras. This paper focuses on the problem of  recovering a  $K$-sparse signal $\\mathbf{x}\\in\\mathbb{R}^{1\\times N}$, i.e., $K\\ll N$ and $\\sum_{i=1}^{N} 1\\{x_i\\neq 0\\} = K$. In the  mainstream framework of compressed sensing (CS), $\\mathbf{x}$ is recovered from $M$  linear measurements $\\mathbf{y} = \\mathbf{xS}\\in\\mathbb{R}^{1\\times M}$, where $\\mathbf{S}\\in\\mathbb{R}^{N\\times M}$ is often a Gaussian (or Gaussian-like) design matrix.  In our proposed method, the design matrix $\\mathbf{S}$ is generated from an $\\alpha$-stable distribution with $\\alpha\\approx 0$. Our decoding algorithm  mainly requires  one linear scan of the coordinates, followed by a few iterations on a small number of coordinates which are ``undetermined'' in the previous iteration. Our practical algorithm consists of two estimators. In the first iteration, the {\\em (absolute) minimum estimator} is able to  filter out a majority of the zero coordinates. The {\\em gap estimator}, which is applied in each iteration, can accurately recover the magnitudes of the nonzero coordinates.  Comparisons with linear programming (LP) and orthogonal matching pursuit (OMP) demonstrate that our algorithm can be significantly faster in  decoding speed and more accurate in recovery quality, for the task of exact spare recovery. Our procedure is robust against measurement noise. Even when there are no sufficient measurements,  our algorithm can still reliably recover a significant portion of the nonzero coordinates.    ", "authors": [{"affiliation": " Cornell University", "name": "Ping Li"}, {"affiliation": " Rutgers University", "name": " Cun-Hui Zhang"}], "title": "Exact Sparse Recovery with L0 Projections"}, "1162": {"abstract": "Many data sets contain rich information about objects, as well as pairwise relations between them.  For instance, in networks of websites, scientific papers, and other documents, each node has content consisting of a collection of words, as well as hyperlinks or citations to other nodes. In order to perform inference on such data sets, and make predictions and recommendations, it is useful to have models that are able to capture the processes which generate the text at each node and the links between them. In this paper, we combine classic ideas in topic modeling with a variant of the mixed-membership block model recently developed in the statistical physics community.  The resulting model has the advantage that its parameters, including the mixture of topics of each document and the resulting overlapping communities, can be inferred with a simple and scalable expectation-maximization algorithm. We test our model on three data sets, performing unsupervised topic classification and link prediction. For both tasks, our model outperforms several existing state-of-the-art methods, achieving higher accuracy with significantly less computation, analyzing a data set with 1.3 million words and 44 thousand links in a few minutes.", "authors": [{"affiliation": " University of New Mexico", "name": "YAOJIA ZHU"}, {"affiliation": " University of New Mexico", "name": " Xiaoran Yan"}, {"affiliation": " The University of Maryland College Park", "name": " Lise Getoor"}, {"affiliation": " Santa Fe Institute", "name": " Cristopher Moore"}], "title": "Scalable Text and Link Analysis with Mixed-Topic Link Models"}, "752": {"abstract": "The network inference problem consists of reconstructing the edge set of a network given traces representing the chronology of infection times as epidemics spread through the network. This problem is a paradigmatic representative of prediction tasks in machine learning that require deducing a latent structure from observed patterns of activity in a network, which often require an unrealistically large number of resources (e.g., amount of available data, or computational time). A fundamental question is to understand which properties we can predict with a reasonable degree of accuracy with the available resources, and which we cannot. We define the trace complexity as the number of distinct traces required to achieve high fidelity in reconstructing the topology of the hidden network or, more generally, some of its properties. We give simpler and more efficient algorithms for the objective of perfect reconstruction with high probability. Moreover, we prove that our algorithms are nearly optimal, by proving an information-theoretic lower bound on the number of traces that an optimal reconstruction algorithm requires for performing this task in the worst case. Given these strong lower bounds, we turn our attention to special cases, such as trees and bounded-degree graphs, and to property recovery tasks, such as reconstructing the degree distribution of an unobserved network. We show that these problems require a much smaller (and more realistic) number of traces, making them potentially solvable in practice.", "authors": [{"affiliation": " Cornell", "name": "Bruno Abrahao"}, {"affiliation": " Sapienza University", "name": " Flavio Chierichetti"}, {"affiliation": " Cornell", "name": " Robert Kleinberg"}, {"affiliation": " Sapienza University of Rome", "name": " Alessandro Panconesi"}], "title": "Trace Complexity of Network Inference"}, "1205": {"abstract": "Sponsored search thrives to be the major business model for commercial search engines. Precise predicting of the probability that users click on ads plays a key role in sponsored search, since such prediction is widely used in ranking, filtering, and pricing of the ads. Many of previous studies on click prediction usually apply the machine learning approach, and take advantage of two major kinds of features, including semantic features representing relevance between ads and queries as well as historical click-through features. These existing works, however, ignored one important aspect, the commercial nature, of the sponsor search. They make little attempt to understand user clicks on ads from the perspective of consumer psychology. In this work, through a data analysis on a commercial search engine, we find that many ads in sponsored search usually leverage some specific text patterns, e.g, \u00d2official site\u00d3, \u00d2x% off\u00d3, \u00d2guaranteed return in x days\u00d3, to attract consumer psychological desires, , and those text patterns can give rise to significant difference in terms of click-through rate. Based on these observations, it becomes an important problem of how to explore the consumer psychological desires to improve the accuracy of click prediction in sponsored search. To address this issue, we first propose a method to automatically mine patterns representative for  the consumer psychological desires from ads' text. Based on extracted text patterns, we propose new features and incorporate them into the learning framework of click prediction in sponsored search. Large scale evaluations on the click-through log from a commercial search engine demonstrate that our proposed consumer psychological desire features can result in significant improvement in terms of accuracy of click prediction in sponsored search. ", "authors": [{"affiliation": " Microsoft", "name": "Taifeng Wang"}, {"affiliation": " ", "name": " Jiang Bian"}, {"affiliation": " Microsoft Research", "name": " Tie-Yan Liu"}], "title": "Exploring Consumer Psychology for Click Prediction in Sponsored Search"}, "554": {"abstract": "Cascades are ubiquitous in various network environments such as epidemic networks, traffic networks, water distribution networks and social networks. The outbreaks of cascades will often bring bad or even devastating effects. How to accurately predict the cascading outbreaks in early stage is of paramount importance for people to avoid these bad effects. Although there have been some pioneering works on cascading outbreaks detection, how to predict, rather than detect, the cascading outbreaks is still an open problem. In this paper, we attempt harnessing historical cascade data, propose a novel data driven approach to select important nodes as sensors, and predict the outbreaks based on the cascading behaviors of these sensors. In particular, we propose Orthogonal Sparse LOgistic Regression (OSLOR) method to jointly optimize node selection and outbreak prediction, where the prediction loss are combined with an orthogonal regularizer and L1 regularizer to guarantee good prediction accuracy and the sparsity and low-redundancy of selected sensors. We evaluate the proposed method on a real online social network dataset including 182.7 million information cascades. The experimental results show that the proposed OSLOR significantly and consistently outperform topological measure based method and other data driven methods in prediction performances.", "authors": [{"affiliation": " Tsinghua University", "name": "Shifei JIN"}, {"affiliation": " Tsinghua University", "name": " Peng Cui"}, {"affiliation": " Tsinghua University", "name": " Linyun Yu"}, {"affiliation": " IBM T. J. Watson Research Lab", "name": " Fei Wang"}, {"affiliation": " Tsinghua University", "name": " Shiqiang Yang"}], "title": "Cascading Outbreak Prediction in Networks: A Data-Driven Approach"}, "550": {"abstract": "Given the vast amount of information on theWorldWideWeb, recommender systems are increasingly being used to help filter irrelevant data and suggest information that would interest users. Traditional systems make recommendations based on a single domain e.g., movie or book domain. Recent work examine the correlations in different domains and design models that exploit user preferences on a source domain to predict user preferences on a target domain. However, these methods are based on matrix factorization and can only be applied to two-dimensional data. Transferring high dimensional data from one domain to another requires decomposing the high dimensional data to binary relations which results in information loss. Furthermore, this decomposition creates a large number of matrices that need to be transferred and combining them in the target domain is non-trivial. Separately, researchers have looked into using social network information to improve recommendation. However, this social network information has not been explored in cross domain collaborative filtering. In this work, we propose a generalized cross domain collaborative filtering framework that integrates social network information seamlessly with cross domain data. This is achieved by utilizing tensor factorization with topic based social regularization. This framework is able to transfer high dimensional data without the need for decomposition by finding shared implicit cluster-level tensor from multiple domains. Extensive experiments conducted on real world datasets indicate that the proposed framework outperforms state-of-art algorithms for item recommendation, user recommendation and tag recommendation.", "authors": [{"affiliation": " National University of Singapore", "name": "Wei Chen"}, {"affiliation": " National University of Singapore", "name": " Wynne Hsu"}, {"affiliation": " National University of Singapore", "name": " Mong-Li Lee"}], "title": "Making Recommendations from Multiple Domains"}, "1202": {"abstract": "Understanding how research themes evolve over time in a research community is useful in many ways (e.g., revealing important milestones and discovering emerging major research trends).  In this paper, we propose a novel way of analyzing literature citation to explore the research topics and the theme evolution by modeling article citation relations with a probabilistic generative model.  The key idea is to represent a research paper by a ``bag of citations'' and model such a ``citation document'' with a probabilistic topic model. We explore the extension of a particular topic model, i.e., Latent Dirichlet Allocation~(LDA), for citation analysis, and show that such a Citation-LDA can facilitate discovering of  individual research topics as well as the theme evolution from multiple related topics, both of which in turn lead to the construction of evolution graphs for characterizing research themes.  We test the proposed citation-LDA on two datasets: the ACL Anthology Network~(AAN) of natural language research literatures and PubMed Central~(PMC) archive of biomedical and life sciences literatures, and demonstrate that Citation-LDA can effectively discover evolution of research themes, with better formed topics than (conventional) Content-LDA.", "authors": [{"affiliation": " UIUC", "name": "Xiaolong Wang"}, {"affiliation": " UIUC", "name": " ChengXiang Zhai"}, {"affiliation": " UIUC", "name": " Dan Roth"}], "title": "Understanding Evolution of Research Themes: A Probabilistic Generative Model for Citations"}, "235": {"abstract": "Friending recommendation has successfully contributed to the explosive growth of on-line social networks. Most friending recommendation services today aim to support passive friending, where a user passively selects friending targets from the recommended candidates. In this paper, we advocate recommendation support for active friending, where a user actively specifies a friending target. To the best of our knowledge, a recommendation designed to provide guidance for a user to systematically approach his friending target, has not been explored in existing on-line social networking services. To maximize the probability that the friending target would accept an invitation from the user, we formulate a new optimization problem, namely, Acceptance Probability Maximization (APM), and develop a polynomial time algorithm, called Selective Invitation with Tree and In-Node Aggregation (SITINA), to find the optimal solution. We implement an active friending service with SITINA in Facebook to validate our idea. Our user study and experimental results manifest that SITINA outperforms manual selection and the baseline approach in solution quality efficiently.", "authors": [{"affiliation": " Academia Sinica", "name": "De-Nian Yang"}, {"affiliation": " Academia Sinica", "name": " Hui-Ju Hung"}, {"affiliation": " ", "name": " Wang-Chien Lee"}, {"affiliation": " Microsoft Research", "name": " Wei Chen"}], "title": "Maximizing Acceptance Probability for Active Friending in On-Line Social Networks"}, "1199": {"abstract": "In the internet era there has been an explosion in the amount of digital text information available, leading to difficulties of scale for traditional inference algorithms for topic models.  Recent advances in stochastic variational inference algorithms for latent Dirichlet allocation (LDA) have made it feasible to learn topic models on large-scale corpora, but these methods do not currently take full advantage of the collapsed representation of the model.  We propose a stochastic algorithm for collapsed variational Bayesian inference for LDA, which is simpler and more efficient than the state of the art method.  In experiments on large-scale text corpora, the algorithm was found to converge faster and often to a better solution than the previous method.  Human-subject experiments also demonstrated that the method can learn coherent topics in seconds on small corpora, facilitating the use of topic models in interactive document analysis software.", "authors": [{"affiliation": " UC Irvine", "name": "James Foulds"}, {"affiliation": " UC Irvine", "name": " Levi Boyles"}, {"affiliation": " UC Irvine", "name": " Christopher Dubois"}, {"affiliation": " UC Irvine", "name": " Padhraic Smyth"}, {"affiliation": " University of Amsterdam", "name": " max Welling"}], "title": "Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation"}, "1052": {"abstract": "Genome-wide association studies (GWAS) have become a popular method for analyzing sets of DNA sequences in order to discover the genetic basis of disease.  Unfortunately, statistics published as the result of GWAS can be used to identify individuals participating in the study. To prevent privacy breaches, even previously published results have been removed from public databases, impeding researchers' access to the data and hindering collaborative research.  Existing techniques for privacy-preserving GWAS focus on answering specific questions, such as correlations between a given pair of SNPs (DNA sequence variations). This does not fit the typical GWAS process, where the analyst may not know in advance which SNPs to consider and which statistical tests to use, how many SNPs are significant for a given dataset, etc.  We present a set of practical, privacy-preserving data mining algorithms for GWAS datasets.  Our framework supports \\emph{exploratory} data analysis, where the analyst does not know a priori how many and which SNPs to consider.  We develop privacy-preserving algorithms for computing the number and location of SNPs that are significantly associated with the disease, the significance of any statistical test between a given SNP and the disease, any measure of correlation between SNPs and the block structure of correlations.  We evaluate our algorithms on real-world datasets and demonstrate that they produce significantly more accurate results than prior techniques while guaranteeing differential privacy.", "authors": [{"affiliation": " U.S. Naval Research Laboratory", "name": "Aaron Johnson"}, {"affiliation": " The University of Texas at Austin", "name": " Vitaly Shmatikov"}], "title": "Privacy-Preserving Data Exploration in Genome-Wide Association Studies"}, "1192": {"abstract": "Partitioning large graphs is difficult, especially when performed in the limited models of computation afforded to modern large scale computing systems. In this work we introduce restreaming graph partitioning and develop algorithms that scale similarly to streaming partitioning algorithms yet empirically perform as well as fully offline algorithms. In streaming partitioning, graphs are partitioned serially in a single pass. Restreaming partitioning is motivated by scenarios where approximately the same dataset is routinely streamed, making it possible to transform streaming partitioning algorithms into an iterative procedure. This combination of simplicity and powerful performance allows restreaming algorithms to be easily adapted to efficiently tackle more challenging partitioning objectives. In particular, we consider the problem of stratified graph partitioning, a multi-constrained partitioning problem where each of many node strata require balancing. Stratified partitioning is motivated by the challenge of studying network effects on social networks, where it is desirable to isolate disjoint dense subgraphs with matching user demographics. As such, we partition large social networks such that all partition subgraphs exhibit matching degree distributions --- a novel achievement for non-regular graphs. We again observe that restreaming partitioning can match fully offline approaches while utilizing only a fraction of the memory. As part of our results, we observe a fundamental difference in the ease with which social graphs are partitioned when compared to web graphs. Namely, the modular structure of web graphs appears to motivate full offline optimization, whereas the locally dense structure of social graphs precludes significant gains from global manipulations. Indeed, it is on the increasingly important, and increasingly large social graphs that restreaming graph partition algorithms outperform their complex offline competitors.", "authors": [{"affiliation": " Cornell University", "name": "Joel Nishimura"}, {"affiliation": " Cornell University", "name": " Johan Ugander"}], "title": "Restreaming Graph Partitioning: Simple Versatile Algorithms for Advanced Balancing"}, "470": {"abstract": "Online recruiting systems have gained immense attention in the wake of more and more job seekers searching jobs and enterprises finding candidates on the Internet. A critical problem in a recruiting system is how to maximally satisfy the desires of both job seekers and enterprises with reasonable recommendations or search results. In this paper, we investigate and compare various online recruiting systems from a product perspective. We then point out several key functions that help achieve a win-win situation between job seekers and enterprises for a successful recruiting system. Based on the observations and key functions, we design, implement and deploy a web-based application of recruiting system, named iHR, for Xiamen Talent Service Center. The system utilizes the latest advances in data mining and recommendation technologies to create a user-oriented service for a myriad of audience in job marketing community. Empirical evaluation and online user studies demonstrate the efficacy and effectiveness of our proposed system. Currently, iHR has been deployed at http://i.xmrc.com.cn/XMRCIntel.", "authors": [{"affiliation": " Florida International University", "name": "Lei Li"}, {"affiliation": " Xiamen University", "name": " Wenxing Hong"}, {"affiliation": " Xiamen Talent Service Center", "name": " Wenfu Pan"}, {"affiliation": " Florida International University", "name": " Tao Li"}], "title": "iHR: An Online Recruiting System for Xiamen Talent Service Center"}, "40": {"abstract": "Generalized additive models (GAMs) combine single-feature models called shape functions through a linear function.  Previous studies have shown that while GAMs can easily be interpreted by users, the accuracy of GAMs is significantly lower than that of complex models.  In this paper, we suggest to add selected terms of interacting pairs of features to a GAM. The resulting model class, which we call \\ensuremath{GA^{2}M}-Models, for \\emph{Generalized Additive Models plus Interactions}, consists of univariate variate terms and a small number of feature interaction terms. Since they only include one-dimensional and two-dimensional components, the components of \\ensuremath{GA^{2}M}-models can be visualized and interpreted by users. To overcome the bottleneck of selecting good pairs of features, we develop a novel, computationally efficient method called FAST for ranking all possible pairs of features as candidates for inclusion into the model.  Our large-scale empirical study shows the effectiveness of FAST in ranking candidate pairs of features. In addition, we show the surprising result that on our datasets, \\ensuremath{GA^{2}M}-models have almost the same performance as the best full-complexity models. Thus this paper postulates that for many problems, \\ensuremath{GA^{2}M}-models can yield models that are both intelligible and accurate.", "authors": [{"affiliation": " Cornell University", "name": "Yin Lou"}, {"affiliation": " Microsoft Research", "name": " Rich Caruana"}, {"affiliation": " Cornell University", "name": " Johannes Gehrke"}, {"affiliation": " Cornell University", "name": " Giles Hooker"}], "title": "Accurate Intelligible Models with Pairwise Interactions"}, "41": {"abstract": "Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making in e-commerce. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or to demote some target products. In recent years, fake review detection has attracted significant attention from both the business and research communities. Several research papers have been published. However, due to the difficulty of human labeling required for supervised learning and evaluation, the problem remains to be highly challenging. This work proposes a novel angle to the problem by modeling \u00d2spamicity\u00d3 as latent. An unsupervised model, called Author Spamicity Model (ASM) is proposed. We work in the Bayesian setting, which facilitates modeling spamicity of authors as latent and allows us to exploit various observed behavioral footprints of reviewers. The intuition hinges on the hypothesis that opinion spammers have different behavioral distributions than non-spammers. This creates a distributional divergence between the latent population distributions of two clusters: spammers and non-spammers. Model inference results in learning the population distributions of the two clusters. Several extensions of ASM are also considered leveraging from different priors. Experiments on a real-life Amazon review dataset demonstrate the effectiveness of the proposed models which significantly outperform other state-of-the-art competitors.", "authors": [{"affiliation": " UIC", "name": "ARJUN MUKHERJEE"}, {"affiliation": " ", "name": " Abhinav Kumar"}, {"affiliation": " ", "name": " Bing Liu"}, {"affiliation": " ", "name": " Junhui Wang"}, {"affiliation": " ", "name": " meichun Hsu"}, {"affiliation": " ", "name": " Malu Castellanos"}, {"affiliation": "", "name": " riddhiman Ghosh"}], "title": "Spotting Opinion Spammers using Behavioral Footprints"}, "324": {"abstract": "In recent years, information trustworthiness becomes a serious issue when user-generated contents prevail in our information world. In this paper, we investigate the important problem of estimating information trustworthiness from the perspective of correlating and comparing multiple data sources. To a certain extent, the consistency degree is an indicator of information reliability--Information unanimously agreed by all the sources is more likely to be reliable. Based on this principle, we develop an effective computational approach to identify consistent information from multiple data sources. Particularly, we analyze vast amounts of information collected from multiple review platforms (multiple sources) in which people can rate and review the items they have purchased. The major challenge is that different platforms attract diverse sets of users, and thus information cannot be compared directly at the surface. However, latent reasons hidden in user ratings are mostly shared by multiple sources, and thus inconsistency about an item only appears when some source provides ratings deviating from the common latent reasons. Therefore, we propose a novel two-step procedure to calculate information consistency degrees for a set of items which are rated by multiple sets of users on different platforms. We first build a Multi-Source Deep Belief Network (MSDBN) to identify the common reasons hidden in multi-source rating data, and then calculate a consistency score for each item by comparing individual sources with the reconstructed data derived from the latent reasons. We conduct experiments on real user ratings collected from Orbitz, Priceline and TripAdvisor on all the hotels in Las Vegas and New York City. Experimental results demonstrate that the proposed approach successfully finds the hotels that receive inconsistent, and possibly unreliable, ratings.", "authors": [{"affiliation": " University at Buffalo", "name": "Liang Ge"}, {"affiliation": " Univ. of Buffalo", "name": " Jing Gao"}, {"affiliation": " The State University of New York", "name": " Xiaoyi Li"}, {"affiliation": " University at Buffalo", "name": " Aidong Zhang"}], "title": "Multi-Source Deep Learning for Information Trustworthiness Estimation"}, "9": {"abstract": "Micro-blogging services, such as Twitter, and location-based social network applications have generated short text messages associated with geographic information, posting time, and user ids. The availability of such data received from users offers a good opportunity to understand the user's spatial-temporal behavior and preference. In this paper, we propose a probabilistic model to exploit such data to understand individual users' mobility behavior from spatial, temporal and activity aspects. To the best of our knowledge, our work offers the first solution to jointly model individual user's mobility behavior from the three aspects. Our model has a variety of applications, such as user profiling and location prediction; it can be employed to answer questions such as ``Can we infer the location of a user given a tweet posted by the user and the posting time?\" Experimental results on two real-world data sets show that the proposed model is effective in discovering users' spatial-temporal topics, and outperforms state-of-the-art baseline significantly for the task of location prediction for tweets.", "authors": [{"affiliation": " Nanyang Technological Univ.", "name": "Quan Yuan"}, {"affiliation": " Nanyang Technological University", "name": " Gao Cong"}, {"affiliation": " Nanyang Technological University", "name": " Zongyang Ma"}, {"affiliation": " Nanyang Technological University", "name": " Aixin Sun"}, {"affiliation": " Nanyang Technological University", "name": " Nadia Thalmann"}], "title": "Where, When and What: Discover Spatio-Temporal Topics for Twitter Users"}, "146": {"abstract": "The effectiveness of existing top-N recommendation methods decreases as the sparsity of the datasets increases. To alleviate this problem, we present an item-based method for generating top-N recommendations that learns the item-item similarity matrix as the product of two low dimensional latent factor matrices. These matrices are learned using a structural equation modeling approach, wherein the value being estimated is not used for its own estimation. A comprehensive set of experiments on multiple datasets at three different sparsity levels indicate that the proposed method can handle sparse datasets effectively and outperforms other state-of-the-art \\topn recommendation methods. The experimental results also show that the relative performance gains compared to competing methods increase as the data gets sparser.", "authors": [{"affiliation": " University of Minnesota", "name": "Santosh Kabbur"}, {"affiliation": " University of Minnesota", "name": " George Karypis"}], "title": "FISM: Factored Item Similarity Models for Top-N Recommender Systems"}, "200": {"abstract": "Users in online social networks play a variety of social roles and statuses. For example, users in Twitter can be represented as advertiser, content contributor, information receiver, etc; users in Linkedin can be in different professional roles, such as engineer, salesperson and recruiter. Previous research work mainly focuses on using  categorical and textual information to predict the attributes of users. However, it cannot be applied to a large number of users in  real social networks, since much of such information is missing, outdated and non-standard. In this paper, we investigate  the social roles and statuses that people act in online social networks in the perspective of network structures, since the uniqueness of social networks is connecting people. We quantitatively analyze a number of key social principles and theories that correlate with social roles and statuses. We systematically study how the network characteristics reflect the social situations of users in an online society. We discover patterns of homophily, the tendency of users to connect with users with similar social roles and statuses.  In addition, we observe that different factors in social theories influence the social role/status of individual user to various extent, since these social principles represent different aspects of the network. We then introduce an optimization framework based on Factor Conditioning Symmetry, and we propose a probabilistic model to integrate the optimization framework on local structural information as well as network influence to infer the unknown social roles and statuses of online users. We will present experiment results to show the effectiveness of the inference.", "authors": [{"affiliation": " UIC", "name": "Yuchen Zhao"}, {"affiliation": " Univ. of Illinois at Chicago", "name": " Guan Wang"}, {"affiliation": " University of Illinois", "name": " Philip Yu"}, {"affiliation": " Linkedin", "name": " Shaobo Liu"}, {"affiliation": " Linkedin", "name": " Simon Zhang"}], "title": "Inferring Social Roles and Statuses in Social Networks"}, "618": {"abstract": "The key algorithmic problem in viral marketing is identifying a set of influential users (called seeds) in a social network, who, when convinced to adopt a product, shall influence other users in the network, leading to a large number of adoptions. When two or more players compete with similar products on the same network we talk about competitive viral marketing, which so far has been studied exclusively from the perspective of one of the competing players.  In this paper we propose and study the novel problem of competitive viral marketing from the perspective of the host, i.e., the third party owning the social networks. The host sells viral marketing campaign as a service to its customers, keeping control of the selection of seeds. Each company specifies its seed budget and the host allocates the seeds accordingly. From the host\u00d5s perspective, it is important not only to choose the seeds to maximize the collective expected spread, but also to assign seeds to companies so that it guarantees the \u00d2bang for the buck\" for all companies is nearly identical, which we formalize as the fair seed allocation problem.  We propose a new diffusion model that extends the classical Linear Threshold model to capture the competitive aspect in viral marketing. Our model is intuitive and retains the desired properties of monotonicity and submodularity. We show that the fair seed allocation problem is NP-hard, and develop an efficient algorithm called Needy Greedy. We run experiments on three real-world social networks, showing that our algorithm is effective and scalable.", "authors": [{"affiliation": " University of British Columbia", "name": "Wei Lu"}, {"affiliation": " Yahoo! Research", "name": " Francesco Bonchi"}, {"affiliation": " University of British Columbia", "name": " Amit Goyal"}, {"affiliation": "", "name": " Laks V.S. Lakshmanan"}], "title": "The Bang for the Buck: Fair Competitive Viral Marketing from the Host Perspective"}, "142": {"abstract": "Big Data Pipelines decompose complex analyses of large data sets into a series of simpler tasks, with independently tuned components for each task. This modular setup allows re-use of components across several different pipelines. However, the interaction of independently tuned pipeline components yields poor end-to-end performance as errors introduced by one component cascade through the whole pipeline, affecting overall accuracy.   We provide a novel model for reasoning across components in Big Data Pipelines in a probabilistically well-founded manner. Our key idea is to view the interaction of components as dependencies on an underlying graphical model.  Different message passing algorithms on this graphical model provide various ways to trade-off end-to-end performance and computational cost for inference.  We instantiate our framework with an efficient beam search algorithm, and demonstrate its efficiency on two Big Data Pipelines: parsing and relation extraction.", "authors": [{"affiliation": " Cornell University", "name": "Karthik Raman"}, {"affiliation": " Cornell University", "name": " Adith Swaminathan"}, {"affiliation": " Cornell", "name": " Thorsten Joachims"}, {"affiliation": " Cornell University", "name": " Johannes Gehrke"}], "title": "A Probabilistic Framework for Big Data Pipelines"}, "770": {"abstract": "We propose a robust framework to jointly perform two key modeling tasks involving high dimensional data: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among responses while adjusting for their predictors. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecifica- tion. This issue is exacerbated when dealing with high dimensional noisy data. In this work, we propose instead to minimize a regularized distance criterion, which is motivated by the minimum distance functionals used in nonparametric methods for their excellent robustness properties. The proposed estimates can be obtained efficiently by leveraging a sequential quadratic programming algorithm. We provide theoretical justification such as estimation consistency for the proposed estimator. Additionally, we shed light on the robustness of our estimator through its linearization, which yields a combination of weighted lasso and graphical lasso with the sample weights providing an intuitive explanation of the robustness. We demonstrate the merits of our framework through simulation study and the analysis of real financial and genetics data.", "authors": [{"affiliation": " IBM Research", "name": "Aurelie Lozano"}, {"affiliation": " IBM Research", "name": " Huijing Jiang"}, {"affiliation": " Virginia Tech", "name": " Xinwei Deng"}], "title": "Robust Sparse Estimation of Multiresponse Regression and Inverse Covariance Matrix via the L2 distance"}, "773": {"abstract": "Although hashing techniques have been popular for the large scale similarity search problem, most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment, i.e., the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world applications, there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogenous domains, e.g., recommending relevant posts and images to a certain Facebook user. In this paper, we address the problem of \u00d2comparing apples to oranges\u00d3 under the large scale setting. Specifically, we propose a novel Relation-aware Heterogeneous Hashing(RaHH), which provides a general framework for generating hash codes of data entities sitting in multiple heterogenous domains. Unlike some existing hashing methods that map heterogeneous data in a common Hamming space, the RaHH approach constructs a Hamming space for each type of data entities, and learns optimal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover, the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with improved accuracy. To validate the proposed RaHH method, we conduct extensive evaluations on two large datasets crawled from the popular social media sites, i.e., Flickr and Tencent Weibo. The experimental results clearly demonstrate that the RaHH outperforms several state-of-the-art hashing methods with significant performance gains.", "authors": [{"affiliation": " Tsinghua University", "name": "Mingdong Ou"}, {"affiliation": " Tsinghua University", "name": " Peng Cui"}, {"affiliation": " IBM T. J. Watson Research Lab", "name": " Fei Wang"}, {"affiliation": " IBM Research", "name": " Jun Wang"}], "title": "Comparing Apples to Oranges: A Scalable Solution with Heterogeneous Hashing"}, "897": {"abstract": "Outlier detection and ensemble learning are well established research directions in data mining yet the application of ensemble techniques to outlier detection has been rarely studied. Here, we propose and study subsampling as a technique to induce diversity among individual outlier detectors. We show analytically and experimentally that an outlier detector based on a subsample per se, besides inducing diversity, can, under certain conditions, already improve upon the results of the same outlier detector on the complete dataset. Building an ensemble on top of several subsamples is further improving the results. While in the literature so far the intuition that ensembles improve over single outlier detectors has just been transferred from the classification literature, here we also justify analytically why ensembles are also expected to work in the unsupervised area of outlier detection.  As a side effect, running an ensemble of several outlier detectors on subsamples of the dataset is more efficient than ensembles based on other means of introducing diversity and, depending on the sample rate and the size of the ensemble, can be even more efficient than just the single outlier detector on the complete data.  ", "authors": [{"affiliation": " University of Alberta", "name": "Arthur Zimek"}, {"affiliation": " University of Alberta", "name": " Matthew Gaudet"}, {"affiliation": " University of Alberta", "name": " Ricardo J. G. Campello"}, {"affiliation": " University of Alberta", "name": " Jorg Sander"}], "title": "Subsampling for Efficient and Effective Unsupervised Outlier Detection Ensembles"}, "611": {"abstract": "People use various social media for different purposes. The information on each site is often partial. When sources of complementary information are integrated, a better profile of a user can be built to improve online services such as verifying online information. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem.  We introduce a methodology (MOBIUS) for finding a mapping among identities of individuals across social media sites. It consists of three key components: the first component identifies users' unique behavioral patterns that lead to information redundancies across sites; the second component constructs features that exploit information redundancies due to these behavioral patterns; and the third component employs machine learning for effective user identification. We formally define the cross-media user identification problem, show that MOBIUS is effective in identifying users across social media sites, and discuss how further improvements can be made. This study paves the way for analysis and mining across social media sites, and facilitates the creation of novel online services across social media sites.", "authors": [{"affiliation": " Arizona State University", "name": "Reza Zafarani"}, {"affiliation": " Arizona State University", "name": " Huan Liu"}], "title": "Connecting Users across Social Media Sites: A Behavioral-Modeling Approach"}, "510": {"abstract": "Learning algorithms that embed objects into Euclidean space have become the methods of choice for a wide range of problems, ranging from recommendation and image search to playlist prediction and language modeling. Probabilistic embedding methods provide elegant approaches to these problems, but can be expensive to train and store as a large monolithic model. In this paper, we propose a method that trains not one monolithic model, but multiple local embeddings for a class of pairwise conditional models especially suited for sequence and co-occurrence modeling. We show that computation and memory for training these multi-space models can be efficiently parallelized over many nodes of a cluster. Focusing on sequence modeling for music playlists, we show that the method substantially speeds up training while maintaining high model quality.", "authors": [{"affiliation": " Dept. of Computer Science, Cornell University", "name": "Shuo Chen"}, {"affiliation": " Dept. of Computer Science, Cornell University", "name": " Jiexun Xu"}, {"affiliation": " Cornell", "name": " Thorsten Joachims"}], "title": "Multi-space Probabilistic Sequence Modeling"}, "483": {"abstract": "With the explosion of mobile devices with cameras, online search has moved beyond test to other modalities like images, voice, and writing. For many applications like Fashion, image-based search offers a compelling interface as compared to text forms by better capturing the visual attributes. In this paper, we present a simple and fast search algorithm that uses color as the main feature for building visual search. We show that low level cues such as color can be used to quantify image similarity and also to discriminate among products with different visual appearances. We demonstrate the effectiveness of our approach through a mobile shopping application. Our approach outperforms several other state-of-the-art image retrieval algorithms for large scale image data.", "authors": [{"affiliation": " eBay Research Labs", "name": "Anurag Bhardwaj"}, {"affiliation": " EBay Research Lab", "name": " Atish Das Sarma"}, {"affiliation": " EBay Research Labs", "name": " Wei Di"}, {"affiliation": " eBay Research Labs", "name": " Raffay Hamid"}, {"affiliation": " eBay Research Labs", "name": " Robinson Piramuthu"}, {"affiliation": " eBay Research", "name": " Neel Sundaresan"}], "title": "Palette Power: Enabling Visual Search through Colors"}, "801": {"abstract": "Traditionally, feature construction and feature selection are two important but separate processes in data mining. However, many real world applications require an integrated approach for creating, refining and selecting features. To address this problem, we propose FeaFiner (short for Feature Refiner), an efficient formulation that simultaneously generalizes low-level features into higher level concepts and then selects relevant concepts based on the target variable. Specifically, we formulate a double sparsity optimization problem that identifies groups in the low-level features, generalizes higher level features using the groups and performs feature selection. Since in many clinical researches nonoverlapping groups are preferred for better interpretability, we further improve the formulation to generalize features using mutually exclusive feature groups. The proposed formulation is challenging to solve due to the orthogonality constraints, non-convexity objective and non-smoothness penalties. We apply a recently developed augmented Lagrangian method to solve this formulation in which each subproblem is solved by a non-monotone spectral projected gradient method. Our numerical experiments show that this approach is computationally efficient and also capable of producing solutions of high quality. We also present a generalization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation. Finally, the proposed FeaFiner method is validated on Alzheimer's Disease Neuroimaging Initiative dataset, where low-level biomarkers are automatically generalized into robust higher level concepts which are then selected for predicting the disease status measured by Mini Mental State Examination and Alzheimer's Disease Assessment Scale cognitive subscore. Compared to existing predictive modeling methods, FeaFiner provides intuitive and robust feature concepts and competitive predictive accuracy.", "authors": [{"affiliation": " Arizona State University", "name": "Jiayu Zhou"}, {"affiliation": " Simon Fraser University", "name": " Zhaosong Lu"}, {"affiliation": " IBM Research ", "name": " Jimeng Sun"}, {"affiliation": " Arizona State University", "name": " Lei Yuan"}, {"affiliation": " IBM T. J. Watson Research Lab", "name": " Fei Wang"}, {"affiliation": " Arizona State University", "name": " Jieping Ye"}], "title": "FeaFiner: Biomarker Identification from Medical Data through Feature Generalization and Selection"}, "1287": {"abstract": "We present novel, efficient, model based kernels for time series data rooted in the reservoir computation framework. The kernels are implemented by fitting reservoir models sharing the same fixed deterministically constructed state transition part to individual time series. The proposed kernels can naturally handle time series of different length without the need to specify a parametric model class for the time series. Compared with other time series kernels, our kernels are computationally efficient. We show how the model distances used in the kernel can be calculated analytically or efficiently estimated. The experimental results on synthetic and benchmark time series classification tasks confirm the efficiency of the proposed kernel in terms of both generalization accuracy and computational speed. This paper also investigates on-line reservoir kernel construction for extremely long time series.", "authors": [{"affiliation": " University of Birmingham", "name": "Huanhuan Chen"}, {"affiliation": " University of Birmingham", "name": " Fengzhen Tang"}, {"affiliation": " University of Birmingham", "name": " Peter Tino"}, {"affiliation": " University of Birmingham", "name": " Xin Yao"}], "title": "Model-based Kernel for Efficient Time Series Analysis"}, "1045": {"abstract": "Twitter has become an increasingly important source of information, with more than 400 million tweets posted per day. The task to link the named entity mentions detected from tweets with the corresponding real world entities in the knowledge base is called tweet entity linking. This task is of practical importance and can facilitate many different tasks, such as personalized recommendation and user interest discovery. The tweet entity linking task is challenging due to the noisy, short, and informal nature of tweets. Previous methods focus on linking entities in Web documents, and largely rely on the context around the entity mention and the topical coherence between entities in the document. However, these methods cannot be effectively applied to the tweet entity linking task due to the insufficient context information contained in a tweet. In this paper, we propose KAURI, a novel graph-based unified framework to collectively link all the named entity mentions in all tweets posted by a user via modeling the user's topics of interest. Our assumption is that each user has an underlying topic interest distribution over various named entities. To model this information, we propose a graph-based user interest propagation algorithm which integrates the global user interest information across tweets with the intra-tweet local information. We extensively evaluated the performance of KAURI over manually annotated tweet corpus, and the experimental results show that KAURI significantly outperforms the baseline methods in terms of accuracy, and KAURI is efficient and scales well to tweet stream.", "authors": [{"affiliation": " Tsinghua University", "name": "Wei Shen"}, {"affiliation": " Tsinghua University", "name": " Jianyong Wang"}, {"affiliation": " HP Lab", "name": " Ping Luo"}, {"affiliation": " Google Research", "name": " Min Wang"}], "title": "Linking Named Entities in Tweets with Knowledge Base via User Interest Modeling"}, "1289": {"abstract": "The low-rank regression model has been studied and applied to capture the underlying classes/tasks correlation patterns, such that the regression/classification results can be enhanced. In this paper, we will prove that the low-rank regression model is equivalent to doing linear regression in the linear discriminant analysis (LDA) subspace. Our new theory reveals the learning mechanism of low-rank regression, and shows that the low-rank structures exacted from classes/tasks are connected to the LDA projection results. Thus, the low-rank regression efficiently works for the high-dimensional data.  Moreover, we will propose new discriminant low-rank ridge regression and sparse low-rank regression methods. Both of them are equivalent to doing regularized regression in the regularized LDA subspace. These new regularized objectives provide better data mining results than existing low-rank regression in both theoretical and empirical validations. We evaluate our discriminant low-rank regression methods by six benchmark datasets. In all empirical results, our discriminant low-rank models consistently show better results than the corresponding full-rank methods.", "authors": [{"affiliation": " Chris Ding, Feiping Nie, Heng Huang, University of Texas, Arlington", "name": "Xiao Cai"}], "title": "Discriminant Low-Rank Regression: On The Equivalence of Low-Rank Regression and Discriminant Analysis"}, "355": {"abstract": "The problem of identifying the optimal location to open a new retail store has been the focus of past research, especially in the field of land economy, due to its importance in the success of a business. Traditional approaches to the problem have factored in demographics, revenue and aggregated human flow statistics from nearby or remote areas. However, the acquisition of relevant data is usually expensive. With the growth of location-based social networks, fine grained data describing user mobility and popularity of places has recently become attainable.  In this paper we study the predictive power of various machine learning features on the popularity of retail stores in the city through the use of a dataset collected from Foursquare in New York. The features we mine are based on two general signals: geographic, where features are formulated according to the types and density of nearby places, and user mobility which includes transitions between venues or the incoming flow of mobile users from distant areas. After studying the effectiveness of features individually, we integrate them in a supervised learning framework.  Our evaluation suggests that the best performing features are common across the three different commercial chains considered in the analysis, although variations may exist too, as explained by heterogeneities in the way retail facilities attract users. Features that exploit the semantics information of Foursquare venues, encoding the competitiveness of an area with respect to a given type of business (for example restaurant or coffee shop) together with features modeling incoming transitions from distant areas are doing best. Features that encode information on user transitions across venues in proximity are also effective in this context. Finally, we show that performance improve significantly when combining multiple features in supervised learning algorithms, suggesting that the retail success of a business may depend on multiple of factors.", "authors": [{"affiliation": " University of Cambridge", "name": "Dmytro Karamshuk"}, {"affiliation": " University of Cambridge", "name": " Anastasios Noulas"}, {"affiliation": " University of Cambridge", "name": " Salvatore Scellato"}, {"affiliation": " University of Cambridge", "name": " Vincenzo Nicosia"}, {"affiliation": " University of Cambridge", "name": " Cecilia Mascolo"}], "title": "Geo-Spotting: Mining Online Location-based Services for Optimal Retail Store Placement"}, "688": {"abstract": "In order to satisfy and positively surprise the users, a recommender system needs to recommend items the users will like and most probably would not have found on their own. This requires the recommender system to recommend a broader range of items including niche items as well. Such an approach also support online-stores that often offer more items than traditional stores and need recommender systems to enable users to find the not so popular items as well. However, popular items that hold a lot of usage data are more easy to recommend and, thus, niche items are often excluded from the recommendations. In this paper, we propose a new collaborative filtering approach that is based on the items' usage contexts. The approach increases the rating predictions for niche items with fewer usage data available and improves the aggragate diversity of the recommendations.", "authors": [{"affiliation": " Fraunhofer FIT", "name": "Katja Niemann"}, {"affiliation": " Fraunhofer Institute for Applied Information Technology", "name": " Martin Wolpers"}], "title": "A New Collaborative Filtering Approach for Increasing the Aggregate Diversity of Recommender Systems"}, "685": {"abstract": "Support Vector Machines (SVMs) are a leading tool in machine learning and have been used with considerable success for the task of time series forecasting. However, a key challenge when using SVMs for time series is the question of how to deeply integrate time elements into the learning process. To meet this challenge, we investigated the distribution of errors in the forecasts delivered by standard SVMs. Once we identified the samples that produced the largest errors, we are able to derive their distinction from outliers/noisy samples and detect their correlation with distribution shifts that occur in the time series. This is why we propose a time-dependent loss function which allows the inclusion of the information about the distribution shifts in the series directly into the SVM learning process. We present experimental results which  indicate that using a time-dependent loss function is highly promising, reducing the overall variance of the errors, as well as delivering more accurate predictions.", "authors": [{"affiliation": " The University of Melbourne", "name": "Goce Ristanoski"}, {"affiliation": " The University of Melbourne", "name": " Wei Liu"}, {"affiliation": " The University of Melbourne", "name": " James Bailey"}], "title": "Time-Dependent Loss Enhanced SVM for Time Series Regression"}, "478": {"abstract": "Many different machine learning algorithms exist; taking into account each algorithm\u00d5s hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.", "authors": [{"affiliation": " UBC", "name": "Chris Thornton"}, {"affiliation": " UBC", "name": " Frank Hutter"}, {"affiliation": " UBC", "name": " Holger Hoos"}, {"affiliation": " UBC", "name": " Kevin Leyton-Brown"}], "title": "Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms"}, "479": {"abstract": "We present a novel learning algorithm, DirectRank, which directly and exactly optimizes ranking measures without resorting to any upper bounds or approximations. Our approach is essentially an iterative coordinate ascent method in optimization. In each iteration, we choose one coordinate and only update the corresponding parameter, with all others remaining fixed. Since the ranking measure is a step-wise function of a single parameter, we propose an novel line search algorithm which could locate the interval with the best ranking measure along this coordinate quite efficiently. In order to stabilize our system in small datasets, we construct a probabilistic framework for document-query pairs to maximize the likelihood of objective permutation of top-_ documents. This iterative procedure ensures convergence. More, we integrate regression trees as our weak learners in order to gain more power. Experiments on Microsoft LETOR datasets and two large datasets, Yahoo challenge and Microsoft 30K web, show our improvements over state-of-the-art systems.", "authors": [{"affiliation": " Wright State University", "name": "Ming Tan"}, {"affiliation": " Wright State University", "name": " Tian Xia"}, {"affiliation": " Wright State University", "name": " Lily Guo"}, {"affiliation": " Wright State University", "name": " Shaojun Wang"}], "title": "Direct Optimization of Ranking Measures for Learning to Rank Models"}}