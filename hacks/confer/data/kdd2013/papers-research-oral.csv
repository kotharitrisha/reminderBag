18,Fast Structure Learning in Generalized Stochastic Processes with Latent Factors,Research,"Understanding and quantifying the impact of unobserved processes in the multivariate time series data is a major challenge in analysis of multivariate stochastic processes. In this paper, we analyze a flexible stochastic process model, the generalized linear auto-regressive process (GLARP) and identify the conditions with which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood. To illustrate the power of this framework, we examine three examples, including two popular models for count data, i.e, Poisson and Conwey-Maxwell Poisson vector auto-regressive processes, and one powerful but less studies model for continuous data, i.e., Pareto vector auto-regressive processes for heavy-tailed time series data. We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it. Experiment results on two synthetic datasets, one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models.","Mohammad Taha Bahadori*, University of Southern Califor; Yan Liu, University of Southern California; Eric Xing, CMU",Mining rich data types\Temporal / time series*,"Time Series Analysis, Temporal Dependency Analysis, Latent Factors, Convex Optimization"52,TurboGraph: A Fast Parallel Graph Engine Handling Billion-scale Graphs in a Single PC,Research,"Graphs are used to model many real objects such as social networks, web graphs, chemical compounds,  and biological structures. Many real applications in various fields require efficient and effective management of large-scale graph structured data. Although distributed graph engines such as \GBase and \Pregel handle billion-scale graphs, the user must be skilled at managing and tuning a distributed system in a cluster, which is a nontrivial job for the ordinary user. Furthermore, these distributed systems need many machines in a cluster in order to provide reasonable performance. In order to address this problem, a disk-based parallel graph engine called \GraphChi, has been recently proposed. Although \GraphChi significantly outperforms all representative (disk-based) distributed graph engines, we observe that \GraphChi still has serious performance problems for many important types of graph queries due to 1) limited parallelism and 2) separate steps for I/O processing and CPU processing. In this paper, we propose a general, disk-based graph engine called \TurboGraph\footnote {https://sites.google.com/site/turbographdb/} to process billion-scale graphs very efficiently by using modern hardware on a single PC. \TurboGraph is the first truly parallel graph engine that exploits 1) \emph{full parallelism} including multi-core parallelism and FlashSSD IO parallelism and 2) \emph{full overlap} of CPU processing and I/O processing as much as possible. Specifically, we propose a novel parallel execution model, called \PinSlide. \TurboGraph also provides engine-level operators such as BFS which are implemented under the pin-and-slide model. Extensive experimental results with large real datasets show that \TurboGraph consistently and significantly outperforms \GraphChi by up to four orders of magnitude.","Wook-Shin Han*, POSTECH; Sangyeon Lee, POSTECH; Kyungyeol  Park, POSTECH; Jeong-Hoon Lee, POSTECH; Min-Soo Kim, DGIST; Jinha Kim, POSTECH; Hwanjo Yu, Pohang University of Science and Technology",Big Data*; Big Data\Large scale optimization; Big Data\Scalable methods; Social\Social and information networks,93,Flexible and Robust Co-regularized Multi-Domain Graph Clustering,Research,"Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this paper, we propose a flexible and robust framework, CGC (Co-regularized Graph Clustering), based on non-negative matrix factorization (NMF), to tackle these challenges.  CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. Extensive experimental results on UCI benchmark data sets, newsgroup data sets and biological interaction networks demonstrate the effectiveness of our approach.","Wei Cheng*, UNC at Chapel Hill; xiang Zhang, Case Western Reserve University; Patrick Sullivan, UNC at Chapel Hill; Wei Wang, University of California, Los Angeles",Graph mining*; Applications\Healthcare and medicine; Bioinformatics; Social\Social and information networks; Transfer learning; Unsupervised learning\Clustering,graph clustering; nonnegative matrix factorization; co-regularization95,Density-Based Logistic Regression,Research,"This paper introduces a nonlinear logistic regression model for classification. The main idea is to map the data to a feature space based on kernel density estimation. A discriminative model is then learned to optimize the feature weights as well as the bandwidth of a Nadaraya-Watson kernel density estimator. We then propose a hierarchical optimization algorithm for learning the coefficients and kernel bandwidths in an integrated way. Compared to other nonlinear models such as kernel logistic regression (KLR) and SVM, our approach is far more efficient since it solves an optimization problem with a much smaller size. Two other major advantages are that it can cope with categorical attributes in a unified fashion and naturally handle multi-class problems. Moveover, our approach inherits from logistic regression good interpretability of the model, which is important for clinical applications but not offered by KLR and SVM. Extensive results on real datasets, including a clinical prediction application currently under deployment in a major hospital, show that our approach not only achieves superior classification accuracy, but also drastically reduces the computing time as compared to other leading methods.","Wenlin Chen, ; Yixin Chen*, Washington University in St Louis; Yi Mao, ",Supervised learning\Classification*; Applications\Healthcare and medicine; Supervised learning\Regression,"classification, logistic regression, kernel density estimation"120,Extracting Social Events for Learning Better Information Diffusion Models,Research,"Learning of the information diffusion model is a fundamental problem in the study of information diffusion in social networks. Existing approaches learn the diffusion models from events in social networks. However, events in social networks may have different underlying reasons. Some of them may be caused by the social influence inside the network, while others may reflect external trends in the ""real world"". Most existing work on the learning of diffusion models does not distinguish the events caused by the social influence from those caused by external trends.     In this paper, we extract social events from the data stream in social networks, and then use the extracted social events to improve the learning of information diffusion models. We propose a LADP (Latent Action Diffusion Path) model to incorporate the information diffusion model with the model of external trends, and then design an EM-based algorithm to infer the diffusion probabilities, the external trends and the sources of events efficiently ","Shuyang Lin*, UIC; Fengjiao Wang, University of Illinois at Chic; Qingbo Hu, University of Illinois at Chic; philip Yu, University of Illinois at Chicago","Social\Social and information networks*; Economy, markets\Viral marketing; Graph mining","Social Event, Information Diffusion, Social Influence"131,Mining Lines in the Sand: On Trajectory Discovery From Untrustworthy Data in Cyber-Physical System,Research,"A Cyber-Physical System (CPS) integrates physical (i.e., sensor) devices with cyber (i.e., informational) components to form a context sensitive system that responds intelligently to dynamic changes in real-world situations. The CPS has wide applications in scenarios such as environment monitoring, battlefield surveillance and traffic control. One key research problem of CPS is called “mining lines in the sand”. With a large number of sensors (sand) deployed in a designated area, the CPS is required to discover all the trajectories (lines) of passing intruders in real time. There are two crucial challenges that need to be addressed: (1) the collected sensor data are not trustworthy; (2) the intruders do not send out any identification information, the system needs to distinguish multiple intruders and track their movements. In this study, we propose a method called LiSM (Line-in-the-Sand Miner) to discover the trajectories from untrustworthy sensor data. LiSM constructs a watching network from sensor data and computes the locations of intruder appearances based on the link information of the network. The system retrieves a cone-model from the historical trajectories and tracks multiple intruders based on this model. Finally the system validates the mining results and updates the sensor's reliability in a feedback process. Extensive experiments on both real and synthetic datasets demonstrate the feasibility and applicability of the proposed methods.","Lu-An Tang*, UIUC; Xiao Yu, University of Illinois at Urbana-Champaign; Quanquan Gu, CS, UIUC; Jiawei Han, UIUC; Alice Leung, BBN; Thomas La Porta, PSU",Mining rich data types*; Applications; Data streams; Mining rich data types\Spatial,"cyber physical system, sensor data mining, trajectory discovery"142,A Probabilistic Framework for Big Data Pipelines,Research,"Big Data Pipelines decompose complex analyses of large data sets into a series of simpler tasks, with independently tuned components for each task. This modular setup allows re-use of components across several different pipelines. However, the interaction of independently tuned pipeline components yields poor end-to-end performance as errors introduced by one component cascade through the whole pipeline, affecting overall accuracy.   We provide a novel model for reasoning across components in Big Data Pipelines in a probabilistically well-founded manner. Our key idea is to view the interaction of components as dependencies on an underlying graphical model.  Different message passing algorithms on this graphical model provide various ways to trade-off end-to-end performance and computational cost for inference.  We instantiate our framework with an efficient beam search algorithm, and demonstrate its efficiency on two Big Data Pipelines: parsing and relation extraction.","Karthik Raman*, Cornell University; Adith Swaminathan, Cornell University; Thorsten Joachims, Cornell; Johannes Gehrke, Cornell University",Big Data*; Big Data\Novel statistical techniques for big data; Probabilistic methods,"Big Data Pipelines, Modular Design, Probabilistic Inference"166,Fast and Scalable Polynomial Kernels via Explicit Feature Maps,Research,"Approximation of non-linear kernels using random feature mapping has been successfully employed in large-scale data analysis applications, accelerating the training of kernel machines. While previous random feature mappings run in $O(ndD)$ time for $n$ training samples in $d$-dimensional space and $D$ random feature maps, we propose a novel randomized tensor product technique, called \textit{Tensor Sketching}, for approximating any polynomial kernel in $O(n(d+D \log{D}))$ time. Also, we introduce both \textit{absolute} and \textit{relative} error bounds for our approximation to guarantee the reliability of our estimation algorithm. Empirically, Tensor Sketching achieves higher accuracy and often runs orders of magnitude faster than the state-of-the-art approach for large-scale real-world datasets.","Ninh Pham*, IT University of Copenhagen; Rasmus Pagh, IT University of Copenhagen",Supervised learning\Support vector machines*; Big Data\Scalable methods; Dimensionality reduction; Probabilistic methods,"Polynomial Kernel, SVM, Tensor Product, Count Sketch, FFT"172,SiGMa: Simple Greedy Matching for Aligning Large Knowledge Bases,Research,"The Internet has enabled the creation of a growing number of large-scale knowledge bases in a variety of domains containing complementary information. Tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of large-scale knowledge bases still poses a considerable challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algorithm for aligning knowledge bases with millions of entities and facts. SiGMa is an iterative propagation algorithm which leverages both the structural information from the relationship graph as well as flexible similarity measures between entity properties in a greedy local search, thus making it scalable. Despite its greedy nature, our experiments indicate that SiGMa can efficiently match some of the world's largest knowledge bases with high accuracy. We provide additional experiments on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art approaches both in accuracy and efficiency. ","Simon Lacoste-Julien*, INRIA / ENS; Konstantina Palla, University of Cambridge; Alex Davies, University of Cambridge; Gjergji Kasneci, Microsoft Research; Thore Graepel, Microsoft Research; Zoubin Ghahramani, Cambridge University",Web mining*; Big Data\Scalable methods,"knowledge base, alignment, large-scale, entity, relationship, greedy algorithm"191,Multi-Source Learning with Block-wise Missing Data For Alzheimer's Disease Prediction,Research,"With the advances and increasing sophistication in data collection techniques, we are facing with large amounts of data collected from multiple heterogeneous sources in many applications. For example, in the study of Alzheimer's Disease (AD), different types of measurements such as neuroimages, gene/protein expression data, genetic data etc. are often collected and analyzed together for improved predictive power. It is believed that a joint learning of multiple data sources is beneficial as different data sources may contain complementary information, and feature-pruning and data source selection are critical for learning interpretable models from high-dimensional data. Very often the collected data comes with block-wise missing entries; for example, a patient without the MRI scan will have no information in the MRI data block, making his/her overall record incomplete. There has been a growing interest in the data mining community on expanding traditional techniques for single-source complete data analysis to the study of multi-source incomplete data. The key challenge is how to effectively integrate information from multiple heterogeneous sources in the presence of block-wise missing data. In this paper we first investigate the situation of complete data and present a unified ``bi-level"" learning model for multi-source data. Then we give a natural extension of this model to the more challenging case with incomplete data. Our major contributions are threefold: (1) the proposed models handle both feature-level and source-level analysis in a unified formulation and include several existing feature learning approaches as special cases; (2) the model for incomplete data avoids direct imputation of the missing elements and thus provides superior performances. Moreover, it can be easily generalized to other applications with block-wise missing data sources; (3) efficient optimization algorithms are presented for both of the complete and incomplete model. ","Shuo Xiang*, Arizona State University; Lei Yuan, Arizona State University; Wei Fan, IBM Research; Yalin Wang, ; Paul Thompson, ; Jieping Ye, Arizona State University",Applications\Healthcare and medicine*; Bioinformatics; Supervised learning\Classification,198,Representing Documents Through Their Readers,Research,"From Twitter to Facebook to Reddit, users have become accustomed to sharing the articles they read with friends or followers on their social networks. While previous work has modeled what these shared stories say about the user who shares them, the converse question remains unexplored: what can we learn about an article from the identities of its likely readers?   To address this question, we model the content of news articles and blog posts by attributes of the people who are likely to share them. For example, many Twitter users describe themselves in a short profile, labeling themselves with phrases such as ""vegetarian"" or ""liberal."" By assuming that a user's labels correspond to topics in the articles he shares, we can learn a labeled dictionary from a training corpus of articles shared on Twitter. Thereafter, we can code any new document as a sparse non-negative linear combination of user labels, where we encourage correlated labels to appear together in the output via a structured sparsity penalty.   Finally, we show that our approach yields a novel document representation that can be effectively used in many problem settings, from recommendation to modeling news dynamics. For example, while the top politics stories will change drastically from one month to the next, the ""politics"" label will still be there to describe them. We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012, demonstrating that our approach is effective.","Khalid El-Arini*, Carnegie Mellon University; Min Xu, Carnegie Mellon University; Emily Fox, University of Washington; Carlos Guestrin, University of Washington","Mining rich data types\Text*; Recommender systems\Content based methods; Social\Social media; Unsupervised learning\Topic, graphical and latent variable models; Web mining","document modeling, Twitter, structured sparsity, personalization"240,Statistical Quality Estimation for General Crowdsourcing Tasks,Research,"One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control, which is to expect high-quality results from crowd workers who are neither necessarily very capable nor motivated. A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks. For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been proposed. However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing,  which occupy the majority on most crowdsourcing marketplaces. In this paper, we propose a statistical quality estimation method for such general crowdsourcing tasks. Our method is based on the two-stage procedure;  multiple workers are first requested to work on the same tasks in the creation stage, and then another set of workers review and grade each artifact in the review stage. We model the ability of each author and the bias of each reviewer, and propose a two-stage probabilistic generative model using the graded response model in the item response theory. Experiments using several general crowdsourcing tasks show that our method outperforms popular vote aggregation methods, especially when the number of reviewers for each work is small, which implies that our method can deliver high quality results with lower costs.","Yukino Baba*, The University of Tokyo; Hisashi Kashima, The University of Tokyo",Web mining*; Applications; User modeling,"crowdsourcing, human computation, quality estimation"245,Mining Frequent Graph Patterns with Differential Privacy,Research,"Discovering frequent graph patterns in a graph database offers valuable information in a variety of applications. However, if the graph dataset contains sensitive data of individuals such as mobile phone-call graphs and web-click graphs, releasing discovered frequent patterns may present a threat to the privacy of individuals. {\em Differential privacy} has recently emerged as the {\em de facto} standard for private data analysis due to its provable privacy guarantee. In this paper we propose the first differentially private algorithm for mining frequent graph patterns.  We first show that previous techniques on differentially private discovery of frequent {\em itemsets} cannot apply in mining frequent graph patterns due to the inherent complexity of handling structural information in graphs. We then address this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling based algorithm. Unlike previous work on frequent itemset mining, our techniques do not rely on the output of a non-private mining algorithm. Instead, we observe that both frequent graph pattern mining and the guarantee of differential privacy can be unified into an MCMC sampling framework. In addition, we establish the privacy and utility guarantee of our algorithm and propose an efficient neighboring pattern counting technique as well. Experimental results show that the proposed algorithm is able to output frequent patterns with good precision.","Entong Shen*, North Carolina State Univ; Ting Yu, North Carolina State University",Security and privacy*; Security and privacy\Anonymization,"differential privacy, privacy-preserving data mining"249,Approximate Graph Mining with Label Costs,Research,"   Many real-world graphs have complex labels on the nodes and edges.    Mining only exact patterns yields limited insights, since it may be    hard to find exact matches. However, in many domains it is relatively    easy to compute some cost (or distance) between different labels.    Using this information, it becomes possible to mine a much richer set    of approximate subgraph patterns, which preserve the topology but allow    bounded label mismatches. We present novel and scalable methods to    efficiently solve the approximate isomorphism problem. We show that    the mined approximate patterns yield interesting patterns in several    real-world graphs ranging from IT and protein interaction networks to    protein structures.","Pranay Anchuri*, RPI; Mohammed Zaki, Rensselaer Polytechnic Institute; Omer Barkol, HP Labs; Shahar Golan, HP Labs; Moshe Shamy, HP Labs",Graph mining*; Big Data\Large scale optimization; Mining rich data types; Rule and pattern mining; Sampling,"approximate graph mining, approximate matching; single graph mining"251,Robust Principal Component Analysis via Capped Norms,Research,"In many applications such as image and video processing, the data matrix often possesses a low-rank structure capturing the global information and a sparse component capturing the local information simultaneously. How to accurately extract the low-rank and sparse components is a major challenge. Robust Principal Component Analysis (RPCA) is a general framework to extract such structures. It is well studied that under certain assumptions, convex optimization using trace norm and $\ell_1$-norm can be an effective computation surrogate of the difficult RPCA problem. However, it is based on a strong assumption which may not hold in real-world applications, and the approximation error in these convex relaxations often cannot be neglected. In this paper, we present a novel non-convex formulation for the RPCA problem using the capped trace norm and the capped $\ell_1$-norm. In addition, we present two algorithms to solve the non-convex optimization: one is based on the Difference of Convex functions (DC) framework and the other attempts to solve the sub-problems via a greedy approach. Compared to existing convex formulations, our approach gives proper interpretation and both of the proposed algorithms achieve better accuracy for the RPCA problem which can be verified by empirical results. Furthermore, between the two proposed algorithms, the greedy algorithm is more efficient than the DC programming, while they achieve comparable accuracy.","Qian Sun*, Arizona State University; Shuo Xiang, Arizona State University; Jieping Ye, Arizona State University",Unsupervised learning*; Mining rich data types; Rule and pattern mining,"Robust PCA, DC programming, ADMM, low-rank, sparsity, trace norm, image processing"291,The Role of Information Diffusion in the Evolution of Social Networks,Research,"Every day millions of users are connected through online social networks, generating a rich trove of data that allows us to study the mechanisms behind human interactions. Triadic closure has been treated as the major mechanism for creating social links: if Alice follows Bob and Bob follows Charlie, Alice will follow Charlie. Here we present an analysis of longitudinal micro-blogging data, revealing a more nuanced view of the strategies employed by users when expanding their social circles. While the network structure affects the spread of information among users, the network is in turn shaped by this communication activity. This suggests a link creation mechanism whereby Alice is more likely to follow Charlie after seeing many messages by Charlie. We characterize users with a set of parameters associated with different link creation strategies, estimated by a Maximum-Likelihood approach. Triadic closure does have a strong effect on link formation, but shortcuts based on traffic are another key factor in interpreting network evolution. However, individual strategies for following other users are highly heterogeneous. Link creation behaviors can be summarized by classifying users in different categories with distinct structural and behavioral characteristics. Users who are popular, active, and influential tend to create traffic-based shortcuts, making the information diffusion process more efficient in the network. ","Lilian Weng*, Indiana University; Jacob Ratkiewicz, Google Inc.; Nicola Perra, Northeastern University; Bruno Goncalves, Aix-Marseille Universite; Carlos Castillo, Qatar Computing Research Institute; Francesco Bonchi, Yahoo! Research; Rossano Schifanella, Universita degli Studi di Torino, Italy; Filippo Menczer, Indiana University; Alessandro Flammini, Indiana University",Social\Social and information networks*; Social\Social media; Unsupervised learning\Clustering; User modeling,"Link creation, Traffic, Network evolution, Information diffusion, Shortcut, User behavior, Social media, Network structure"293,LCARS: A Location-Content-Aware Recommender System,Research,"Newly emerging location-based and event-based social network services provide us with a new platform to understand users' preferences based on their activity history.  A user can only visit a limited number of venues/events and  most of them are within a limited distance range, so the user-item matrix is very sparse, which creates a big challenge for traditional collaborative  filtering-based recommender systems. The problem becomes even more challenging when people travel to a new city where they have no activity history.   In this paper, we propose LCARS, a location-content-aware recommender system that offers a particular user a set of venues (e.g., restaurants and shopping malls) or  events (e.g., concerts and exhibitions)  by giving consideration to both personal interest and  local preference. This recommender system can facilitate people's travel not only near the area in which they live, but also in a city that is new to them. Specifically, LCARS consists of two components: offline modeling and online recommendation. The offline modeling part, called LCA-LDA,  is designed to learn the interest of each individual user and the local preference of each individual city by capturing  item co-occurrence patterns and exploiting  item contents. The online recommendation part automatically combines the learnt interest of the querying user and the local preference of the querying city to produce the top-k recommendations. To speed up this online process, a scalable query processing technique is developed by extending the classic Threshold Algorithm (TA). We evaluate the performance of our recommender system on two large-scale real data sets, DoubanEvent and Foursquare. The results show the superiority of LCARS in recommending spatial items for users, especially when traveling to new cities, in terms of both  effectiveness and efficiency.","Hongzhi Yin*, Peking University; Yizhou Sun, ; Bin Cui, Peking University; Zhiting Hu, ; Ling Chen, ","Recommender systems*; Recommender systems\Cold-start; Recommender systems\Collaborative filtering; Recommender systems\Content based methods; Unsupervised learning\Topic, graphical and latent variable models","recommender systems, probabilistic generative model, event-based social networks, location-based services, user preferences"295,Multi-Label Relational Neighbor Classification using Social Context Features,Research,"Networked data, extracted from social media, web pages, and bibliographic databases, can contain entities of multiple classes, interconnected through different types of links. In this paper, we focus on the problem of performing multi-label classification on networked data, where the instances in the network can be assigned multiple labels. In contrast to traditional content-only classification methods, relational learning succeeds in improving classification performance by leveraging the correlation of the labels between linked instances. However, instances in a network can be linked for various causal reasons, hence treating all links in a homogeneous way can limit the performance of relational classifiers.  In this paper, we propose a multi-label iterative relational neighbor classifier that employs social context features (SCRN). Our classifier incorporates a class propagation probability distribution obtained from instances’ social features, which are in turn extracted from the network topology. This class-propagation probability captures the node’s intrinsic likelihood of belonging to each class, and serves as a prior weight for each class when aggregating the neighbors’ class labels in the collective inference procedure. Experiments on different real-world datasets demonstrate that our proposed classifier can boost classification performance over several commonly used benchmarks on networked multi-label data.","Xi Wang, University of Central Florida; Gita Sukthankar*, University of Central Florida",Supervised learning\Classification*; Social\Community detection; Social\Social and information networks; Supervised learning; Supervised learning\Multi-label,"collective classification, relational neighbor classifiers, multi-label learning, edge clustering"341,Model Selection in Markovian Procsses,Research,"When analyzing data that originated from a dynamical system, a common practice is to encompass the problem in the well known frameworks of Markov Decision Processes (MDPs) and Reinforcement Learning (RL). The state space in these solutions is usually chosen in some heuristic fashion and the formed MDP can then be used to simulate and predict data, as well as indicate the best possible action in each state. The model chosen to characterize the data affects the complexity and accuracy of any further action we may wish to apply, yet few methods that rely on the dynamic structure to select such a model were suggested.  In this work we address the problem of how to use time series data to choose from a finite set of candidate discrete state spaces, where these spaces are constructed by a domain expert. We formalize the notion of model selection consistency in the proposed setup. We then discuss the difference between our proposed framework and the classical Maximum Likelihood (ML) framework, and give an example where ML fails. Afterwards, we suggest alternative selection criteria and show them to be weakly consistent. We then define weak consistency for a model construction algorithm and show a simple algorithm that is weakly consistent. Finally, we test the performance of the suggested criteria and algorithm on both simulated and real world data.","Assaf Hallak*, The Technion; Dotan Di-Castro, Technion; Shie Mannor, Technion",Mining rich data types\Temporal / time series*; Big Data\Novel statistical techniques for big data; Dimensionality reduction; Feature selection; Unsupervised learning\Clustering,"Model Selection, Reinforcement Learning, Markov Decision Processes, Dynamic Mailing Policies"392,Summarizing Probabilistic Frequent Patterns: A Fast Approach,Research,"Mining probabilistic frequent patterns from uncertain data has received a great deal of attention in recent years due to the wide applications. However, probabilistic frequent pattern mining suffers from the problem that an exponential number of result patterns are generated, which seriously hinders further evaluation and analysis. In this paper, we focus on the problem of mining probabilistic representative frequent patterns (P-RFP), which is the minimal set of patterns with adequately high probability to represent all frequent patterns.  Observing the bottleneck in checking whether a pattern can probabilistically represent another, which involves the computation of a joint probability of the supports of two patterns, we introduce a novel approximation of the joint probability with both theoretical and empirical proofs.  Based on the approximation, we propose an Approximate P-RFP Mining (APM) algorithm, which effectively and efficiently compresses the set of probabilistic frequent patterns.  To our knowledge, this is the first attempt to analyze the relationship between two probabilistic frequent patterns through an approximate approach. Our experiments on both synthetic and real-world datasets demonstrate that the APM algorithm accelerates P-RFP mining dramatically, orders of magnitudes faster than an exact solution. Moreover, the error rate of APM is guaranteed to be very small when the database contains hundreds transactions, which further affirms APM is a practical solution for summarizing probabilistic frequent patterns.","Chunyang Liu*, UTS; Ling Chen, ; Chengqi Zhang, QCIS, University of Technology, Sydney",Rule and pattern mining*,"Pattern Summarization, Uncertain Data"395,Network Discovery via Constrained Tensor Analysis of fMRI Data,Research,"We pose the problem of network discovery which involves simplifying spatio-temporal data into nodes and edges. Such problems naturally exist in fMRI scans of human subjects. These scans consist of activations of thousands of voxels over time with the aim to simplify them into the underlying cognitive network being used. We propose supervised and semi-supervised variations of this problem and postulate a constrained tensor decomposition formulation and a corresponding alternating least squares solver that is easily implementable. We show this formulation works well in controlled experiments where supervision is incomplete, superfluous and noisy and is able to recover the underlying ground truth network. We then show that for real fMRI data our approach can reproduce well known results in neurology regarding the default mode network in resting state healthy and Alzheimer affected individuals. Finally, we show that the reconstruction error of the decomposition provides a useful measure of the network strength and is useful at predicting key cognitive scores both by itself and with additional clinical information.","Ian Davidson*, University of California - Davis",Applications\Healthcare and medicine*; Mining rich data types; Unsupervised learning,"fMRI, spatial data"399,"Guided Learning for Role Discovery (GLRD): Framework, Algorithms, and Applications",Research,"Role discovery in graphs is an emerging area that allows analysis of complex graphs in an intuitive way. In contrast to community discovery, which finds groups of highly connected nodes, the role discovery problem finds groups of nodes that share similar topological structure in the graph, and hence a common role or function such as being a broker or a periphery node. However, existing work so far is completely unsupervised, which is undesirable for a number of reasons. We provide an alternating least squares framework that allows convex constraints to be placed on the role discovery problem, which can provide useful supervision. In particular we explore supervision to enforce i) sparsity, ii) diversity, and iii) alternativeness in the roles. We illustrate the usefulness of this supervision on various data sets and applications.","Sean Gilpin*, U.C. Davis; Tina Eliassi-Rad, ; Ian Davidson, University of California - Davis",Graph mining*; Semi-supervised learning,"Role discovery, graph mining"411,Simple and Deterministic Matrix Sketching,Research,"We adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting. The algorithm receives the rows of a large matrix $A \in \R^{n \times m}$ one after the other in a streaming fashion. For $\ell = \lceil1/ \eps \rceil$ it maintains a sketch matrix $B \in \R^ { \ell \times m}$ such that for any unit vector $x$ \[ \|Ax\|^2 \ge \|Bx\|^2 \ge  \|Ax\|^2 - \eps \|A\|_{f}^2 \ . \] Sketch updates per row in $A$ require amortized $O(m\ell)$ operations. This gives the first algorithm whose error guaranty decreases proportional to $1/\ell$ using $O(m \ell)$ space. Prior art algorithms produce bounds proportional to $1/\sqrt{\ell}$. Our experiments corroborate that the faster convergence rate is observed in practice. The presented algorithm also stands out in that it is: deterministic, simple to implement, and elementary to prove.  Regardless of streaming aspects, the algorithm can be used to compute a $1+\eps'$ approximation to the best rank $k$ approximation of any matrix $A \in \R^{n \times m}$. This requires $O(mn\ell')$ operations and $O(m\ell')$ space where $\ell' = \sum \sigma^2_i / \eps' \sigma^2_{k+1}$ and $\sigma_i$ are the singular values of $A$ in descending magnitude order. In many practical applications, e.g. PCA, $\ell'$ is assumed to be constant.","Edo Liberty*, Yahoo!",Data streams*; Big Data\Scalable methods; Dimensionality reduction,"Streaming matrix sketches, Matrix approximation, Low rank approximation"425,Discovering Latent Influence in Online Social Activities via Shared Cascade Poisson Processes,Research,"Many people share their activities with others through online communities. These shared activities have an impact on other users' activities. For example, users are likely to become interested in items that are adopted (e.g. liked, bought and shared) by their friends. In this paper, we propose a probabilistic model for discovering latent influence from sequences of item adoption events. An inhomogeneous Poisson process is used for modeling a sequence, in which adoption by a user triggers the subsequent adoption of the same item by other users. For modeling adoption of multiple items, we employ multiple inhomogeneous Poisson processes, which share parameters, such as influence for each user and relations between users. The proposed model can be used for finding influential users, discovering relations between users and predicting item popularity in the future. We present an efficient Bayesian inference procedure of the proposed model based on the stochastic EM algorithm. The effectiveness of the proposed model is demonstrated by using real data sets in a social bookmark sharing service.","Tomoharu Iwata*, NTT Communication Science Laboratories; Amar Shah, University of Cambridge; Zoubin Ghahramani, Cambridge University","Unsupervised learning\Topic, graphical and latent variable models*; Mining rich data types\Temporal / time series; Probabilistic methods; Social; User modeling","Poisson processes, latent variable models, Bayesian inference"434,Indexed Block Coordinate Descent for Large-Scale Linear Classiﬁcation with Limited Memory,Research,"Linear Classification has achieved complexity linear to the data size. However, in many applications, data contain large amount of samples that does not help improve the quality of model, but still cost much I/O and memory to process. In this paper, we show how a Block Coordinate Descent method based on Nearest-Neighbor Index can significantly reduce such cost when learning a dual-sparse model. In particular, we employ truncated loss function to induce a series of convex programs with superior dual sparsity, and solve each dual using Indexed Block Coordinate Descent, which makes use of Approximate Nearest Neighbor (ANN) search to select active dual variables without I/O cost on irrelevant samples. We prove that, despite the bias and weak guarantee from ANN query, the proposed algorithm has global convergence to the solution defined on entire dataset, with sublinear complexity in each iteration. Experiments in both sufficient and limited memory conditions show that the proposed approach learns times faster than other state-of-the-art solvers without sacrificing accuracy.","En-Hsu Yen*, National Taiwan University; Chun-Fu Chang, National Taiwan University; Ting-Wei Lin, National Taiwan University; Shan-Wei Lin, National Taiwan University; Shou-De Lin, National Taiwan University",Big Data\Large scale optimization*; Big Data; Big Data\Scalable methods; Nearest neighbors; Supervised learning\Classification; Supervised learning\Support vector machines,"Limited-Memory,Large-Scale,Linear Classification,Ramp-Loss,SVM,Block Coordinate Descent,Nearest-Neighbor"444,Active Learning and Search on Low-Rank Matrices,Research,"Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Low-rank matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. Using variational approximations or Markov chain Monte Carlo sampling to approximate the posterior distribution over models, we can choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many ""positive"" data points as possible. We evaluate our methods on simulated data, and also show their applicability to movie ratings prediction and the discovery of drug-target interactions.","Dougal Sutherland*, Carnegie Mellon University; Barnabás Póczos, Carnegie Mellon University; Jeff Schneider, ",Recommender systems\Collaborative filtering*; Adaptive learning\Active learning; Recommender systems; Recommender systems\Cold-start; Unsupervised learning\Matrix/tensor factorization,"Active matrix factorization, collaborative prediction, drug discovery"503,A Phrase Mining Framework for Recursive Construction of a Topical Hierarchy,Research,"A high quality hierarchical organization of the concepts in a dataset at different levels of granularity has many valuable applications such as search, summarization, and content browsing. In this paper we propose an algorithm for recursively constructing a hierarchy of topics from a collection of content-representative documents. We characterize each topic in the hierarchy by an integrated ranked list of mixed-length phrases. Our mining framework is based on a phrase-centric view for clustering, extracting, and ranking topical phrases. Experiments with datasets from three different domains illustrate our ability to generate hierarchies of high quality topics represented by meaningful phrases.","Chi Wang, University of Illinois; Marina Danilevsky*, University of Illinois; Nihit Desai, University of Illinois at Urbana-Champaign; Yinan Zhang, University of Illinois at Urbana-Champaign; Phuong Nguyen, University of Illinois at Urbana-Champaign; Thrivikrama Taula, University of Illinois at Urbana-Champaign; Jiawei Han, University of Illinois at Urbana-Champaign",Unsupervised learning*; Graph mining; Mining rich data types\Text; Probabilistic methods,"topical hierarchy, phrase mining, hierarchy construction, topical phrases, topical keyphrase extraction, phrase ranking, link-based mining"511,DTW-D: Time Series Semi-Supervised Learning from a Single Example,Research,"Classification of time series data is an important problem with applications in virtually every scientific endeavor. The large research community working on time series classification has typically used the UCR Archive to test their algorithms. In this work we argue that the availability of this resource has isolated much of the research community from the following reality, labeled time series data is often very difficult to obtain.  The obvious solution to this problem is the application of semi-supervised learning; however, as we shall show, direct applications of off-the-shelf semi-supervised learning algorithms do not typically work well for time series. In this work we explain why semi-supervised learning algorithms typically fail for time series problems, and we introduce a simple but very effective fix. We demonstrate our ideas on diverse real word problems.  ","Yanping Chen*, UCR; Bing Hu, ; Eamonn Keogh, University of California - Riverside; Gustavo  Batista, ",Mining rich data types\Temporal / time series*; Nearest neighbors; Semi-supervised learning; Supervised learning\Classification,"Time Series, Semi-Supervised Learning, Classification"528,Diversity Maximization Under Matroid Constraints,Research,"Aggregator websites typically present documents in the form of representative clusters.  In order for users to get a broader perspective, it is important to deliver a  diversified set of representative documents in those clusters.  One approach to diversification is to maximize  the average dissimilarity among documents.  Another way to capture diversity is to  avoid showing several documents from the same category (e.g. from the same news channel).  We model the latter approach as a (partition) matroid constraint, and  study diversity maximization problems under matroid constraints.  We present the first constant-factor approximation algorithm for this problem,  using a new technique.  Our local search $0.5$-approximation algorithm  is also the first constant-factor approximation for the max-dispersion problem under matroid constraints. Our combinatorial proof technique for maximizing diversity under matroid constraints  uses the existence of a family of Latin squares which may also be of independent interest.   In order to apply these diversity maximization algorithms in the context of aggregator websites and as a preprocessing step for our diversity maximization tool, we develop  greedy clustering algorithms that maximize weighted coverage of a predefined set of topics. Our algorithms are based on computing a set of cluster centers, where clusters are formed around them.  We show the better performance of our algorithms for diversity  and coverage maximization by running experiments on real (Twitter) and synthetic data in the context of real-time search over micro-posts.  Finally we perform a user study validating our algorithms and diversity metrics.","Zeinab Abbassi*, Columbia University; Vahab Mirrokni, Google; Mayur Thakur, Google",Foundations*; Graph mining; Recommender systems,"Diversity Maximization, Coverage Maximization, Product Search, Microblog Mining, Online Clustering, Matroid Constraints"543,Succinct Interval-Splitting Tree for Scalable Similarity Search of Compound-Protein Pairs with Property Constraints,Research,"Analyzing functional interactions between small compounds and proteins is indispensable in genomic drug discovery. Since rich information on various compound-protein interactions is available in recent molecular databases, strong demands for making best use of such databases require to invent powerful methods to help us find new functional compound- protein pairs on a large scale. We present the succinct interval-splitting tree algorithm (SITA) that efficiently performs similarity search in databases for compound-protein pairs with respect to both binary fingerprints and real-valued properties. SITA achieves both time and space efficiency by developing the data structure called interval-splitting trees, which enables to efficiently prune the useless portions of search space, and by incorporating the ideas behind wavelet tree, a succinct data structure to compactly represent trees. We experimentally test SITA on the ability to retrieve similar compound-protein pairs/substrate-product pairs for a query from large databases with over 200 million compound- protein pairs/substrate-product pairs and show that SITA performs better than other possible approaches.","Yasuo Tabei*, JST; Akihiro Kishimoto, IBM Research, Dublin; Masaaki Kotera, Kyoto University; Yoshihiro Yamanishi, Kyushu university",Applications\Healthcare and medicine*; Bioinformatics,"drug discovery, drug-target interaction prediction"556,Social Influence Based Clustering of Heterogeneous Information Networks,Research,"Social networks continue to grow in size and the type of information hosted. We witness a growing interest in clustering a social network of people based on both their social relationships and their participations in activity based information networks. In this paper, we present a social influence based clustering framework for analyzing heterogeneous information networks with three unique features. First, we introduce a novel social influence based vertex similarity metric in terms of both self-influence similarity and co-influence similarity. We compute self-influence and co-influence based similarity in terms of propagating heat diffusion kernel on social graph and on its associated activity graphs and influence graphs respectively. Second, we compute the combined social influence based similarity between each pair of vertices by unifying the self-similarity and multiple co-influence similarity scores through a weight function with an iterative update method. Third, we design an iterative learning algorithm, SI-Cluster, for social influence based graph clustering. It can dynamically refine the K clusters by continuously quantifying and adjusting the weights on self-influence similarity and on multiple co-influence similarity scores towards the clustering convergence. To make SI-Cluster converge fast, we transformed a sophisticated nonlinear fractional programming problem of multiple weights into a straightforward nonlinear parametric programming problem of single variable. Our experiment results show that SI-Cluster not only achieves a better balance between self-influence and co-influence similarities but also scales extremely well for large graph clustering. ","Yang Zhou*, Georgia Institute of Technolog; Ling Liu, Georgia Institute of Technology",Social\Social and information networks*; Social,"Graph Clustering, Social Influence, Heterogeneous Information Network"569,Selective Sampling on Graphs for Classification,Research,"Selective sampling is an active variant of online learning in which the learner is allowed to adaptively query the label of an observed example. The goal of selective sampling is to achieve a good trade-off between prediction performance and the number of queried labels. Existing selective sampling algorithms are designed for vector-based data. In this paper, motivated by the ubiquity of graph representations in real-world applications, we propose to study selective sampling on graphs. We first present an online version of the well-known Learning with Local and Global Consistency method (OLLGC). It is essentially a second-order online learning algorithm, and can be seen as an online ridge regression in the Hilbert space of functions defined on graphs. We prove its regret bound in terms of the structural property (cut size) of a graph, or the spectral property of graph Laplaican. Based on OLLGC, we present a selective sampling algorithm, namely Selective Sampling with Local and Global Consistency (SSLGC), which queries the label of each node based on the confidence of the linear function on graphs. Its bound on the label complexity is also derived. We analyze the low-rank approximation of graph kernels, which enables the online algorithms scale to large graphs. Experiments on benchmark graph datasets show that OLLGC outperforms the state-of-the-art first-order algorithm significantly, and SSLGC achieves comparable or even better results than OLLGC while querying substantially fewer nodes. Moreover, SSLGC is overwhelmingly better than random sampling.","Quanquan Gu*, CS, UIUC; Charu Aggarwal, IBM Research; Jialu Liu, UIUC; Jiawei Han, University of Illinois at Urbana-Champaign",Supervised learning*; Adaptive learning; Sampling; Semi-supervised learning\Learning with partial labels; Supervised learning\Classification,571,WiseMarket: A New Paradigm for Managing Wisdom of Online Social Users,Research,"The benefits of crowdsourcing are well-recognized today for an increasingly broad range of problems. Meanwhile, the rapid development of social media makes it possible to seek the wisdom of a crowd in a broader sphere. However, it is not trivial to implement crowdsourcing platform on social media, specifically to make social media users as workers, we need to address the following two challenges: 1) how to motivate users to participate in tasks, and 2) how to aggregate their opinions. In this paper, we present Wise Market as an effective institution of crowds on social media to motivate users to participate in a task with care and correctly aggregate their opinions on pairwise choice problems. The Wise Market consists of a set of investors each with an associated individual confidence in their prediction, and after investment, only the ones whose choices are the same as the whole market are granted rewards. Therefore, the social media user has to give his/her “best” answer in order to get reward, discouraging careless answers from sloppy users.Using this framework, we define the Effective Market Problem (EMP) as an optimization problem to minimize expected cost of paying out rewards while guaranteeing aminimum confidence level. We propose exact algorithms for calculating the market confidence and the expected cost with O(n log2 n) time cost in a Wise Market with n investors. Especially in the real situations of social media where the numbers of users are enormous, we design a Central Limit Theorem-based approximation algorithm for calculating the market confidence with O(n) time cost, as well as a bounded approximation algorithm for calculating the expected cost with O(n) time cost. Finally, we conducted empirical studies to validate effectiveness of the proposed algorithms on real and synthetic data.","Chen Cao, HKUST; Yongxin Tong, HKUST; Lei Chen*, HKUST; H.V.  Jagadish, University of Michigan","Social\Social and information networks*; Economy, markets",572,Querying Discriminative and Representative Samples for Batch Mode Active Learning,Research,"Empirical risk minimization (ERM) provides a principal guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label and these data may have different distribution. In this paper, we generalize the empirical risk minimization principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound we develop a practical batch mode active learning method. The proposed formulation involves a non-convex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying most uncertain and representative queries. Experiments on benchmark data sets and real-world applications demonstrate the superior performance of our proposed method in comparison with the state-of-the-art methods.","Zheng Wang*, Arizona State University; Jieping Ye, Arizona State University",Adaptive learning\Active learning*; Supervised learning\Classification,577,Recursive Regularization for Large-scale Classification with Hierarchical and Graphical Dependencies,Research,"The two key challenges in hierarchical classification are to leverage the hierarchical dependencies between the class-labels for improving performance, and, at the same time maintaining scalability across large hierarchies. In this paper we propose a regularization framework for large-scale hierarchical classification that addresses both the problems. Specifically, we incorporate the hierarchical dependencies between the class-labels into the regularization structure of the parameters thereby encouraging classes nearby in the hierarchy to share similar model parameters. Furthermore, we extend our approach to scenarios where the dependencies between the class-labels are encoded in the form of a graph rather than a hierarchy. To enable large-scale training, we develop a parallel-iterative optimization scheme that can comfortably handle the largest datasets with hundreds of thousands of classes and millions of instances and learning terabytes of parameters. Our experiments showed consistent and significant performance improvements over other competing approaches and achieved state-of-the-art results on leading benchmark datasets.","Siddharth Gopal*, CMU; Yiming Yang, CMU","Supervised learning\Classification*; Big Data\Distributed computing --- cloud, map-reduce, MPI, others; Big Data\Large scale optimization","Hierarchical Classification, Recursive Regularization, Parallel Optimization, Large-scale Evaluation"583,Denser than the densest subgraph: extracting optimal quasi-cliques with quality guarantees,Research,"Finding dense subgraphs is an important graph-mining task with many applications. Given that the direct optimization of edge density is not meaningful, as even a single edge achieves maximum density, research has focused on optimizing alternative density functions. A very popular among such functions is the average degree, whose maximization leads to the well-known \emph{densest-subgraph} notion.  Surprisingly enough, however, densest subgraphs are typically large graphs, with small edge density, and large diameter.  In this paper, we define a novel density function, which gives subgraphs of much higher quality than densest subgraphs: the graphs found by our method are compact, dense, and with smaller diameter. We show that the proposed function can be derived from a general framework, which includes other important density functions as subcases and for which we show interesting general theoretical properties.  We evaluate our algorithms on real and synthetic datasets, and we also devise several application studies as variants of our original problem. When compared with the method that finds the subgraph of the largest average degree, our algorithms return denser subgraphs with smaller diameter. Finally, our work opens new research directions which we discuss in detail.","Charalampos Tsourakakis*, Carnegie Mellon University; Francesco Bonchi, Yahoo! Research; Aristides Gionis, Aalto University; Francesco Gullo, Yahoo! Research; Maria Tsiarli, University of Pittsburgh",Graph mining*; Social\Social and information networks,"dense subgraph, algorithm engineering, experimentation"611,Connecting Users across Social Media Sites: A Behavioral-Modeling Approach,Research,"People use various social media for different purposes. The information on each site is often partial. When sources of complementary information are integrated, a better profile of a user can be built to improve online services such as verifying online information. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem.  We introduce a methodology (MOBIUS) for finding a mapping among identities of individuals across social media sites. It consists of three key components: the first component identifies users' unique behavioral patterns that lead to information redundancies across sites; the second component constructs features that exploit information redundancies due to these behavioral patterns; and the third component employs machine learning for effective user identification. We formally define the cross-media user identification problem, show that MOBIUS is effective in identifying users across social media sites, and discuss how further improvements can be made. This study paves the way for analysis and mining across social media sites, and facilitates the creation of novel online services across social media sites.","Reza Zafarani*, Arizona State University; Huan Liu, Arizona State University",Social\Social media*; User modeling; Web mining,user identification connecting users across sites behavioral modeling623,A General Bootstrap Performance Diagnostic,Research,"As datasets become larger, more complex, and more available to diverse groups of analysts, it would be quite useful to be able to automatically and generically assess the quality of estimates, much as we are able to automatically train and evaluate predictive models such as classifiers. However, despite the fundamental importance of estimator quality assessment in data analysis, this task has eluded highly automatic solutions.  While the bootstrap provides perhaps the most promising step in this direction, its level of automation is limited by the difficulty of evaluating its finite sample performance and even its asymptotic consistency.  Thus, we present here a general diagnostic procedure which directly and automatically evaluates the accuracy of the bootstrap's outputs, determining whether or not the bootstrap is performing satisfactorily when applied to a given dataset and estimator.  We show via an extensive empirical evaluation on a variety of estimators and simulated and real datasets that our proposed diagnostic is effective. Our experiments on a real-world query workload from Conviva Inc. involving 1.7TB of data (n=500M) show that the diagnostic correctly identifies that the bootstrap is applicable on 219 out of the 268 SQL-like queries, with 7 false positives and 12 false negatives.","Ariel Kleiner, ; Ameet Talwalkar*, UC Berkeley; Sameer Agarwal, ; Michael Jordan, ; Ion Stoica, ",Unsupervised learning*; Big Data\Novel statistical techniques for big data,"bootstrap diagnostic, quantifying estimator uncertainty"627, MI$^2$LS:  Multi-Instance Learning from Multiple Information Sources,Research,"In Multiple Instance Learning (MIL), each entity  is normally expressed as a set of instances.    Most of the current MIL methods only deal with the case when each instance is represented by one type of features. However, in many real world applications, entities are often described from several different information sources (views). For example, when applying MIL to image categorization, the characteristics of each image can be derived from both its RGB features and SIFT features. Previous research work has shown that, in traditional learning methods, by considering the consistencies between different information sources, the classification performance can be improved.   To incorporate  the consistencies between different  information sources into MIL, we propose a novel research framework --  Multi-Instance Learning from Multiple Information Sources (MI$^2$LS).  Based on this framework, an algorithm -- Fast MI$^2$LS (FMI$^2$LS) is designed for this research framework, which combines Concave-Convex Constraint Programming (CCCP) method and an adapted Stoachastic Gradient Descent (SGD) method. Some theoretical analysis on time complexity and generalization error bound are given based on the proposed method. Experimental results on three datasets, i.e., Reuters21578, WebKB,  and a novel application -- insider threat detection dataset, demonstrate the superior performance of the proposed method against several  state-of-the-art MIL techniques on both efficiency and effectiveness.","Dan Zhang*, Purdue University; Jingrui He, Stevens Institute of Technolog; Richard Lawrence, IBM Research",Supervised learning\Classification*; Big Data\Large scale optimization,"Multiple Instance Learning, Large Scale, Multi-View Learning"631,Learning to question: Leveraging user preferences for shopping advice,Research,"We present ShoppingAdvisor, a novel recommender system that helps users in shopping technical products in an e-commerce website. ShoppingAdvisor leverages both user preferences and technical product attributes in order to generate its suggestions. The system elicits user preferences via a binary-tree-shaped flowchart, where each node is a question to the user. At each node, ShoppingAdvisor suggests a ranked list of products that match the preferences of the user, and that gets progressively re fined along the path from the root of the tree to one of its leafs. In this paper we show (i) how to learn the structure of the tree, i.e., which questions to ask at each node, and (ii) how to produce a suitable ranking at each node. First, we adapt the classical top-down strategy for building decision trees in order to find the best user attribute to ask at each node. Differently than decision trees, ShoppingAdvisor partitions the user space rather than the product space. Second, we show how to employ a learning-to-rank approach in order to learn, at each node of the tree, a ranking of products appropriate to the users who reach at that node. We experiment with two real-world datasets for cars and cameras, and a synthetic one. We use mean reciprocal rank to evaluate ShoppingAdvisor, and show how the performance increases by more than 50% along the path from root to leaf. We also show how collaborative recommendation algorithms such as k-nearest neighbor benefits from feature selection done by the ShoppingAdvisor tree. Our experiments show that ShoppingAdvisor produces good quality interpretable recommendations, while requiring less input from users and being able to handle the cold-start problem. ","Mahashweta Das*, Univ of Texas at Arlington; Gianmarco De Francisci Morales, Yahoo! Research; Aristides Gionis, Aalto University; Ingmar Weber, Qatar Computing Research Institute",Recommender systems*; Applications\E-commerce; Social\Social media; Web mining,"recommendation, learning, ranking, collaborative content"668,Mining High Utility Episodes in Complex Event Sequences,Research,"Frequent episode mining (FEM) is an interesting research topic in data mining with wide range of applications. However, the traditional framework of FEM treats all events as having the same importance/utility and assumes that a same type of event appears at most once at a time point. These simplifying assumptions do not reflect the characteristics of scenarios in real applications and thus the useful information of episodes in terms of utilities such as profits is lost. Furthermore, most studies on FEM focused on mining episodes in simple event sequences and few considered the scenario of complex event sequences, where different events can occur simultaneously. To address these issues, in this paper, we incorporate the concept of utility into episode mining and address a new problem of mining high utility episodes from complex event sequences, which has not been explored so far. In the proposed framework, the importance/utility of different events is considered and multiple events can appear simultaneously. Several novel features are incorporated into the proposed framework to resolve the challenges raised by this new problem, such as the absence of anti-monotone property and the huge set of candidate episodes. Moreover, an efficient algorithm named UP-Span (Utility ePisodes mining by Spanning prefixes) is proposed for mining high utility episodes with several strategies incorporated for pruning the search space to achieve high efficiency. Experimental results on real and synthetic datasets show that UP-Span has excellent performance and serves as an effective solution to the new problem of mining high utility episodes from complex event sequences.","Cheng-Wei Wu, National Cheng Kung University; Yu Feng Lin, National Cheng Kung University, Taiwan, ROC; Philip Yu, University of Illinois; Vincent Tseng*, National Cheng Kung University",Rule and pattern mining*; Mining rich data types; Mining rich data types\Sequence,"Utility mining, episode mining, high utility episodes, complex event sequences"707,STRIP: Stream Learning of Influence Probabilities,Research,"Influence-driven diffusion of information is a fundamental process in social networks. Learning the latent variables of such process, i.e., the influence strength along each link, is a central question towards understanding the structure and function of complex networks, modeling information cascades, and developing applications such as viral marketing.  Motivated by modern microblogging platforms, such as twitter, in this paper we study the problem of learning influence probabilities in a data-stream scenario, in which the network topology is relatively stable and the challenge of a learning algorithm is to keep up with a continuous stream of tweets. Our contribution is a number of randomized approximation algorithms, categorized according to the available space (superlinear, linear, and sublinear in the number of nodes $n$) and according to different models (landmark and sliding window). Among several results, we show that we can learn influence probabilities with one pass over the data, using O(n log n) space, in both the landmark model and the sliding-window model, showing that our algorithm is within a logarithmic factor of optimal.  For truly large graphs, when one needs to operate with sublinear space, we show that we can still learning influence probabilities in one pass, assuming that we restrict our attention to the most active users.  Our thorough experimental evaluation on large social graph demonstrates that the empirical performance of our algorithms agrees with that predicted by the theory.","Konstantin Kutzkov, University of Copenhagen; Albert Bifet, Yahoo! Research; Francesco Bonchi*, Yahoo! Research; Aristides Gionis, Aalto University","Economy, markets\Viral marketing*; Data streams",722,Automatic selection of social media responses to news,Research,"Social media responses to news have increasingly gained in importance as they can enhance a consumer's news reading experience, promote information sharing and aid journalists in assessing their readership's response to a story. Given that the number of responses to an online news article may be huge, a common challenge is that of selecting only the most interesting responses for display.  This paper addresses this challenge by casting message selection as an optimization problem. We define an objective function which jointly models the messages' utility scores and their entropy. We propose a near-optimal solution to the underlying optimization problem which  leverages the submodularity property of the objective function. Our solution first learns the utility of individual messages in isolation and then produces a diverse selection of interesting messages by maximizing the defined objective function.  The intuition behind our work is that an interesting selection of messages contains diverse, informative, opinionated and popular messages referring to the news article, written mostly by users that have authority on the topic. Our intuitions are embodied by a rich set of content, social and user features capturing the aforementioned aspects. We evaluate our approach through both human and automatic experiments, and demonstrate it outperforms the state of the art.  Additionally, we perform an in-depth analysis of the annotated ``interesting'' responses, shedding light on the subjectivity around the selection process and the perception of interestingness.","Tadej _tajner*, Jo_ef Stefan Institute; Bart Thomee, Yahoo! Research; Ana Maria Popescu, Yahoo! Labs; Marco Pennacchiotti, eBay Inc.; Alejandro Jaimes, Yahoo!",Social\Social media*; Mining rich data types\Text; Sampling; Social,"Social Media, Microblogging, Information Overload, Information Filtering, Information Reduction, Sampling, Summarization"730,Collaborative Boosting for Activity Classification in Microblogs,Research,"Users' daily activities, such as dining and shopping, inherently reflect their habits, intents and preferences, thus provide invaluable information for services such as personalized information recommendation and targeted advertising. Users' activity information, although ubiquitous on social media, has largely been unexploited. This paper addresses the task of user activity classification in microblogs, where users can publish short messages and maintain social networks online. We identify the importance of modeling a user's individuality, and that of exploiting opinions of the user's friends for accurate activity classification. In this light, we propose a novel collaborative boosting framework comprising a text-to-activity classifier for each user, and a mechanism for collaboration between classifiers of users having social connections. The collaboration between two classifiers includes exchanging their own training instances and their dynamically changing labeling decisions. We propose an iterative learning procedure that is formulated as gradient descent in learning function space, while opinion exchange between classifiers is implemented with a weighted voting in each learning iteration. We show through experiments that on real-world data from Sina Weibo, our method outperforms existing off-the-shelf algorithms that do not take users' individuality or social connections into account.","Yangqiu Song*, HKUST; Zhengdong Lu, Huawei; Cane Wing-Ki Leung, Huawei; Qiang Yang, Hong Kong University of Science and Technology",Transfer learning*; Semi-supervised learning,752,Trace Complexity of Network Inference,Research,"The network inference problem consists of reconstructing the edge set of a network given traces representing the chronology of infection times as epidemics spread through the network. This problem is a paradigmatic representative of prediction tasks in machine learning that require deducing a latent structure from observed patterns of activity in a network, which often require an unrealistically large number of resources (e.g., amount of available data, or computational time). A fundamental question is to understand which properties we can predict with a reasonable degree of accuracy with the available resources, and which we cannot. We define the trace complexity as the number of distinct traces required to achieve high fidelity in reconstructing the topology of the hidden network or, more generally, some of its properties. We give simpler and more efficient algorithms for the objective of perfect reconstruction with high probability. Moreover, we prove that our algorithms are nearly optimal, by proving an information-theoretic lower bound on the number of traces that an optimal reconstruction algorithm requires for performing this task in the worst case. Given these strong lower bounds, we turn our attention to special cases, such as trees and bounded-degree graphs, and to property recovery tasks, such as reconstructing the degree distribution of an unobserved network. We show that these problems require a much smaller (and more realistic) number of traces, making them potentially solvable in practice.","Bruno Abrahao*, Cornell; Flavio Chierichetti, Sapienza University; Robert Kleinberg, Cornell; Alessandro Panconesi, Sapienza University of Rome",Graph mining*; Probabilistic methods; Sampling; Social\Social and information networks,network inference; information cascades770,Robust Sparse Estimation of Multiresponse Regression and Inverse Covariance Matrix via the L2 distance,Research,"We propose a robust framework to jointly perform two key modeling tasks involving high dimensional data: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among responses while adjusting for their predictors. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecifica- tion. This issue is exacerbated when dealing with high dimensional noisy data. In this work, we propose instead to minimize a regularized distance criterion, which is motivated by the minimum distance functionals used in nonparametric methods for their excellent robustness properties. The proposed estimates can be obtained efficiently by leveraging a sequential quadratic programming algorithm. We provide theoretical justification such as estimation consistency for the proposed estimator. Additionally, we shed light on the robustness of our estimator through its linearization, which yields a combination of weighted lasso and graphical lasso with the sample weights providing an intuitive explanation of the robustness. We demonstrate the merits of our framework through simulation study and the analysis of real financial and genetics data.","Aurelie Lozano*, IBM Research; Huijing Jiang, IBM Research; Xinwei Deng, Virginia Tech",Supervised learning*; Bioinformatics; Feature selection; Supervised learning\Regression,"Robust estimation, sparse learning, variable selection, high dimensional data, multiresponse regression, inverse covariance, L2E"773,Comparing Apples to Oranges: A Scalable Solution with Heterogeneous Hashing,Research,"Although hashing techniques have been popular for the large scale similarity search problem, most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment, i.e., the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world applications, there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogenous domains, e.g., recommending relevant posts and images to a certain Facebook user. In this paper, we address the problem of “comparing apples to oranges” under the large scale setting. Specifically, we propose a novel Relation-aware Heterogeneous Hashing(RaHH), which provides a general framework for generating hash codes of data entities sitting in multiple heterogenous domains. Unlike some existing hashing methods that map heterogeneous data in a common Hamming space, the RaHH approach constructs a Hamming space for each type of data entities, and learns optimal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover, the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with improved accuracy. To validate the proposed RaHH method, we conduct extensive evaluations on two large datasets crawled from the popular social media sites, i.e., Flickr and Tencent Weibo. The experimental results clearly demonstrate that the RaHH outperforms several state-of-the-art hashing methods with significant performance gains.","Mingdong Ou, Tsinghua University; Peng Cui*, Tsinghua University; Fei Wang, IBM T. J. Watson Research Lab; Jun Wang, IBM Research",Big Data\Scalable methods*; Recommender systems; Social,811,Text-Based Measures of Document Diversity,Research,"Quantitative notions of diversity are central to scientific disciplines ranging from conservation biology to the reflexive study of science. However, there has been relatively little work on measuring the diversity of text documents via their content. In this paper we present a text-based framework for quantifying the diversity of documents. We learn a topic model over a corpus of documents, and compute a distance matrix between pairs of topics based on measures such as topic co-occurrence. We then use these pairwise distance measures, combined with the distribution of topics in each document, to estimate each document's diversity relative to the rest of the corpus. Our approach provides several advantages over existing methods.  It is fully data-driven, requiring only the text from a corpus of documents as input, it produces human-readable explanations, and it can be generalized to score diversity of other entities such as authors, academic departments, or journals.  We describe experimental results on several large data sets which suggest that our approach does accurately capture document diversity.","Kevin Bache*, University of California, Irvine; Padhraic Smyth, UC Irvine; David Newman, University of California, Irvine","Unsupervised learning\Exploratory analysis*; Mining rich data types\Text; Probabilistic methods; Semi-supervised learning\Anomaly/novelty detection; Unsupervised learning; Unsupervised learning\Topic, graphical and latent variable models","Diversity, Text Mining, Interdisciplinarity"863,SVM_{pAUC}^{tight}: A New Support Vector Method for Optimizing Partial AUC Based on a Tight Convex Upper Bound,Research,"The area under the ROC curve (AUC) is a well known performance measure in machine learning and data mining. In an increasing number of applications, however, ranging from ranking applications to a variety of important bioinformatics applications, performance is measured in terms of the partial area under the ROC curve between two specified false positive rates. In recent work, we proposed a structural SVM based approach for optimizing this performance measure (Narasimhan and Agarwal, 2013). In this paper, we develop a new support vector method, SVM_{pAUC}^{tight}, that optimizes a tighter convex upper bound on the partial AUC loss, which leads to both improved accuracy and reduced computational complexity. In particular, by rewriting the empirical partial AUC risk as a maximum over subsets of negative instances, we derive a new formulation, where a modified form of the earlier optimization objective is evaluated on each of these subsets, leading to a tighter hinge relaxation on the partial AUC loss. As with our previous method, the resulting optimization problem can be solved using a cutting-plane algorithm, but the new method has better run time guarantees. We also discuss a projected subgradient method for solving this problem, which offers additional computational savings in certain settings. We demonstrate on a wide variety of bioinformatics tasks, ranging from protein-protein interaction prediction to drug discovery tasks, that the proposed method does, in many cases, perform significantly better on the partial AUC measure than the previous structural SVM approach. In addition, we also develop extensions of our method to learn sparse and group sparse models, often of interest in biological applications.","Harikrishna Narasimhan*, Indian Institute of Science; Shivani Agarwal, Indian Institute of Science, Bangalore",Supervised learning*; Bioinformatics; Supervised learning\Learning to rank; Supervised learning\Support vector machines,"Partial AUC, SVM, Cutting-plane Method, ROC Curve"897,Subsampling for Efficient and Effective Unsupervised Outlier Detection Ensembles,Research,"Outlier detection and ensemble learning are well established research directions in data mining yet the application of ensemble techniques to outlier detection has been rarely studied. Here, we propose and study subsampling as a technique to induce diversity among individual outlier detectors. We show analytically and experimentally that an outlier detector based on a subsample per se, besides inducing diversity, can, under certain conditions, already improve upon the results of the same outlier detector on the complete dataset. Building an ensemble on top of several subsamples is further improving the results. While in the literature so far the intuition that ensembles improve over single outlier detectors has just been transferred from the classification literature, here we also justify analytically why ensembles are also expected to work in the unsupervised area of outlier detection.  As a side effect, running an ensemble of several outlier detectors on subsamples of the dataset is more efficient than ensembles based on other means of introducing diversity and, depending on the sample rate and the size of the ensemble, can be even more efficient than just the single outlier detector on the complete data.  ","Arthur Zimek*, University of Alberta; Matthew Gaudet, University of Alberta; Ricardo J. G. Campello, University of Alberta; Jörg Sander, University of Alberta",Semi-supervised learning\Anomaly/novelty detection*,ensemble outlier detection914,Big Data Analytics with Small Footprint: Squaring the Cloud,Research,"This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost and power. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported *cluster* implementations for common benchmark problems. A key design criterion is *rapid exploration* of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computational kernels (iii), BIDMach, a machine learning system that includes efficient model optimizers, (iv) Butterfly mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative update algorithms. We present several benchmark problems as case studies to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem.","John Canny*, UC Berkeley; Huasha Zhao, UC Berkeley",Big Data*; Dimensionality reduction; Probabilistic methods; Supervised learning; Unsupervised learning,"Matrix Toolkit, Machine Learning Toolkit, Stochastic Gradient Descent, Factor Models, Regression, Matrix Completion, Distributed Inference, Benchmarks."921,A space efficient streaming algorithm for triangle counting using the birthday paradox,Research,"We study the problem of counting the number of triangles in a massive streamed graph. Modern massive graphs are truly \emph{dynamic graphs} and can be accurately characterized as a \emph{stream of edges}. How does one store a small sample of the past and still answer meaningful questions about the entire graph? This is the aim of a small space streaming algorithm.   We design a space efficient algorithm that approximates the transitivity (global clustering coefficient) with only a single pass through a graph of $n$ vertices. Our procedure is based on the classic probabilistic result, \emph{the birthday paradox}. Under conditions commonly held by social networks, we can prove that our algorithm requires $O(\sqrt{n})$ space to provide accurate estimates for the transitivity and number of triangles in the graph.  We run a detailed set of experiments on a variety of real graphs and demonstrate that the storage of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 50,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains \emph{a real-time estimate} of the transitivity/number of triangles of a graph, by storing a miniscule fraction of edges. ","Madhav Jha, Pennsylvania State University; C. Seshadhri*, Sandia National Labs; Ali Pinar, Sandia National Labs",Social\Social and information networks*; Big Data\Novel statistical techniques for big data; Data streams; Graph mining; Sampling,980,One Theme in All Views: Modeling Consensus Topics in Multiple Contexts,Research,"New challenges have been presented to classical topic models when applied to social media, as user-generated content suffers from significant problems of data sparseness. A variety of heuristic adjustments to these models have been proposed, with many based on the use of context information to improve the performance of topic modeling. Existing contextualized topic models rely on arbitrary manipulation of the model structure, by incorporating various context variables into the generative process of classical topic models in an ad hoc manner. Such manipulations usually result in much more sophisticated model structures and inference procedures, and substantial difficulty to generalize any of them to accommodate arbitrary types or combinations of contexts. In this paper we explore a different direction. We propose a general solution that is able to exploit multiple types of contexts without arbitrary manipulation of the structure of classical topic models. We formulate different types of contexts as multiple views of the partition of the corpus. A co-regularization framework is proposed to let the views collaborate with each other, vote for the consensus topics, and distinguish them from view-specific topics. Experiments with real world datasets prove that the proposed method is both effective and flexible to utilize arbitrary types of contexts.","Jian Tang*, Peking University; Ming Zhang, ; Qiaozhu Mei, University of Michigan","Mining rich data types\Text*; Probabilistic methods; Unsupervised learning\Clustering; Unsupervised learning\Topic, graphical and latent variable models; Web mining","topic modeling, co-regularization, user-generated content"1006,Information Cascade at Group Scale,Research,"Identifying the k most influential individuals in a social network is a well-studied problem. The objective is to detect k individuals in a (social) network who will influence the maximum number of people, if they are independently convinced of adopting a new strategy (product, idea, etc). There are cases  in real life, however, where we aim to instigate groups instead of individuals to trigger network diffusion. Such cases abound, e.g.,  billboards, TV commercials and  newspaper ads are utilized extensively to boost the popularity and raise awareness.  In this paper, we generalize the ``influential nodes'' problem. Namely we are interested to locate the most ``influential groups'' in a network. As the first paper to address this problem: we (1) propose a fine-grained model of information diffusion for the group-based problem, (2) show that the process is submodular and present an algorithm to determine the influential groups under this model (with a precise approximation bound), (3) propose a coarse-grained model that inspects the network at group level (not individuals) significantly speeding up calculations for large networks, (4) show that the diffusion function we design here is submodular in general case, and propose an approximation algorithm for this coarse-grained model, and finally by conducting experiments on real datasets, (5) demonstrate that seeding members of selected groups to be the first adopters can broaden diffusion (when compared to the influential individuals case). Moreover, we can identify these influential groups much faster (up to 12 million times speedup), delivering a practical solution to this problem.","Milad Eftekhar*, University of Toronto; Yashar Ganjali, University of Toronto; Nick Koudas, University of Toronto and Sysomos Inc","Social\Social and information networks*; Economy, markets\Viral marketing; Social","Social networks, Information cascade"1024,Debiasing Social Wisdom,Research,"With the explosive growth of social networks, many applications are increasingly harnessing the pulse of online crowds for a variety of tasks such as marketing, advertising, and opinion mining. An important example is the wisdom of crowd effect that has been well studied for such tasks when the crowd is non-interacting. However, these studies don't explicitly address the network effects in social networks. A key difference in this setting is the presence of social influences that arise from these interactions and undermine the wisdom of the crowd~\cite{LRS+11}.    Using a natural model of opinion formation, we analyze the effect of these interactions on an individual's opinion and estimate her propensity to conform. We then propose efficient sampling algorithms incorporating these conformity values to arrive at a debiased estimate of the wisdom of a crowd. We analyze the trade-off between the sample size and estimation error and validate our algorithms using both real data obtained from online user experiments and synthetic data.","Abhimanyu Das, Microsoft; Sreenivas Gollapudi*, Microsoft Research; Rina Panigrahy, Microsoft Research; Mahyar Salek, Microsoft",Social\Social and information networks*; Graph mining; Sampling,"social influence, wisdom of crowd, social sampling, opinion formation, debiasing"1041,Estimating Unbiased Sharer Reputation via Social Data Calibration,Research,"Online social networks have become important channels for users to share content with their connections and diffuse information. Although much work has been done to identify socially influential users, the problem of finding ``reputable'' sharers, {\em who share good content}, has received relatively little attention.  Availability of such reputation scores can be useful for various applications like recommending people to follow, procuring high quality content in a scalable way, creating a content reputation economy to incentivize high quality sharing, and many more. To estimate sharer reputation, it is intuitive to leverage data that records how recipients respond (through clicking, liking, etc.) to content items shared by a sharer.  However, such data is usually biased --- it has a {\em selection bias} since the shared items can only be seen and responded to by users connected to the sharer in most social networks, and it has a {\em response bias} since the response is usually influenced by the relationship between the sharer and the recipient (which may not indicate whether the shared content is good).  To correct for such biases, we propose to utilize an additional data source that provides {\em unbiased} goodness estimates for a small set of shared items, and calibrate biased social data through a novel multi-level hierarchical model that describes how the unbiased data and biased data are jointly generated according to sharer reputation scores. The unbiased data also provides the ground truth for quantitative evaluation of different methods.  Experiments based on such ground-truth data show that our proposed model significantly outperforms existing methods that estimate social influence using biased social data. ","Jaewon Yang, Stanford University; Bee-Chung Chen*, LinkedIn; Deepak Agarwal, LinkedIn",User modeling*; Social\Social media,1045,Linking Named Entities in Tweets with Knowledge Base via User Interest Modeling,Research,"Twitter has become an increasingly important source of information, with more than 400 million tweets posted per day. The task to link the named entity mentions detected from tweets with the corresponding real world entities in the knowledge base is called tweet entity linking. This task is of practical importance and can facilitate many different tasks, such as personalized recommendation and user interest discovery. The tweet entity linking task is challenging due to the noisy, short, and informal nature of tweets. Previous methods focus on linking entities in Web documents, and largely rely on the context around the entity mention and the topical coherence between entities in the document. However, these methods cannot be effectively applied to the tweet entity linking task due to the insufficient context information contained in a tweet. In this paper, we propose KAURI, a novel graph-based unified framework to collectively link all the named entity mentions in all tweets posted by a user via modeling the user's topics of interest. Our assumption is that each user has an underlying topic interest distribution over various named entities. To model this information, we propose a graph-based user interest propagation algorithm which integrates the global user interest information across tweets with the intra-tweet local information. We extensively evaluated the performance of KAURI over manually annotated tweet corpus, and the experimental results show that KAURI significantly outperforms the baseline methods in terms of accuracy, and KAURI is efficient and scales well to tweet stream.","Wei Shen*, Tsinghua University; Jianyong Wang, Tsinghua University; Ping Luo, HP Lab; Min Wang, Google Research",Mining rich data types\Unstructured*; Social\Social media; User modeling,1083,Redundancy-Aware Maximal Cliques,Research,"Recent research efforts have made notable progress in improving the performance of (exhaustive) maximal clique enumeration (MCE). However, exiting algorithms still suffer from exploring the huge search space of MCE. Furthermore, their results are often undesirable as many of the returned maximal cliques have large overlapping parts. This redundancy leads to problems in both computational efficiency and usefulness of MCE.  In this paper, we aim at providing a concise and complete summarization of the set of maximal cliques, which is useful to many applications. We propose the notion of \tau-visible MCE to achieve this goal and design algorithms to realize the notion. Based on the refined output space, we further consider an efficient computation of the top-$k$ results with guaranteed quality and diversity. Our experimental results demonstrate that our approach is capable of producing output of high usability and our algorithms achieve superior efficiency over classic MCE algorithms.","Jia Wang*, Chinese University of Hong Kong; James Cheng, Chinese University of Hong Kong; Ada Wai-Chee Fu, Chinese University of Hong Kong ",Graph mining*; Mining rich data types\Unstructured; Sampling; Social\Community detection,"Maximal clique enumeration, clique summarization, clique concise representation, redundancy-aware cliques"1136,Confluence: Conformity Influence in Large Social Networks,Research,"Conformity is a type of social influence involving a change in opinion or behavior in order to fit in with a group. Employing several social networks as the source for our experimental data, we study how the effect of conformity plays a role in changing users' online behaviors. We formally define several major types of conformity in individual, peer, and group levels. We propose {\it Confluence} model  to formalize the effects of social conformity into a probabilistic model. Confluence can distinguish and quantify the effects of the different types of conformities. To scale up to large social networks, we propose a distributed learning method that can construct the Confluence model efficiently with near linear speedup.   Our experimental results on four different types of large social networks, i.e., Flickr, Gowalla, Weibo and Co-Author, verify the existence of the conformity phenomena. Leveraging the conformity information, Confluence can accurately predict actions of users. Our experiments show that  Confluence can significantly improve the prediction accuracy by up to 5-10\% compared with several alternative methods. ","Jie Tang*, Tsinghua University; Sen Wu, Tsinghua University; Jimeng Sun, IBM Research ",Social\Social and information networks*; Social\Social media,"Conformity, Social influence, Social network"1148,Mining Discriminative Subgraphs from Global-state Networks,Research,"Global-state networks provide a powerful mechanism to model the increasing heterogeneity in data generated by current systems. Such a network comprises of a series of network snapshots with dynamic local states at nodes, and a global network state indicating the occurrence of an event. Mining discriminative subgraphs from global-state networks allows us to identify the influential sub-networks that have maximum impact on the global state and unearth the complex relationships between the local entities of a network and their collective behavior. In this paper, we explore this problem and design a technique called MINDS to mine minimally discriminative subgraphs from large global-state networks. To combat the exponential subgraph search space, we derive the concept of an edit map and perform Metropolis Hastings sampling on the map to compute the answer set. Furthermore, we formulate the idea of network-constrained decision trees to learn prediction models on a subgraph without compromising on the underlying network structure. Extensive experiments on real datasets demonstrate excellent accuracy in terms of prediction quality. Additionally, MINDS achieves a speed-up of at least two orders of magnitude over baseline techniques.","Sayan Ranu*, IBM; Minh Hoang, UC Santa Barbara; Ambuj Singh, UC Santa Barbara",Graph mining*; Bioinformatics; Rule and pattern mining; Sampling,"Protein interaction network, bioinformatics, graph mining"1162,Scalable Text and Link Analysis with Mixed-Topic Link Models ,Research,"Many data sets contain rich information about objects, as well as pairwise relations between them.  For instance, in networks of websites, scientific papers, and other documents, each node has content consisting of a collection of words, as well as hyperlinks or citations to other nodes. In order to perform inference on such data sets, and make predictions and recommendations, it is useful to have models that are able to capture the processes which generate the text at each node and the links between them. In this paper, we combine classic ideas in topic modeling with a variant of the mixed-membership block model recently developed in the statistical physics community.  The resulting model has the advantage that its parameters, including the mixture of topics of each document and the resulting overlapping communities, can be inferred with a simple and scalable expectation-maximization algorithm. We test our model on three data sets, performing unsupervised topic classification and link prediction. For both tasks, our model outperforms several existing state-of-the-art methods, achieving higher accuracy with significantly less computation, analyzing a data set with 1.3 million words and 44 thousand links in a few minutes.","YAOJIA ZHU*, University of New Mexico; Xiaoran  Yan, University of New Mexico; Lise Getoor, The University of Maryland College Park; Cristopher Moore, Santa Fe Institute","Probabilistic methods*; Social\Community detection; Social\Link prediction; Social\Social and information networks; Unsupervised learning\Clustering; Unsupervised learning\Topic, graphical and latent variable models","Document classification, Topic modeling, Link prediction, Stochastic block model"1167,Exact Sparse Recovery with L0 Projections,Research,"Many applications concern sparse signals, for example, detecting anomalies from the differences between consecutive images taken by surveillance cameras. This paper focuses on the problem of  recovering a  $K$-sparse signal $\mathbf{x}\in\mathbb{R}^{1\times N}$, i.e., $K\ll N$ and $\sum_{i=1}^{N} 1\{x_i\neq 0\} = K$. In the  mainstream framework of compressed sensing (CS), $\mathbf{x}$ is recovered from $M$  linear measurements $\mathbf{y} = \mathbf{xS}\in\mathbb{R}^{1\times M}$, where $\mathbf{S}\in\mathbb{R}^{N\times M}$ is often a Gaussian (or Gaussian-like) design matrix.  In our proposed method, the design matrix $\mathbf{S}$ is generated from an $\alpha$-stable distribution with $\alpha\approx 0$. Our decoding algorithm  mainly requires  one linear scan of the coordinates, followed by a few iterations on a small number of coordinates which are ``undetermined'' in the previous iteration. Our practical algorithm consists of two estimators. In the first iteration, the {\em (absolute) minimum estimator} is able to  filter out a majority of the zero coordinates. The {\em gap estimator}, which is applied in each iteration, can accurately recover the magnitudes of the nonzero coordinates.  Comparisons with linear programming (LP) and orthogonal matching pursuit (OMP) demonstrate that our algorithm can be significantly faster in  decoding speed and more accurate in recovery quality, for the task of exact spare recovery. Our procedure is robust against measurement noise. Even when there are no sufficient measurements,  our algorithm can still reliably recover a significant portion of the nonzero coordinates.    ","Ping Li*, Cornell University; Cun-Hui Zhang, Rutgers University",Dimensionality reduction*; Big Data\Novel statistical techniques for big data; Data streams; Probabilistic methods,"Sparsity, compressed sensing, random projections"1186,Clustered Graph Randomization: Network Exposure to Multiple Universes,Research,"A/B testing is a standard approach for evaluating the effect of online experiments; the goal is to estimate the `average treatment effect' of a new feature or condition by exposing a sample of the overall population to it. A drawback with A/B testing is that it is poorly suited for experiments involving social interference, when the treatment of individuals spills over to neighboring individuals along an underlying social network. In this work, we propose a novel methodology for using graph clustering to analyze average treatment effects under social interference. To begin, we characterize graph-theoretic conditions under which individuals can be considered to be `network exposed' to an experiment. We then show how clustered graph randomization admits an efficient exact algorithm to compute the probabilities for each vertex being network exposed under several of these exposure conditions. Using these probabilities as inverse weights, a Horvitz-Thompson estimator can then provide an effect estimate that is unbiased, provided that the exposure model has been properly specified.   Given an estimator that is unbiased, we focus on minimizing the variance. First, we develop simple sufficient conditions for the variance of the estimator to be asymptotically small in n, the size of the graph. However, for general randomization schemes, this variance can be lower bounded by an exponential function of the degrees of a graph. In contrast, we show that if a graph satisfies a condition on the growth rate of neighborhoods, then there exists a natural clustering algorithm, based on vertex neighborhoods, for which the variance of the estimator can be upper bounded by a linear function of the degrees. Thus we show that proper cluster randomization can lead to exponentially lower estimator variance when experimentally measuring average treatment effects under interference.","Johan Ugander*, Cornell University; Brian Karrer, Facebook; Lars Backstrom, Facebook; Jon Kleinberg, Cornell",Social\Social and information networks*; Graph mining; Social\Community detection,"online experimentation, causal inference, network effects"1199,Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation,Research,"In the internet era there has been an explosion in the amount of digital text information available, leading to difficulties of scale for traditional inference algorithms for topic models.  Recent advances in stochastic variational inference algorithms for latent Dirichlet allocation (LDA) have made it feasible to learn topic models on large-scale corpora, but these methods do not currently take full advantage of the collapsed representation of the model.  We propose a stochastic algorithm for collapsed variational Bayesian inference for LDA, which is simpler and more efficient than the state of the art method.  In experiments on large-scale text corpora, the algorithm was found to converge faster and often to a better solution than the previous method.  Human-subject experiments also demonstrated that the method can learn coherent topics in seconds on small corpora, facilitating the use of topic models in interactive document analysis software.","James Foulds*, UC Irvine; Levi Boyles, UC Irvine; Christopher Dubois, UC Irvine; Padhraic Smyth, UC Irvine; max Welling, University of Amsterdam","Unsupervised learning\Topic, graphical and latent variable models*; Big Data\Scalable methods; Mining rich data types\Text; Probabilistic methods; Unsupervised learning","Topic models, latent Dirichlet allocation, online inference, stochastic inference, variational Bayesian inference"1205,Exploring Consumer Psychology for Click Prediction in Sponsored Search,Research,"Sponsored search thrives to be the major business model for commercial search engines. Precise predicting of the probability that users click on ads plays a key role in sponsored search, since such prediction is widely used in ranking, filtering, and pricing of the ads. Many of previous studies on click prediction usually apply the machine learning approach, and take advantage of two major kinds of features, including semantic features representing relevance between ads and queries as well as historical click-through features. These existing works, however, ignored one important aspect, the commercial nature, of the sponsor search. They make little attempt to understand user clicks on ads from the perspective of consumer psychology. In this work, through a data analysis on a commercial search engine, we find that many ads in sponsored search usually leverage some specific text patterns, e.g, “official site”, “x% off”, “guaranteed return in x days”, to attract consumer psychological desires, , and those text patterns can give rise to significant difference in terms of click-through rate. Based on these observations, it becomes an important problem of how to explore the consumer psychological desires to improve the accuracy of click prediction in sponsored search. To address this issue, we first propose a method to automatically mine patterns representative for  the consumer psychological desires from ads' text. Based on extracted text patterns, we propose new features and incorporate them into the learning framework of click prediction in sponsored search. Large scale evaluations on the click-through log from a commercial search engine demonstrate that our proposed consumer psychological desire features can result in significant improvement in terms of accuracy of click prediction in sponsored search. ","Taifeng Wang*, Microsoft; Jiang Bian, ; Tie-Yan Liu, Microsoft Research","User modeling*; Economy, markets\Online advertising; Rule and pattern mining; Supervised learning\Classification; Web mining","Click Prediction, Sponsored Search, Psychological Advertising, Consumer Behavior"1287,Model-based Kernel for Efficient Time Series Analysis,Research,"We present novel, efficient, model based kernels for time series data rooted in the reservoir computation framework. The kernels are implemented by fitting reservoir models sharing the same fixed deterministically constructed state transition part to individual time series. The proposed kernels can naturally handle time series of different length without the need to specify a parametric model class for the time series. Compared with other time series kernels, our kernels are computationally efficient. We show how the model distances used in the kernel can be calculated analytically or efficiently estimated. The experimental results on synthetic and benchmark time series classification tasks confirm the efficiency of the proposed kernel in terms of both generalization accuracy and computational speed. This paper also investigates on-line reservoir kernel construction for extremely long time series.","Huanhuan Chen*, University of Birmingham; Fengzhen Tang, University of Birmingham; Peter Tino, University of Birmingham; Xin Yao, University of Birmingham",Supervised learning\Classification*; Mining rich data types\Temporal / time series; Supervised learning\Neural networks; Supervised learning\Support vector machines,"Reservoir Computing, Time Series Classification, Kernel Methods"