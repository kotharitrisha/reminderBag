<?xml version="1.0" encoding="UTF-8"?>
<?mso-application progid="Excel.Sheet"?><Workbook xmlns="urn:schemas-microsoft-com:office:spreadsheet" xmlns:c="urn:schemas-microsoft-com:office:component:spreadsheet" xmlns:html="http://www.w3.org/TR/REC-html40" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:ss="urn:schemas-microsoft-com:office:spreadsheet" xmlns:x2="http://schemas.microsoft.com/office/excel/2003/xml" xmlns:x="urn:schemas-microsoft-com:office:excel" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><OfficeDocumentSettings xmlns="urn:schemas-microsoft-com:office:office"><Colors><Color><Index>3</Index><RGB>#c0c0c0</RGB></Color></Colors></OfficeDocumentSettings><ExcelWorkbook xmlns="urn:schemas-microsoft-com:office:excel"><WindowHeight>9000</WindowHeight><WindowWidth>13860</WindowWidth><WindowTopX>240</WindowTopX><WindowTopY>75</WindowTopY><ProtectStructure>False</ProtectStructure><ProtectWindows>False</ProtectWindows></ExcelWorkbook><Styles><Style ss:ID="Default" ss:Name="Default"/><Style ss:ID="Result" ss:Name="Result"><Font ss:Bold="1" ss:Italic="1" ss:Underline="Single"/></Style><Style ss:ID="Result2" ss:Name="Result2"><Font ss:Bold="1" ss:Italic="1" ss:Underline="Single"/></Style><Style ss:ID="Heading" ss:Name="Heading"><Font ss:Bold="1" ss:Italic="1" ss:Size="16"/></Style><Style ss:ID="Heading1" ss:Name="Heading1"><Font ss:Bold="1" ss:Italic="1" ss:Size="16"/></Style><Style ss:ID="co1"/><Style ss:ID="ta1"/><Style ss:ID="ce1"/></Styles><ss:Worksheet ss:Name="Research"><Table ss:StyleID="ta1"><Column ss:Span="6" ss:Width="64.008"/><Row ss:Height="12.1032"><Cell><Data ss:Type="String">Research</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:StyleID="ce1"><Data ss:Type="String">Paper ID</Data></Cell><Cell ss:StyleID="ce1"><Data ss:Type="String">Paper Title</Data></Cell><Cell ss:StyleID="ce1"><Data ss:Type="String">Track Name</Data></Cell><Cell ss:StyleID="ce1"><Data ss:Type="String">Abstract</Data></Cell><Cell ss:StyleID="ce1"><Data ss:Type="String">Author Names</Data></Cell><Cell ss:StyleID="ce1"><Data ss:Type="String">Subject Areas</Data></Cell><Cell ss:StyleID="ce1"><Data ss:Type="String">Free text keywords</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">9</Data></Cell><Cell><Data ss:Type="String">Where, When and What: Discover Spatio-Temporal Topics for Twitter Users</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Micro-blogging services, such as Twitter, and location-based social network applications have generated short text messages associated with geographic information, posting time, and user ids. The availability of such data received from users offers a good opportunity to understand the user's spatial-temporal behavior and preference. In this paper, we propose a probabilistic model to exploit such data to understand individual users' mobility behavior from spatial, temporal and activity aspects. To the best of our knowledge, our work offers the first solution to jointly model individual user's mobility behavior from the three aspects. Our model has a variety of applications, such as user profiling and location prediction; it can be employed to answer questions such as ``Can we infer the location of a user given a tweet posted by the user and the posting time?" Experimental results on two real-world data sets show that the proposed model is effective in discovering users' spatial-temporal topics, and outperforms state-of-the-art baseline significantly for the task of location prediction for tweets.</Data></Cell><Cell><Data ss:Type="String">Quan Yuan*, Nanyang Technological Univ.; Gao Cong, Nanyang Technological University; Zongyang Ma, Nanyang Technological University; Aixin Sun, Nanyang Technological University; Nadia Thalmann, Nanyang Technological University</Data></Cell><Cell><Data ss:Type="String">User modeling*; Mining rich data types; Mining rich data types\Spatial; Mining rich data types\Text; Social\Social media; Unsupervised learning\Topic, graphical and latent variable models</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">10</Data></Cell><Cell><Data ss:Type="String">Multi-Label Classification by Mining Label and Instance Correlations from Heterogeneous Information Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Multi-label classification is prevalent in many real-world applications, where each example can be associated with a set of multiple labels simultaneously. The key challenge of multi-label classification comes from the large space of all possible label sets, which is exponential to the number of candidate labels. Most previous work focuses on exploiting correlations among different labels to facilitate the learning process. It is usually assumed that the label correlations are either given beforehand or can be derived directly from data samples by counting their label co-occurrences. However, in many real-world tasks, the label correlations are neither given, nor feasible to learn directly from data samples of moderate sizes. Heterogeneous information networks can provide abundant knowledge about relationships among different types of entities including data samples and label concepts. In this paper, we propose to use heterogeneous information networks to facilitate the multi-label classification process. By mining the linkage structure of heterogeneous information networks, multiple types of relationships among different label concepts and the data samples can be extracted. Then we can use these relationships to effectively induce the correlations among different class labels in general, as well as the dependencies among the label sets of inter-connected data examples. Empirical studies on real-world tasks demonstrate that the performance of multi-label classification can be effectively boosted using heterogeneous information networks.</Data></Cell><Cell><Data ss:Type="String">Xiangnan Kong, Univ. of Illinois at Chicago; Bokai Cao, Renmin University of China; Philip Yu*, University of Illinois</Data></Cell><Cell><Data ss:Type="String">Supervised learning\Multi-label*; Supervised learning; Supervised learning\Classification</Data></Cell><Cell><Data ss:Type="String">Multi-label classification, heterogeneous information network, label correlations</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">18</Data></Cell><Cell><Data ss:Type="String">Fast Structure Learning in Generalized Stochastic Processes with Latent Factors</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Understanding and quantifying the impact of unobserved processes in the multivariate time series data is a major challenge in analysis of multivariate stochastic processes. In this paper, we analyze a flexible stochastic process model, the generalized linear auto-regressive process (GLARP) and identify the conditions with which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood. To illustrate the power of this framework, we examine three examples, including two popular models for count data, i.e, Poisson and Conwey-Maxwell Poisson vector auto-regressive processes, and one powerful but less studies model for continuous data, i.e., Pareto vector auto-regressive processes for heavy-tailed time series data. We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it. Experiment results on two synthetic datasets, one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models.</Data></Cell><Cell><Data ss:Type="String">Mohammad Taha Bahadori*, University of Southern Califor; Yan Liu, University of Southern California; Eric Xing, CMU</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Temporal / time series*</Data></Cell><Cell><Data ss:Type="String">Time Series Analysis, Temporal Dependency Analysis, Latent Factors, Convex Optimization</Data></Cell></Row><Row ss:Height="68.652"><Cell><Data ss:Type="String">40</Data></Cell><Cell><Data ss:Type="String">Accurate Intelligible Models with Pairwise Interactions</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Generalized additive models (GAMs) combine single-feature models called shape functions through a linear function.  Previous studies have shown that while GAMs can easily be interpreted by users, the accuracy of GAMs is significantly lower than that of complex models.

In this paper, we suggest to add selected terms of interacting pairs of features to a GAM. The resulting model class, which we call \ensuremath{GA^{2}M}-Models, for \emph{Generalized Additive Models plus Interactions}, consists of univariate variate terms and a small number of feature interaction terms. Since they only include one-dimensional and two-dimensional components, the components of \ensuremath{GA^{2}M}-models can be visualized and interpreted by users. To overcome the bottleneck of selecting good pairs of features, we develop a novel, computationally efficient method called FAST for ranking all possible pairs of features as candidates for inclusion into the model.

Our large-scale empirical study shows the effectiveness of FAST in ranking candidate pairs of features. In addition, we show the surprising result that on our datasets, \ensuremath{GA^{2}M}-models have almost the same performance as the best full-complexity models. Thus this paper postulates that for many problems, \ensuremath{GA^{2}M}-models can yield models that
are both intelligible and accurate.</Data></Cell><Cell><Data ss:Type="String">Yin Lou*, Cornell University; Rich Caruana, Microsoft Research; Johannes Gehrke, Cornell University; Giles Hooker, Cornell University</Data></Cell><Cell><Data ss:Type="String">Supervised learning*; Big Data\Scalable methods; Supervised learning\Classification; Supervised learning\Learning to rank; Supervised learning\Regression</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">41</Data></Cell><Cell><Data ss:Type="String">Spotting Opinion Spammers using Behavioral Footprints</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making in e-commerce. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or to demote some target products. In recent years, fake review detection has attracted significant attention from both the business and research communities. Several research papers have been published. However, due to the difficulty of human labeling required for supervised learning and evaluation, the problem remains to be highly challenging. This work proposes a novel angle to the problem by modeling “spamicity” as latent. An unsupervised model, called Author Spamicity Model (ASM) is proposed. We work in the Bayesian setting, which facilitates modeling spamicity of authors as latent and allows us to exploit various observed behavioral footprints of reviewers. The intuition hinges on the hypothesis that opinion spammers have different behavioral distributions than non-spammers. This creates a distributional divergence between the latent population distributions of two clusters: spammers and non-spammers. Model inference results in learning the population distributions of the two clusters. Several extensions of ASM are also considered leveraging from different priors. Experiments on a real-life Amazon review dataset demonstrate the effectiveness of the proposed models which significantly outperform other state-of-the-art competitors.</Data></Cell><Cell><Data ss:Type="String">ARJUN MUKHERJEE*, UIC; Abhinav Kumar, ; Bing Liu, ; Junhui Wang, ; meichun Hsu, ; Malu Castellanos, ; riddhiman Ghosh, </Data></Cell><Cell><Data ss:Type="String">Security and privacy\Spam detection*; Applications</Data></Cell><Cell><Data ss:Type="String">Abuse, Opinion Spam, Fake Reviewer Detection</Data></Cell></Row><Row ss:Index="9" ss:Height="326.0952"><Cell><Data ss:Type="String">52</Data></Cell><Cell><Data ss:Type="String">TurboGraph: A Fast Parallel Graph Engine Handling Billion-scale Graphs in a Single PC</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Graphs are used to model many real objects such as social networks,
web graphs, chemical compounds,  and biological structures. Many
real applications in various fields require efficient and effective
management of large-scale graph structured data. Although
distributed graph engines such as \GBase and \Pregel handle
billion-scale graphs, the user must be skilled at managing and
tuning a distributed system in a cluster, which is a nontrivial job
for the ordinary user. Furthermore, these distributed systems need
many machines in a cluster in order to provide reasonable
performance. In order to address this problem, a disk-based parallel
graph engine called \GraphChi, has been recently proposed. Although
\GraphChi significantly outperforms all representative (disk-based)
distributed graph engines, we observe that \GraphChi still has
serious performance problems for many important types of graph
queries due to 1) limited parallelism and 2) separate steps for I/O
processing and CPU processing. In this paper, we propose a general,
disk-based graph engine called \TurboGraph\footnote
{https://sites.google.com/site/turbographdb/} to process
billion-scale graphs very efficiently by using modern hardware on a
single PC. \TurboGraph is the first truly parallel graph engine that
exploits 1) \emph{full parallelism} including multi-core parallelism
and FlashSSD IO parallelism and 2) \emph{full overlap} of CPU
processing and I/O processing as much as possible. Specifically, we
propose a novel parallel execution model, called \PinSlide.
\TurboGraph also provides engine-level operators such as BFS which
are implemented under the pin-and-slide model. Extensive
experimental results with large real datasets show that \TurboGraph
consistently and significantly outperforms \GraphChi by up to four
orders of magnitude.</Data></Cell><Cell><Data ss:Type="String">Wook-Shin Han*, POSTECH; Sangyeon Lee, POSTECH; Kyungyeol  Park, POSTECH; Jeong-Hoon Lee, POSTECH; Min-Soo Kim, DGIST; Jinha Kim, POSTECH; Hwanjo Yu, Pohang University of Science and Technology</Data></Cell><Cell><Data ss:Type="String">Big Data*; Big Data\Large scale optimization; Big Data\Scalable methods; Social\Social and information networks</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">93</Data></Cell><Cell><Data ss:Type="String">Flexible and Robust Co-regularized Multi-Domain Graph Clustering</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this paper, we propose a flexible and robust framework, CGC (Co-regularized Graph Clustering), based on non-negative matrix factorization (NMF), to tackle these challenges.  CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. Extensive experimental results on UCI benchmark data sets, newsgroup data sets and biological interaction networks demonstrate the effectiveness of our approach.</Data></Cell><Cell><Data ss:Type="String">Wei Cheng*, UNC at Chapel Hill; xiang Zhang, Case Western Reserve University; Patrick Sullivan, UNC at Chapel Hill; Wei Wang, University of California, Los Angeles</Data></Cell><Cell><Data ss:Type="String">Graph mining*; Applications\Healthcare and medicine; Bioinformatics; Social\Social and information networks; Transfer learning; Unsupervised learning\Clustering</Data></Cell><Cell><Data ss:Type="String">graph clustering; nonnegative matrix factorization; co-regularization</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">95</Data></Cell><Cell><Data ss:Type="String">Density-Based Logistic Regression</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">This paper introduces a nonlinear logistic regression model for classification. The main idea is to map the data to a feature space based on kernel density estimation. A discriminative model is then learned to optimize the feature weights as well as the bandwidth of a Nadaraya-Watson kernel density estimator. We then propose a hierarchical optimization algorithm for learning the coefficients and kernel bandwidths in an integrated way. Compared to other nonlinear models such as kernel logistic regression (KLR) and SVM, our approach is far more efficient since it solves an optimization problem with a much smaller size. Two other major advantages are that it can cope with categorical attributes in a unified fashion and naturally handle multi-class problems. Moveover, our approach inherits from logistic regression good interpretability of the model, which is important for clinical applications but not offered by KLR and SVM. Extensive results on real datasets, including a clinical prediction application currently under deployment in a major hospital, show that our approach not only achieves superior classification accuracy, but also drastically reduces the computing time as compared to other leading methods.</Data></Cell><Cell><Data ss:Type="String">Wenlin Chen, ; Yixin Chen*, Washington University in St Louis; Yi Mao, </Data></Cell><Cell><Data ss:Type="String">Supervised learning\Classification*; Applications\Healthcare and medicine; Supervised learning\Regression</Data></Cell><Cell><Data ss:Type="String">classification, logistic regression, kernel density estimation</Data></Cell></Row><Row ss:Height="46.26"><Cell><Data ss:Type="String">120</Data></Cell><Cell><Data ss:Type="String">Extracting Social Events for Learning Better Information Diffusion Models</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Learning of the information diffusion model is a fundamental problem in the study of information diffusion in social networks. Existing approaches learn the diffusion models from events in social networks. However, events in social networks may have different underlying reasons. Some of them may be caused by the social influence inside the network, while others may reflect external trends in the "real world". Most existing work on the learning of diffusion models does not distinguish the events caused by the social influence from those caused by external trends.
   
In this paper, we extract social events from the data stream in social networks, and then use the extracted social events to improve the learning of information diffusion models. We propose a LADP (Latent Action Diffusion Path) model to incorporate the information diffusion model with the model of external trends, and then design an EM-based algorithm to infer the diffusion probabilities, the external trends and the sources of events efficiently
</Data></Cell><Cell><Data ss:Type="String">Shuyang Lin*, UIC; Fengjiao Wang, University of Illinois at Chic; Qingbo Hu, University of Illinois at Chic; philip Yu, University of Illinois at Chicago</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Economy, markets\Viral marketing; Graph mining</Data></Cell><Cell><Data ss:Type="String">Social Event, Information Diffusion, Social Influence</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">131</Data></Cell><Cell><Data ss:Type="String">Mining Lines in the Sand: On Trajectory Discovery From Untrustworthy Data in Cyber-Physical System</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">A Cyber-Physical System (CPS) integrates physical (i.e., sensor) devices with cyber (i.e., informational) components to form a context sensitive system that responds intelligently to dynamic changes in real-world situations. The CPS has wide applications in scenarios such as environment monitoring, battlefield surveillance and traffic control. One key research problem of CPS is called “mining lines in the sand”. With a large number of sensors (sand) deployed in a designated area, the CPS is required to discover all the trajectories (lines) of passing intruders in real time. There are two crucial challenges that need to be addressed: (1) the collected sensor data are not trustworthy; (2) the intruders do not send out any identification information, the system needs to distinguish multiple intruders and track their movements. In this study, we propose a method called LiSM (Line-in-the-Sand Miner) to discover the trajectories from untrustworthy sensor data. LiSM constructs a watching network from sensor data and computes the locations of intruder appearances based on the link information of the network. The system retrieves a cone-model from the historical trajectories and tracks multiple intruders based on this model. Finally the system validates the mining results and updates the sensor's reliability in a feedback process. Extensive experiments on both real and synthetic datasets demonstrate the feasibility and applicability of the proposed methods.</Data></Cell><Cell><Data ss:Type="String">Lu-An Tang*, UIUC; Xiao Yu, University of Illinois at Urbana-Champaign; Quanquan Gu, CS, UIUC; Jiawei Han, UIUC; Alice Leung, BBN; Thomas La Porta, PSU</Data></Cell><Cell><Data ss:Type="String">Mining rich data types*; Applications; Data streams; Mining rich data types\Spatial</Data></Cell><Cell><Data ss:Type="String">cyber physical system, sensor data mining, trajectory discovery</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">132</Data></Cell><Cell><Data ss:Type="String">An Efficient ADMM Algorithm for Multidimensional Anisotropic Total Variation Regularization Problems</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Total variation (TV) regularization has important applications in signal processing including image denoising, image deblurring, and image reconstruction. A significant challenge in the practical use of TV regularization lies in the nondifferentiable convex optimization, which is difficult to solve especially for large-scale problems. In this paper, we propose an efficient alternating augmented Lagrangian method (ADMM) to solve total variation regularization problems. The proposed algorithm is applicable for tensors, thus it can solve multidimensional total variation regularization problems. One appealing feature of the proposed algorithm is that it does not need to solve a linear system of equations, which is often the most computationally expensive part in previous ADMM-based methods. In addition, each step of the proposed algorithm involves a set of independent and smaller problems, which can be solved in parallel. Thus, the proposed algorithm scales to large size problems. Furthermore, the global convergence of the proposed algorithm is guaranteed, and the time complexity of the proposed algorithm is $O(dN/\epsilon)$ on a $d$-mode tensor with $N$ entries for achieving an $\epsilon$-optimal solution. Extensive experimental results demonstrate the superior performance of the proposed algorithm in comparison with current state-of-the-art methods.</Data></Cell><Cell><Data ss:Type="String">Sen Yang*, Arizona State University; Jie Wang, Arizona State University; Wei Fan, IBM Research; Xiatian Zhang, Huawei Technologies Co., Ltd; Peter Wonka, Arizona State University; Jieping Ye, Arizona State University</Data></Cell><Cell><Data ss:Type="String">Mining rich data types*; Big Data\Scalable methods</Data></Cell><Cell><Data ss:Type="String">Multidimensional total variation, ADMM, parallel computing, large scale</Data></Cell></Row><Row ss:Height="202.9608"><Cell><Data ss:Type="String">137</Data></Cell><Cell><Data ss:Type="String">Speeding up Large-Scale Learning with a Social Prior</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Slow convergence and poor initial accuracy are two problems that
plague efforts to use very large feature sets in learning. This is
especially true when only a few features are ``active'' in any
training example, and the frequency of activations of different
features is skewed. We show how these problems can be mitigated
if a graph of relationships between features is known. We study this
problem in a fully Bayesian setting, focusing on the problem of using
Facebook user-IDs as features, with the social network giving the
relationship structure. Our analysis uncovers significant problems
with the obvious regularizations, and motivates a two-component
mixture-model ``social prior'' that is provably better. Empirical
results on large-scale click prediction problems show that our
algorithm can learn as well as the baseline with $12M$ fewer training
examples, and continuously outperforms it for over $60M$ examples.
On a second problem using binned features, our model outperforms the
baseline even after the latter sees 5x as many training
examples.
</Data></Cell><Cell><Data ss:Type="String">Deepayan Chakrabarti*, Facebook; Ralf Herbrich, </Data></Cell><Cell><Data ss:Type="String">Social*; Big Data\Novel statistical techniques for big data; Big Data\Scalable methods; Economy, markets\Online advertising; Graph mining</Data></Cell><Cell><Data ss:Type="String">social prior</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">142</Data></Cell><Cell><Data ss:Type="String">A Probabilistic Framework for Big Data Pipelines</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Big Data Pipelines decompose complex analyses of large data sets into a series of simpler tasks, with independently tuned components for each task. This modular setup allows re-use of components across several different pipelines. However, the interaction of independently tuned pipeline components yields poor end-to-end performance as errors introduced by one component cascade through the whole pipeline, affecting overall accuracy. 

We provide a novel model for reasoning across components in Big Data Pipelines in a probabilistically well-founded manner. Our key idea is to view the interaction of components as dependencies on an underlying graphical model.  Different message passing algorithms on this graphical model provide various ways to trade-off end-to-end performance and computational cost for inference.  We instantiate our framework with an efficient beam search algorithm, and demonstrate its efficiency on two Big Data Pipelines: parsing and relation extraction.</Data></Cell><Cell><Data ss:Type="String">Karthik Raman*, Cornell University; Adith Swaminathan, Cornell University; Thorsten Joachims, Cornell; Johannes Gehrke, Cornell University</Data></Cell><Cell><Data ss:Type="String">Big Data*; Big Data\Novel statistical techniques for big data; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">Big Data Pipelines, Modular Design, Probabilistic Inference</Data></Cell></Row><Row ss:Index="17" ss:Height="12.6432"><Cell><Data ss:Type="String">146</Data></Cell><Cell><Data ss:Type="String">FISM: Factored Item Similarity Models for Top-N Recommender Systems</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The effectiveness of existing top-N recommendation methods decreases as the sparsity of the datasets increases. To alleviate this problem, we present an item-based method for generating top-N recommendations that learns the item-item similarity matrix as the product of two low dimensional latent factor matrices. These matrices are learned using a structural equation modeling approach, wherein the value being estimated is not used for its own estimation. A comprehensive set of experiments on multiple datasets at three different sparsity levels indicate that the proposed method can handle sparse datasets effectively and outperforms other state-of-the-art \topn recommendation methods. The experimental results also show that the relative performance gains compared to competing methods increase as the data gets sparser.</Data></Cell><Cell><Data ss:Type="String">Santosh Kabbur*, University of Minnesota; Xia Ning, NEC Laboratories America; George Karypis, University of Minnesota</Data></Cell><Cell><Data ss:Type="String">Recommender systems*; Recommender systems\Collaborative filtering</Data></Cell><Cell><Data ss:Type="String">recommender systems, top-n, item model, sparse data</Data></Cell></Row><Row ss:Height="23.8392"><Cell><Data ss:Type="String">165</Data></Cell><Cell><Data ss:Type="String">Nonparametric Hierarchal Bayesian Modeling in Non-contractual Heterogeneous Survival Data</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">An important problem in the non-contractual marketing domain is discovering the customer lifetime and assessing the impact of customer's characteristic variables on the lifetime. Unfortunately, the conventional hierarchical Bayes model cannot discern the impact of customer's characteristic variables for each customer. To overcome this problem, we present a new survival model using a non-parametric Bayes paradigm with MCMC. The assumption of a conventional model, logarithm of purchase rate and dropout rate with linear regression, is extended to include our assumption of the Dirichlet Process Mixture of regression. The extension assumes that each customer belongs probabilistically to different mixtures of regression, thereby permitting us to estimate a different impact of customer characteristic variables for each customer. Our model creates several customer groups to mirror the structure of the target data set.
The effectiveness of our proposal is confirmed by a comparison involving a real e-commerce transaction dataset and an artificial dataset; it generally achieves higher predictive performance. In addition, we show that preselecting the actual number of customer groups does not always lead to higher predictive performance.</Data></Cell><Cell><Data ss:Type="String">Shouichi Nagano*, NTT; Yusuke Ichikawa, NTT; Noriko Takaya, NTT; Tadasu Uchiyama, NTT; Makoto Abe, Tokyo University</Data></Cell><Cell><Data ss:Type="String">User modeling*; Applications\E-commerce; Information extraction; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">CRM, Model Choice, Non-parametric Bayes, MCMC</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">166</Data></Cell><Cell><Data ss:Type="String">Fast and Scalable Polynomial Kernels via Explicit Feature Maps</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Approximation of non-linear kernels using random feature mapping has been successfully employed in large-scale data analysis applications, accelerating the training of kernel machines. While previous random feature mappings run in $O(ndD)$ time for $n$ training samples in $d$-dimensional space and $D$ random feature maps, we propose a novel randomized tensor product technique, called \textit{Tensor Sketching}, for approximating any polynomial kernel in $O(n(d+D \log{D}))$ time. Also, we introduce both \textit{absolute} and \textit{relative} error bounds for our approximation to guarantee the reliability of our estimation algorithm. Empirically, Tensor Sketching achieves higher accuracy and often runs orders of magnitude faster than the state-of-the-art approach for large-scale real-world datasets.</Data></Cell><Cell><Data ss:Type="String">Ninh Pham*, IT University of Copenhagen; Rasmus Pagh, IT University of Copenhagen</Data></Cell><Cell><Data ss:Type="String">Supervised learning\Support vector machines*; Big Data\Scalable methods; Dimensionality reduction; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">Polynomial Kernel, SVM, Tensor Product, Count Sketch, FFT</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">172</Data></Cell><Cell><Data ss:Type="String">SiGMa: Simple Greedy Matching for Aligning Large Knowledge Bases</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The Internet has enabled the creation of a growing number of large-scale knowledge bases in a variety of domains containing complementary information. Tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of large-scale knowledge bases still poses a considerable challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algorithm for aligning knowledge bases with millions of entities and facts. SiGMa is an iterative propagation algorithm which leverages both the structural information from the relationship graph as well as flexible similarity measures between entity properties in a greedy local search, thus making it scalable. Despite its greedy nature, our experiments indicate that SiGMa can efficiently match some of the world's largest knowledge bases with high accuracy. We provide additional experiments on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art approaches both in accuracy and efficiency. </Data></Cell><Cell><Data ss:Type="String">Simon Lacoste-Julien*, INRIA / ENS; Konstantina Palla, University of Cambridge; Alex Davies, University of Cambridge; Gjergji Kasneci, Microsoft Research; Thore Graepel, Microsoft Research; Zoubin Ghahramani, Cambridge University</Data></Cell><Cell><Data ss:Type="String">Web mining*; Big Data\Scalable methods</Data></Cell><Cell><Data ss:Type="String">knowledge base, alignment, large-scale, entity, relationship, greedy algorithm</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">179</Data></Cell><Cell><Data ss:Type="String">Cross-Task Crowdsourcing</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Crowdsourcing is an effective method to collect labeled data for various data mining tasks. It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable. Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in individual task. In this case, every single task all needs amounts of data to provide accurate estimation. However, in practice, budget provided by customers for a given target task may be limited, and hence each question can be presented to only a few users, and each user can answer only a few questions. This data sparsity problem can make previous approaches perform poorly due to the overfitting problem on rare data and eventually damaging the data veracity. In this paper, we employ transfer learning, which borrows knowledge from auxiliary history tasks to improve the data veracity in a given target task. The motivation is that users have stable characteristics across different crowdsourcing tasks, and thus data from different tasks can be exploited collectively to estimate users' abilities in the target task. We propose a hierarchical Bayesian model, TLC(Transfer Learning for Crowdsourcing), to implement this idea by considering the overlapping users as a bridge. In addition, to avoid possible negative impacts, TLC introduces task-specific factors to model task differences. The experimental results show that TLC improves the accuracy over several state-of-the-art non-transfer learning approaches significantly under very limited budget in given target domains.</Data></Cell><Cell><Data ss:Type="String">Kaixiang Mo*, HKUST; Erheng Zhong, HKUST; Qiang Yang, Hong Kong University of Science and Technology</Data></Cell><Cell><Data ss:Type="String">Transfer learning*; Other; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">Crowdsourcing</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">191</Data></Cell><Cell><Data ss:Type="String">Multi-Source Learning with Block-wise Missing Data For Alzheimer's Disease Prediction</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">With the advances and increasing sophistication in data collection techniques, we are facing with large amounts of data collected from multiple heterogeneous sources in many applications. For example, in the study of Alzheimer's Disease (AD), different types of measurements such as neuroimages, gene/protein expression data, genetic data etc. are often collected and analyzed together for improved predictive power. It is believed that a joint learning of multiple data sources is beneficial as different data sources may contain complementary information, and feature-pruning and data source selection are critical for learning interpretable models from high-dimensional data. Very often the collected data comes with block-wise missing entries; for example, a patient without the MRI scan will have no information in the MRI data block, making his/her overall record incomplete. There has been a growing interest in the data mining community on expanding traditional techniques for single-source complete data analysis to the study of multi-source incomplete data. The key challenge is how to effectively integrate information from multiple heterogeneous sources in the presence of block-wise missing data. In this paper we first investigate the situation of complete data and present a unified ``bi-level" learning model for multi-source data. Then we give a natural extension of this model to the more challenging case with incomplete data. Our major contributions are threefold: (1) the proposed models handle both feature-level and source-level analysis in a unified formulation and include several existing feature learning approaches as special cases; (2) the model for incomplete data avoids direct imputation of the missing elements and thus provides superior performances. Moreover, it can be easily generalized to other applications with block-wise missing data sources; (3) efficient optimization algorithms are presented for both of the complete and incomplete model. </Data></Cell><Cell><Data ss:Type="String">Shuo Xiang*, Arizona State University; Lei Yuan, Arizona State University; Wei Fan, IBM Research; Yalin Wang, ; Paul Thompson, ; Jieping Ye, Arizona State University</Data></Cell><Cell><Data ss:Type="String">Applications\Healthcare and medicine*; Bioinformatics; Supervised learning\Classification</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="102.1896"><Cell><Data ss:Type="String">194</Data></Cell><Cell><Data ss:Type="String">Evaluating the Crowd with Confidence</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Worker quality control is a crucial aspect of crowdsourcing systems;
typically occupying a large fraction of the time and money
invested on crowdsourcing. In this work, we devise techniques
to generate confidence intervals for worker error rate estimates,
thereby enabling a better evaluation of worker quality. We show
that our techniques generate correct confidence intervals on a range
of real-world datasets, and demonstrate wide applicability by using
them to evict poorly performing workers, and provide confidence
intervals on the accuracy of the answers.</Data></Cell><Cell><Data ss:Type="String">Manas Joglekar*, Stanford University; Hector Garcia-Molina, Stanford University; Aditya Parameswaran, Stanford University</Data></Cell><Cell><Data ss:Type="String">Other*; Design of experiments and sample survey; Foundations; Probabilistic methods; Security and privacy\Spam detection; Unsupervised learning</Data></Cell><Cell><Data ss:Type="String">confidence, crowdsourcing, human computation, guarantees, filtering, intervals</Data></Cell></Row><Row ss:Height="57.456"><Cell><Data ss:Type="String">198</Data></Cell><Cell><Data ss:Type="String">Representing Documents Through Their Readers</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">From Twitter to Facebook to Reddit, users have become accustomed to sharing the articles they read with friends or followers on their social networks. While previous work has modeled what these shared stories say about the user who shares them, the converse question remains unexplored: what can we learn about an article from the identities of its likely readers? 

To address this question, we model the content of news articles and blog posts by attributes of the people who are likely to share them. For example, many Twitter users describe themselves in a short profile, labeling themselves with phrases such as "vegetarian" or "liberal." By assuming that a user's labels correspond to topics in the articles he shares, we can learn a labeled dictionary from a training corpus of articles shared on Twitter. Thereafter, we can code any new document as a sparse non-negative linear combination of user labels, where we encourage correlated labels to appear together in the output via a structured sparsity penalty. 

Finally, we show that our approach yields a novel document representation that can be effectively used in many problem settings, from recommendation to modeling news dynamics. For example, while the top politics stories will change drastically from one month to the next, the "politics" label will still be there to describe them. We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012, demonstrating that our approach is effective.</Data></Cell><Cell><Data ss:Type="String">Khalid El-Arini*, Carnegie Mellon University; Min Xu, Carnegie Mellon University; Emily Fox, University of Washington; Carlos Guestrin, University of Washington</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Text*; Recommender systems\Content based methods; Social\Social media; Unsupervised learning\Topic, graphical and latent variable models; Web mining</Data></Cell><Cell><Data ss:Type="String">document modeling, Twitter, structured sparsity, personalization</Data></Cell></Row><Row ss:Index="25" ss:Height="12.6432"><Cell><Data ss:Type="String">200</Data></Cell><Cell><Data ss:Type="String">Inferring Social Roles and Statuses in Social Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Users in online social networks play a variety of social roles and statuses. For example, users in Twitter can be represented as advertiser, content contributor, information receiver, etc; users in Linkedin can be in different professional roles, such as engineer, salesperson and recruiter. Previous research work mainly focuses on using  categorical and textual information to predict the attributes of users. However, it cannot be applied to a large number of users in  real social networks, since much of such information is missing, outdated and non-standard. In this paper, we investigate  the social roles and statuses that people act in online social networks in the perspective of network structures, since the uniqueness of social networks is connecting people. We quantitatively analyze a number of key social principles and theories that correlate with social roles and statuses. We systematically study how the network characteristics reflect the social situations of users in an online society. We discover patterns of homophily, the tendency of users to connect with users with similar social roles and statuses.  In addition, we observe that different factors in social theories influence the social role/status of individual user to various extent, since these social principles represent different aspects of the network. We then introduce an optimization framework based on Factor Conditioning Symmetry, and we propose a probabilistic model to integrate the optimization framework on local structural information as well as network influence to infer the unknown social roles and statuses of online users. We will present experiment results to show the effectiveness of the inference.</Data></Cell><Cell><Data ss:Type="String">Yuchen Zhao*, UIC; Guan Wang, Univ. of Illinois at Chicago; Philip Yu, University of Illinois; Shaobo Liu, Linkedin; Simon Zhang, Linkedin</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*</Data></Cell><Cell><Data ss:Type="String">social networks; social role; status;</Data></Cell></Row><Row ss:Height="23.8392"><Cell><Data ss:Type="String">211</Data></Cell><Cell><Data ss:Type="String">Adaptive Collective Routing Using Gaussian Process Dynamic Congestion Models</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">We consider the problem of adaptively routing a fleet of cooperative vehicles within a road network in the presence of uncertain and dynamic congestion conditions.  To tackle this problem, we first propose a Gaussian Process Dynamic Congestion Model that can effectively characterize both the dynamics and the uncertainty of congestion conditions.  Our model is efficient and thus facilitates real-time adaptive routing in the face of uncertainty.  Leveraging this congestion model, we develop an efficient algorithm for non-myopic adaptive routing to minimize the collective travel time of all vehicles in the system.  A key property of our approach is the ability to efficiently reason about the long-term value of exploration, which enables collectively balancing the exploration/exploitation trade-off for entire fleets of vehicles.  We validate our approach based on traffic data from two large Asian cities.  We show that our congestion model is  effective in modeling dynamic congestion conditions.  We further  show that our routing algorithm  generates significantly faster routes compared to standard baselines, and achieves near-optimal performance compared to an omniscient routing algorithm. We also present the results from a preliminary field study, which showcases the efficacy of our approach.
</Data></Cell><Cell><Data ss:Type="String">Siyuan Liu*, CMU; Yisong Yue, Carnegie Mellon University; Ramayya Krishnan, Carnegie Mellon University</Data></Cell><Cell><Data ss:Type="String">Adaptive learning\Adaptive models*; Applications\Mobile; Mining rich data types\Spatial; Mining rich data types\Temporal / time series; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">Collective routing, Gaussian Process, Dynamic congestion model</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">235</Data></Cell><Cell><Data ss:Type="String">Maximizing Acceptance Probability for Active Friending in Online Social Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Friending recommendation has successfully contributed to the explosive growth of on-line social networks. Most friending recommendation services today aim to support passive friending, where a user passively selects friending targets from the recommended candidates. In this paper, we advocate recommendation support for active friending, where a user actively specifies a friending target. To the best of our knowledge, a recommendation designed to provide guidance for a user to systematically approach his friending target, has not been explored in existing on-line social networking services. To maximize the probability that the friending target would accept an invitation from the user, we formulate a new optimization problem, namely, Acceptance Probability Maximization (APM), and develop a polynomial time algorithm, called Selective Invitation with Tree and In-Node Aggregation (SITINA), to find the optimal solution. We implement an active friending service with SITINA in Facebook to validate our idea. Our user study and experimental results manifest that SITINA outperforms manual selection and the baseline approach in solution quality efficiently.</Data></Cell><Cell><Data ss:Type="String">De-Nian Yang*, Academia Sinica; Hui-Ju Hung, Academia Sinica; Wang-Chien Lee, ; Wei Chen, Microsoft Research</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Applications; Applications\E-commerce; Social</Data></Cell><Cell><Data ss:Type="String">Friending, Social Network, Social Influence</Data></Cell></Row><Row ss:Height="169.4016"><Cell><Data ss:Type="String">240</Data></Cell><Cell><Data ss:Type="String">Statistical Quality Estimation for General Crowdsourcing Tasks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control,
which is to expect high-quality results from crowd workers who are neither necessarily very capable nor motivated.
A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks.
For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been proposed.
However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing, 
which occupy the majority on most crowdsourcing marketplaces.
In this paper, we propose a statistical quality estimation method for such general crowdsourcing tasks.
Our method is based on the two-stage procedure; 
multiple workers are first requested to work on the same tasks in the creation stage,
and then another set of workers review and grade each artifact in the review stage.
We model the ability of each author and the bias of each reviewer,
and propose a two-stage probabilistic generative model using the graded response model in the item response theory.
Experiments using several general crowdsourcing tasks show that our method outperforms popular vote aggregation methods,
especially when the number of reviewers for each work is small,
which implies that our method can deliver high quality results with lower costs.</Data></Cell><Cell><Data ss:Type="String">Yukino Baba*, The University of Tokyo; Hisashi Kashima, The University of Tokyo</Data></Cell><Cell><Data ss:Type="String">Web mining*; Applications; User modeling</Data></Cell><Cell><Data ss:Type="String">crowdsourcing, human computation, quality estimation</Data></Cell></Row><Row ss:Height="23.8392"><Cell><Data ss:Type="String">245</Data></Cell><Cell><Data ss:Type="String">Mining Frequent Graph Patterns with Differential Privacy</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Discovering frequent graph patterns in a graph database offers valuable information in a variety of applications. However, if the graph dataset contains sensitive data of individuals such as mobile phone-call graphs and web-click graphs, releasing discovered frequent patterns may present a threat to the privacy of individuals. {\em Differential privacy} has recently emerged as the {\em de facto} standard for private data analysis due to its provable privacy guarantee. In this paper we propose the first differentially private algorithm for mining frequent graph patterns. 
We first show that previous techniques on differentially private discovery of frequent {\em itemsets} cannot apply in mining frequent graph patterns due to the inherent complexity of handling structural information in graphs. We then address this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling based algorithm. Unlike previous work on frequent itemset mining, our techniques do not rely on the output of a non-private mining algorithm. Instead, we observe that both frequent graph pattern mining and the guarantee of differential privacy can be unified into an MCMC sampling framework. In addition, we establish the privacy and utility guarantee of our algorithm and propose an efficient neighboring pattern counting technique as well. Experimental results show that the proposed algorithm is able to output frequent patterns with good precision.</Data></Cell><Cell><Data ss:Type="String">Entong Shen*, North Carolina State Univ; Ting Yu, North Carolina State University</Data></Cell><Cell><Data ss:Type="String">Security and privacy*; Security and privacy\Anonymization</Data></Cell><Cell><Data ss:Type="String">differential privacy, privacy-preserving data mining</Data></Cell></Row><Row ss:Height="124.6104"><Cell><Data ss:Type="String">249</Data></Cell><Cell><Data ss:Type="String">Approximate Graph Mining with Label Costs</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">   Many real-world graphs have complex labels on the nodes and edges.
   Mining only exact patterns yields limited insights, since it may be
   hard to find exact matches. However, in many domains it is relatively
   easy to compute some cost (or distance) between different labels.
   Using this information, it becomes possible to mine a much richer set
   of approximate subgraph patterns, which preserve the topology but allow
   bounded label mismatches. We present novel and scalable methods to
   efficiently solve the approximate isomorphism problem. We show that
   the mined approximate patterns yield interesting patterns in several
   real-world graphs ranging from IT and protein interaction networks to
   protein structures.</Data></Cell><Cell><Data ss:Type="String">Pranay Anchuri*, RPI; Mohammed Zaki, Rensselaer Polytechnic Institute; Omer Barkol, HP Labs; Shahar Golan, HP Labs; Moshe Shamy, HP Labs</Data></Cell><Cell><Data ss:Type="String">Graph mining*; Big Data\Large scale optimization; Mining rich data types; Rule and pattern mining; Sampling</Data></Cell><Cell><Data ss:Type="String">approximate graph mining, approximate matching; single graph mining</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">250</Data></Cell><Cell><Data ss:Type="String">Mining Evolutionary Multi-Branch Trees from Text Streams</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Understanding topic hierarchies in text streams and their evolution patterns over time is very important in many applications. In this paper, we propose the method of evolutionary multi-branch tree clustering for streaming text data. We build evolutionary trees in a Bayesian online filtering framework. The tree construction is formulated as an online posterior estimation problem, which considers both the likelihood of the current tree and conditional prior given the previous tree. We also introduce a constraint model to compute the conditional prior of a tree hierarchy in the multi-branch setting. Experiments on real world news data demonstrate that our algorithm can better incorporate historical tree information and is more efficient and effective than the traditional evolutionary hierarchical clustering algorithm.</Data></Cell><Cell><Data ss:Type="String">Xiting Wang, Tsinghua University; Yangqiu Song, HKUST; Shixia Liu*, Microsoft Research; Baining Guo, Microsoft Research Asai</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Text*; Unsupervised learning\Clustering</Data></Cell><Cell><Data ss:Type="String">Bayesian rose tree, evolutionary clustering, hierarchical clustering, constraint model</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">251</Data></Cell><Cell><Data ss:Type="String">Robust Principal Component Analysis via Capped Norms</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">In many applications such as image and video processing, the data matrix often possesses a low-rank structure capturing the global information and a sparse component capturing the local information simultaneously. How to accurately extract the low-rank and sparse components is a major challenge. Robust Principal Component Analysis (RPCA) is a general framework to extract such structures. It is well studied that under certain assumptions, convex optimization using trace norm and $\ell_1$-norm can be an effective computation surrogate of the difficult RPCA problem. However, it is based on a strong assumption which may not hold in real-world applications, and the approximation error in these convex relaxations often cannot be neglected. In this paper, we present a novel non-convex formulation for the RPCA problem using the capped trace norm and the capped $\ell_1$-norm. In addition, we present two algorithms to solve the non-convex optimization: one is based on the Difference of Convex functions (DC) framework and the other attempts to solve the sub-problems via a greedy approach. Compared to existing convex formulations, our approach gives proper interpretation and both of the proposed algorithms achieve better accuracy for the RPCA problem which can be verified by empirical results. Furthermore, between the two proposed algorithms, the greedy algorithm is more efficient than the DC programming, while they achieve comparable accuracy.</Data></Cell><Cell><Data ss:Type="String">Qian Sun*, Arizona State University; Shuo Xiang, Arizona State University; Jieping Ye, Arizona State University</Data></Cell><Cell><Data ss:Type="String">Unsupervised learning*; Mining rich data types; Rule and pattern mining</Data></Cell><Cell><Data ss:Type="String">Robust PCA, DC programming, ADMM, low-rank, sparsity, trace norm, image processing</Data></Cell></Row><Row ss:Index="33" ss:Height="35.0352"><Cell><Data ss:Type="String">261</Data></Cell><Cell><Data ss:Type="String">Active Search on Graphs</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Active search is an increasingly important learning problem in which we use a limited budget of label queries to discover as many members of a certain class as possible.  Numerous real-world applications may be approached in this manner, including fraud detection, product recommendation, and drug discovery.  Active search has model learning and exploration/exploitation features similar to those encountered in active learning and bandit problems, but algorithms for those problems do not fit active search.

Previous work on the active search problem \cite{garnett12} showed that the optimal algorithm requires a lookahead evaluation of expected utility that is exponential in the number of selections to be made and proposed a truncated lookahead heuristic.  Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs.  We suggest selecting points by maximizing a score considering the potential impact of selecting a node, meant to emulate lookahead while avoiding exponential search.  We test the proposed algorithm empirically on real-world graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps.</Data></Cell><Cell><Data ss:Type="String">Xuezhi Wang*, Carnegie Mellon University; Jeff Schneider, ; Roman Garnett, </Data></Cell><Cell><Data ss:Type="String">Graph mining*; Adaptive learning\Active learning; Semi-supervised learning\Learning with partial labels; Social\Social and information networks</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">269</Data></Cell><Cell><Data ss:Type="String">Fast Rank-2 Nonnegative Matrix Factorization for Hierarchical Document Clustering</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Nonnegative matrix factorization (NMF) has been successfully used as a clustering method especially for flat partitioning of documents. In this paper, we propose an efficient hierarchical document clustering method based on a new algorithm for rank-2 NMF. When the two block coordinate descent framework is applied to rank-2 NMF, each subproblem requires a solution for nonnegative least squares (NNLS) with only two columns. We design the algorithm for rank-2 NMF by exploiting the fact that an exhaustive search for the optimal active set can be performed extremely fast when solving these NNLS problems. In addition, we design a measure on the results of rank-2 NMF for determining which leaf node should be further split. On a number of text data sets, our proposed method produces high-quality tree structures in significantly less time compared to other methods such as hierarchical K-means, standard NMF, and latent Dirichlet allocation.</Data></Cell><Cell><Data ss:Type="String">Da Kuang*, Georgia Institute of Technology; Haesun Park, Georgia Institute of Technology</Data></Cell><Cell><Data ss:Type="String">Unsupervised learning*; Unsupervised learning\Clustering; Unsupervised learning\Matrix/tensor factorization</Data></Cell><Cell><Data ss:Type="String">Active-set algorithm, hierarchical document clustering, nonnegative matrix factorization, rank-2 NMF</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">291</Data></Cell><Cell><Data ss:Type="String">The Role of Information Diffusion in the Evolution of Social Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Every day millions of users are connected through online social networks, generating a rich trove of data that allows us to study the mechanisms behind human interactions. Triadic closure has been treated as the major mechanism for creating social links: if Alice follows Bob and Bob follows Charlie, Alice will follow Charlie. Here we present an analysis of longitudinal micro-blogging data, revealing a more nuanced view of the strategies employed by users when expanding their social circles. While the network structure affects the spread of information among users, the network is in turn shaped by this communication activity. This suggests a link creation mechanism whereby Alice is more likely to follow Charlie after seeing many messages by Charlie. We characterize users with a set of parameters associated with different link creation strategies, estimated by a Maximum-Likelihood approach. Triadic closure does have a strong effect on link formation, but shortcuts based on traffic are another key factor in interpreting network evolution. However, individual strategies for following other users are highly heterogeneous. Link creation behaviors can be summarized by classifying users in different categories with distinct structural and behavioral characteristics. Users who are popular, active, and influential tend to create traffic-based shortcuts, making the information diffusion process more efficient in the network. </Data></Cell><Cell><Data ss:Type="String">Lilian Weng*, Indiana University; Jacob Ratkiewicz, Google Inc.; Nicola Perra, Northeastern University; Bruno Goncalves, Aix-Marseille Universite; Carlos Castillo, Qatar Computing Research Institute; Francesco Bonchi, Yahoo! Research; Rossano Schifanella, Universita degli Studi di Torino, Italy; Filippo Menczer, Indiana University; Alessandro Flammini, Indiana University</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Social\Social media; Unsupervised learning\Clustering; User modeling</Data></Cell><Cell><Data ss:Type="String">Link creation, Traffic, Network evolution, Information diffusion, Shortcut, User behavior, Social media, Network structure</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">293</Data></Cell><Cell><Data ss:Type="String">LCARS: A Location-Content-Aware Recommender System</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Newly emerging location-based and event-based social network services provide us with a new platform to understand users' preferences based on their activity history.  A user can only visit a limited number of venues/events and  most of them are within a limited distance range, so the user-item matrix is very sparse, which creates a big challenge for traditional collaborative  filtering-based recommender systems. The problem becomes even more challenging when people travel to a new city where they have no activity history.

 In this paper, we propose LCARS, a location-content-aware recommender system that offers a particular user a set of venues (e.g., restaurants and shopping malls) or  events (e.g., concerts and exhibitions)  by giving consideration to both personal interest and  local preference. This recommender system can facilitate people's travel not only near the area in which they live, but also in a city that is new to them. Specifically, LCARS consists of two components: offline modeling and online recommendation. The offline modeling part, called LCA-LDA,  is designed to learn the interest of each individual user and the local preference of each individual city by capturing  item co-occurrence patterns and exploiting  item contents. The online recommendation part automatically combines the learnt interest of the querying user and the local preference of the querying city to produce the top-k recommendations. To speed up this online process, a scalable query processing technique is developed by extending the classic Threshold Algorithm (TA). We evaluate the performance of our recommender system on two large-scale real data sets, DoubanEvent and Foursquare. The results show the superiority of LCARS in recommending spatial items for users, especially when traveling to new cities, in terms of both  effectiveness and efficiency.</Data></Cell><Cell><Data ss:Type="String">Hongzhi Yin*, Peking University; Yizhou Sun, ; Bin Cui, Peking University; Zhiting Hu, ; Ling Chen, </Data></Cell><Cell><Data ss:Type="String">Recommender systems*; Recommender systems\Cold-start; Recommender systems\Collaborative filtering; Recommender systems\Content based methods; Unsupervised learning\Topic, graphical and latent variable models</Data></Cell><Cell><Data ss:Type="String">recommender systems, probabilistic generative model, event-based social networks, location-based services, user preferences</Data></Cell></Row><Row ss:Height="57.456"><Cell><Data ss:Type="String">294</Data></Cell><Cell><Data ss:Type="String">Probabilistic Path Prediction in Dynamic Environments</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Path prediction is very useful in a wide range of applications. Most of existing solutions, however, are based on eager learning methods where models and patterns are extracted from historical trajectories and used for future prediction. Since such approaches are already committed to a statistically significant sets of models or patterns, problems can arise in dynamic environments where the underlying models change quickly or in regions which are not covered by statistically significant models or patterns.

We propose a “semi-lazy” approach to path prediction. This approach has several advantages. First, the target trajectory to be predicted is first known before the model/pattern is derived, compared to finding models/patterns in a pre-processing stage which might not be relevant for target trajectories provided later. Second, we can use slightly more sophisticated learning algorithms to derive more accurate local models/patterns without unacceptable delay. Finally, we can dynamically construct new models/patterns if the actual and predicted movements do not match, giving rise to self-correcting continuous prediction.

Instead of trying to predict the most accurate location, our model gives a probabilistically predicted path, whose probability is larger than a threshold and whose time interval is the longest. Users can control the confidence of the predicted path by setting a probability threshold. We conducted a comprehensive experimental study on two real-world datasets and four semi-real datasets to show the effectiveness and efficiency of our model. Experimental results show that our model significantly outperforms competitors by 2 to 5-fold.</Data></Cell><Cell><Data ss:Type="String">Jingbo Zhou*, NUS; Anthony TUNG , SoC,NUS; Wei Wu, I2R, Astar; Wee Siong Ng, I2R, Astar</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Spatial*; Applications\Mobile; Mining rich data types\Temporal / time series; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">trajectories analysis, location based service</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">295</Data></Cell><Cell><Data ss:Type="String">Multi-Label Relational Neighbor Classification using Social Context Features</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Networked data, extracted from social media, web pages, and bibliographic databases, can contain entities of multiple classes, interconnected through different types of links. In this paper, we focus on the problem of performing multi-label classification on networked data, where the instances in the network can be assigned multiple labels. In contrast to traditional content-only classification methods, relational learning succeeds in improving classification performance by leveraging the correlation of the labels between linked instances. However, instances in a network can be linked for various causal reasons, hence treating all links in a homogeneous way can limit the performance of relational classifiers.

In this paper, we propose a multi-label iterative relational neighbor classifier that employs social context features (SCRN). Our classifier incorporates a class propagation probability distribution obtained from instances’ social features, which are in turn extracted from the network topology. This class-propagation probability captures the node’s intrinsic likelihood of belonging to each class, and serves as a prior weight for each class when aggregating the neighbors’ class labels in the collective inference procedure. Experiments on different real-world datasets demonstrate that our proposed classifier can boost classification performance over several commonly used benchmarks on networked multi-label data.</Data></Cell><Cell><Data ss:Type="String">Xi Wang, University of Central Florida; Gita Sukthankar*, University of Central Florida</Data></Cell><Cell><Data ss:Type="String">Supervised learning\Classification*; Social\Community detection; Social\Social and information networks; Supervised learning; Supervised learning\Multi-label</Data></Cell><Cell><Data ss:Type="String">collective classification, relational neighbor classifiers, multi-label learning, edge clustering</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">301</Data></Cell><Cell><Data ss:Type="String">Optimizing Parallel Belief Propagation in Junction Trees: Using Regression</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The junction tree approach, with applications in artificial intelligence, computer vision, machine learning, and statistics, is often used for computing posterior distributions in probabilistic graphical models. One of the key challenges associated with junction trees is computational, and several parallel computing technologies - including many-core processors - have been investigated to meet this challenge. Many-core processors (including GPUs) are now programmable, unfortunately their complexities make it hard to manually tune their parameters in order to optimize software performance.  In this paper, we investigate a machine learning approach to minimize the execution time of parallel junction tree algorithms implemented on a GPU. By carefully allocating a GPU’s threads to different parallel computing opportunities in a junction tree, and treating this thread allocation problem as a machine learning problem, we find in experiments that regression - specifically support vector regression - can substantially outperform manual optimization. </Data></Cell><Cell><Data ss:Type="String">Lu Zheng, Carnegie Mellon University; Ole Mengshoel*, Carnegie Mellon University</Data></Cell><Cell><Data ss:Type="String">Probabilistic methods*; Applications; Big Data; Supervised learning\Regression; Supervised learning\Support vector machines</Data></Cell><Cell><Data ss:Type="String">Junction trees; Bayesian networks; Probabilistic graphical models; Parallel processing; GPU</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">324</Data></Cell><Cell><Data ss:Type="String">Multi-Source Deep Learning for Information Trustworthiness Estimation</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">In recent years, information trustworthiness becomes a serious issue when user-generated contents prevail in our information world. In this paper, we investigate the important problem of estimating information trustworthiness from the perspective of correlating and comparing multiple data sources. To a certain extent, the consistency degree is an indicator of information reliability--Information unanimously agreed by all the sources is more likely to be reliable. Based on this principle, we develop an effective computational approach to identify consistent information from multiple data sources. Particularly, we analyze vast amounts of information collected from multiple review platforms (multiple sources) in which people can rate and review the items they have purchased. The major challenge is that different platforms attract diverse sets of users, and thus information cannot be compared directly at the surface. However, latent reasons hidden in user ratings are mostly shared by multiple sources, and thus inconsistency about an item only appears when some source provides ratings deviating from the common latent reasons. Therefore, we propose a novel two-step procedure to calculate information consistency degrees for a set of items which are rated by multiple sets of users on different platforms. We first build a Multi-Source Deep Belief Network (MSDBN) to identify the common reasons hidden in multi-source rating data, and then calculate a consistency score for each item by comparing individual sources with the reconstructed data derived from the latent reasons. We conduct experiments on real user ratings collected from Orbitz, Priceline and TripAdvisor on all the hotels in Las Vegas and New York City. Experimental results demonstrate that the proposed approach successfully finds the hotels that receive inconsistent, and possibly unreliable, ratings.</Data></Cell><Cell><Data ss:Type="String">Liang Ge*, University at Buffalo; Jing Gao, Univ. of Buffalo; Xiaoyi Li, The State University of New York; Aidong Zhang, University at Buffalo</Data></Cell><Cell><Data ss:Type="String">Applications*; Unsupervised learning</Data></Cell><Cell><Data ss:Type="String">Information Trustworthiness, Deep Learning, Multiple Source</Data></Cell></Row><Row ss:Index="41" ss:Height="35.0352"><Cell><Data ss:Type="String">341</Data></Cell><Cell><Data ss:Type="String">Model Selection in Markovian Procsses</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">When analyzing data that originated from a dynamical system, a common practice is to encompass the problem in the well known frameworks of Markov Decision Processes (MDPs) and Reinforcement Learning (RL). The state space in these solutions is usually chosen in some heuristic fashion and the formed MDP can then be used to simulate and predict data, as well as indicate the best possible action in each state. The model chosen to characterize the data affects the complexity and accuracy of any further action we may wish to apply, yet few methods that rely on the dynamic structure to select such a model were suggested.

In this work we address the problem of how to use time series data to choose from a finite set of candidate discrete state spaces, where these spaces are constructed by a domain expert. We formalize the notion of model selection consistency in the proposed setup. We then discuss the difference between our proposed framework and the classical Maximum Likelihood (ML) framework, and give an example where ML fails. Afterwards, we suggest alternative selection criteria and show them to be weakly consistent. We then define weak consistency for a model construction algorithm and show a simple algorithm that is weakly consistent. Finally, we test the performance of the suggested criteria and algorithm on both simulated and real world data.</Data></Cell><Cell><Data ss:Type="String">Assaf Hallak*, The Technion; Dotan Di-Castro, Technion; Shie Mannor, Technion</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Temporal / time series*; Big Data\Novel statistical techniques for big data; Dimensionality reduction; Feature selection; Unsupervised learning\Clustering</Data></Cell><Cell><Data ss:Type="String">Model Selection, Reinforcement Learning, Markov Decision Processes, Dynamic Mailing Policies</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">345</Data></Cell><Cell><Data ss:Type="String">Unsupervised Link Prediction Using Aggregative Statistics on Heterogeneous Social Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The concern of privacy has become an important issue for online social networks. In services such as Foursquare.com, whether a person likes an article is considered private and cannot be disclosed; only the aggregative statistics of articles (i.e., how many people like this article) is revealed. This paper tries to answer a question: can we predict the opinion holder in a heterogeneous social network without any labeled data? This question can be generalized to an unseen-type link prediction with aggregative statistics problem. This paper devises a novel unsupervised framework to solve this problem, including three main components: (1) a three-layer factor graph model and three types of potential functions; (2) a ranked-margin learning algorithm for parameter tuning; and (3) a two-stage inference algorithm for link prediction. Finally, we evaluate our method on four diverse scenarios using four datasets: preference prediction (Foursquare), repost prediction (Twitter), response prediction (Plurk), and citation prediction (DBLP). We further exploit nine unsupervised models to solve this problem as baseline. Our approach not only wins out in all scenarios, but on the average achieves 6.24% AUC and 9.24% NDCG improvement over the best competitors. The source code and datasets are available at http://www.csie.ntu.edu.tw/~d97944007/aggregative/</Data></Cell><Cell><Data ss:Type="String">Tsung-Ting Kuo*, National Taiwan University; Rui Yan, Peking University; Yu-Yang Huang, National Taiwan University; Perng-Hwa Kung, National Taiwan University; Shou-De Lin, National Taiwan University</Data></Cell><Cell><Data ss:Type="String">Social\Link prediction*; Mining rich data types\Unstructured; Sentiment and opinion mining; Unsupervised learning\Topic, graphical and latent variable models; Web mining</Data></Cell><Cell><Data ss:Type="String">Link prediction, Social network mining, Heterogeneous social network, Probabilistic graphical model</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">347</Data></Cell><Cell><Data ss:Type="String">Link Prediction with Social Vector Clocks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">State-of-the-art link prediction utilizes combinations of complex features derived from network panel data.  We here show that computationally less expensive features can achieve the same performance in the common scenario in which the data is available as a sequence of interactions.  Our features are based on social vector clocks, an adaptation of the vector-clock concept introduced in distributed computing to social interaction networks.  In fact, our experiments suggest that by taking into account the order and spacing of interactions, social vector clocks exploit different aspects of link formation so that their combination with previous approaches yields the most accurate predictor to date.</Data></Cell><Cell><Data ss:Type="String">Conrad Lee*, University College Dublin; Bobo Nick, Konstanz Universität; Ulrik Brandes, Konstanz Universität; Pádraig Cunningham, University College Dublin</Data></Cell><Cell><Data ss:Type="String">Social\Link prediction*; Big Data\Scalable methods; Data streams; Social\Social and information networks; Social\Social media</Data></Cell><Cell><Data ss:Type="String">social networks, vector clocks, link prediction, online algorithms</Data></Cell></Row><Row ss:Height="57.456"><Cell><Data ss:Type="String">355</Data></Cell><Cell><Data ss:Type="String">Geo-Spotting: Mining Online Location-based Services for Optimal Retail Store Placement</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The problem of identifying the optimal location to open a new retail store has been the focus of past research, especially in the field of land economy, due to its importance in the success of a business. Traditional approaches to the problem have factored in demographics, revenue and aggregated human flow statistics from nearby or remote areas. However, the acquisition of relevant data is usually expensive. With the growth of location-based social networks, fine grained data describing user mobility and popularity of places has recently become attainable.

In this paper we study the predictive power of various machine learning features on the popularity of retail stores in the city through the use of a dataset collected from Foursquare in New York. The features we mine are based on two general signals: geographic, where features are formulated according to the types and density of nearby places, and user mobility which includes transitions between venues or the incoming flow of mobile users from distant areas. After studying the effectiveness of features individually, we integrate them in a supervised learning framework.

Our evaluation suggests that the best performing features are common across the three different commercial chains considered in the analysis, although variations may exist too, as explained by heterogeneities in the way retail facilities attract users. Features that exploit the semantics information of Foursquare venues, encoding the competitiveness of an area with respect to a given type of business (for example restaurant or coffee shop) together with features modeling incoming transitions from distant areas are doing best. Features that encode information on user transitions across venues in proximity are also effective in this context. Finally, we show that performance improve significantly when combining multiple features in supervised learning algorithms, suggesting that the retail success of a business may depend on multiple of factors.</Data></Cell><Cell><Data ss:Type="String">Dmytro Karamshuk*, University of Cambridge; Anastasios Noulas, University of Cambridge; Salvatore Scellato, University of Cambridge; Vincenzo Nicosia, University of Cambridge; Cecilia Mascolo, University of Cambridge</Data></Cell><Cell><Data ss:Type="String">Applications\Finance*; Economy, markets; Mining rich data types\Spatial; Social\Social and information networks</Data></Cell><Cell><Data ss:Type="String">location-based social networks, optimal retail location, data mining, supervised learning</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">388</Data></Cell><Cell><Data ss:Type="String">Location-Aware Publish/Subscribe</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Location-based services have become widely available on mobile devices. Existing methods employ a pull model or user-initiated model, where a user issues a query to a server which replies with location-aware answers. To provide users with instant replies, a push model or server-initiated model is becoming an inevitable computing model in the next-generation location-based services. In the push model, subscribers register spatio-textual subscriptions to capture their interests, and publishers post spatio-textual messages. This calls for a high-performance location-aware publish/subscribe system to deliver publishers' messages to relevant subscribers.

In this paper, we address the research challenges that arise in designing a location-aware publish/subscribe system. We propose an \rtree based index structure by integrating textual descriptions into \rtree nodes. We devise efficient filtering algorithms and develop effective pruning techniques to improve filtering efficiency. Experimental results show that our method achieves high performance. For  example, our method can filter 500 tweets in a second for 10 million registered subscriptions on a commodity computer.</Data></Cell><Cell><Data ss:Type="String">Guoliang Li*, Tsinghua Univeristy</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Spatial*; Big Data\Scalable methods</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">392</Data></Cell><Cell><Data ss:Type="String">Summarizing Probabilistic Frequent Patterns: A Fast Approach</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Mining probabilistic frequent patterns from uncertain data has received a great deal of attention in recent years due to the wide applications. However, probabilistic frequent pattern mining suffers from the problem that an exponential number of result patterns are generated, which seriously hinders further evaluation and analysis. In this paper, we focus on the problem of mining probabilistic representative frequent patterns (P-RFP), which is the minimal set of patterns with adequately high probability to represent all frequent patterns.  Observing the bottleneck in checking whether a pattern can probabilistically represent another, which involves the computation of a joint probability of the supports of two patterns, we introduce a novel approximation of the joint probability with both theoretical and empirical proofs.  Based on the approximation, we propose an Approximate P-RFP Mining (APM) algorithm, which effectively and efficiently compresses the set of probabilistic frequent patterns.  To our knowledge, this is the first attempt to analyze the relationship between two probabilistic frequent patterns through an approximate approach. Our experiments on both synthetic and real-world datasets demonstrate that the APM algorithm accelerates P-RFP mining dramatically, orders of magnitudes faster than an exact solution. Moreover, the error rate of APM is guaranteed to be very small when the database contains hundreds transactions, which further affirms APM is a practical solution for summarizing probabilistic frequent patterns.</Data></Cell><Cell><Data ss:Type="String">Chunyang Liu*, UTS; Ling Chen, ; Chengqi Zhang, QCIS, University of Technology, Sydney</Data></Cell><Cell><Data ss:Type="String">Rule and pattern mining*</Data></Cell><Cell><Data ss:Type="String">Pattern Summarization, Uncertain Data</Data></Cell></Row><Row ss:Height="23.8392"><Cell><Data ss:Type="String">395</Data></Cell><Cell><Data ss:Type="String">Network Discovery via Constrained Tensor Analysis of fMRI Data</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">We pose the problem of network discovery which involves simplifying spatio-temporal data into nodes and edges. Such problems naturally exist in fMRI scans of human subjects. These scans consist of activations of thousands of voxels over time with the aim to simplify them into the underlying cognitive network being used. We propose supervised and semi-supervised variations of this problem and postulate a constrained tensor decomposition formulation and a corresponding alternating least squares solver that is easily implementable. We show this formulation works well in controlled experiments where supervision is incomplete, superfluous and noisy and is able to recover the underlying
ground truth network. We then show that for real fMRI data our approach can reproduce well known results in neurology regarding the default mode network in resting state healthy and Alzheimer affected individuals. Finally, we show that the reconstruction error of the decomposition provides a useful measure of the network strength and is useful at predicting key cognitive scores both by itself and with additional clinical information.</Data></Cell><Cell><Data ss:Type="String">Ian Davidson*, University of California - Davis</Data></Cell><Cell><Data ss:Type="String">Applications\Healthcare and medicine*; Mining rich data types; Unsupervised learning</Data></Cell><Cell><Data ss:Type="String">fMRI, spatial data</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">399</Data></Cell><Cell><Data ss:Type="String">Guided Learning for Role Discovery (GLRD): Framework, Algorithms, and Applications</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Role discovery in graphs is an emerging area that allows analysis of complex graphs in an intuitive way. In contrast to community discovery, which finds groups of highly connected nodes, the role discovery problem finds groups of nodes that share similar topological structure in the graph, and hence a common role or function such as being a broker or a periphery node. However, existing work so far is completely unsupervised, which is undesirable for a number of reasons. We provide an alternating least squares framework that allows convex constraints to be placed on the role discovery problem, which can provide useful supervision. In particular we explore supervision to enforce i) sparsity, ii) diversity, and iii) alternativeness in the roles. We illustrate the usefulness of this supervision on various data sets and applications.</Data></Cell><Cell><Data ss:Type="String">Sean Gilpin*, U.C. Davis; Tina Eliassi-Rad, ; Ian Davidson, University of California - Davis</Data></Cell><Cell><Data ss:Type="String">Graph mining*; Semi-supervised learning</Data></Cell><Cell><Data ss:Type="String">Role discovery, graph mining</Data></Cell></Row><Row ss:Index="49" ss:Height="12.6432"><Cell><Data ss:Type="String">408</Data></Cell><Cell><Data ss:Type="String">Quadratic Optimization to Identify Highly Heritable Quantitative Traits from Complex Disease Features</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Identifying genetic variation underlying a complex disease is important. Many complex diseases have heterogeneous phenotypes and are products of a variety of genetic and environmental factors acting in concert. Deriving highly heritable quantitative traits of a complex disease can improve the identification of genetic risk of the disease. The most sophisticated methods so far perform unsupervised cluster analysis on phenotypic features; and then a quantitative trait is derived based on each resultant cluster. Heritability is estimated to assess the validity of the derived quantitative traits. However, none of these methods explicitly maximize the heritability of the derived traits. We propose a quadratic optimization approach that directly utilizes heritability as an objective during the derivation of quantitative traits of a disease. This method maximizes an objective function that is formulated by decomposing the traditional maximum likelihood method for estimating heritability of a quantitative trait. We demonstrate the effectiveness of the proposed method in finding a trait with high heritability on synthetic data. We also apply our algorithm to identify highly heritable traits of complex human behavior disorders, such as opioid use disorder and cocaine use disorders. Our approach outperforms standard cluster analysis methods to identify highly heritable traits of opioid use and cocaine use.</Data></Cell><Cell><Data ss:Type="String">Jiangwen Sun, University of Connecticut; Jinbo Bi*, University of Connecticut; Henry Kranzler, University of Pennsylvania</Data></Cell><Cell><Data ss:Type="String">Information extraction*; Applications\Healthcare and medicine; Bioinformatics; Unsupervised learning</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">409</Data></Cell><Cell><Data ss:Type="String">Repetition-Aware Content Placement in Navigational Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Arguably, the most effective technique to ensure wide adoption of a concept (or product) is by repeatedly exposing individuals to messages that reinforce the concept (or promote the product).  Recognizing the role of repeated exposure to a message, in this paper we propose and study a novel framework for the effective placement of content: Given the navigational patterns of users in a network, e.g., web graph, hyperlinked corpus, or road network, and given a model of the relationship between content-adoption and frequency of exposition, we define the repetition-aware content-placement (RACP) problem as that of identifying the set of B nodes on which content should be placed so that the expected number of users adopting that content (conversion rate) is maximized. The key contribution of our work is the introduction of memory into the navigation process, by making user conversion dependent on the number of her exposures to that content. This dependency is captured using a conversion model that is general enough to capture arbitrary dependencies. Our solution to this general problem builds upon the notion of absorbing random walks, which we
extend appropriately in order to address the technicalities of our definitions. Although we show the RACP problem to be NP-hard, we propose a general and efficient algorithmic solution. We present experimental results which demonstrate the scalability of our approach and the usefulness of our framework by considering real-world datasets, including Web-graph data, road networks, and hyperlinked corpora from bibliographic datasets.
</Data></Cell><Cell><Data ss:Type="String">Dora Erdos*, Boston University; Vatche Ishakian, Boston University; Evimaria Terzi, Boston University; Azer Bestavros, Boston University</Data></Cell><Cell><Data ss:Type="String">Web mining*; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">Markov model, absorbing Markov chains, conversion probability</Data></Cell></Row><Row ss:Height="90.9936"><Cell><Data ss:Type="String">411</Data></Cell><Cell><Data ss:Type="String">Simple and Deterministic Matrix Sketching</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">We adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting. The algorithm receives the rows of a large matrix $A \in \R^{n \times m}$ one after the other in a streaming fashion. For $\ell = \lceil1/ \eps \rceil$ it maintains a sketch matrix $B \in \R^ { \ell \times m}$ such that for any unit vector $x$
\[
\|Ax\|^2 \ge \|Bx\|^2 \ge  \|Ax\|^2 - \eps \|A\|_{f}^2 \ .
\]
Sketch updates per row in $A$ require amortized $O(m\ell)$ operations. This gives the first algorithm whose error guaranty decreases proportional to $1/\ell$ using $O(m \ell)$ space. Prior art algorithms produce bounds proportional to $1/\sqrt{\ell}$. Our experiments corroborate that the faster convergence rate is observed in practice.
The presented algorithm also stands out in that it is: deterministic, simple to implement, and elementary to prove.

Regardless of streaming aspects, the algorithm can be used to compute a $1+\eps'$ approximation to the best rank $k$ approximation of any matrix $A \in \R^{n \times m}$. This requires $O(mn\ell')$ operations and $O(m\ell')$ space where $\ell' = \sum \sigma^2_i / \eps' \sigma^2_{k+1}$ and $\sigma_i$ are the singular values of $A$ in descending magnitude order. In many practical applications, e.g. PCA, $\ell'$ is assumed to be constant.</Data></Cell><Cell><Data ss:Type="String">Edo Liberty*, Yahoo!</Data></Cell><Cell><Data ss:Type="String">Data streams*; Big Data\Scalable methods; Dimensionality reduction</Data></Cell><Cell><Data ss:Type="String">Streaming matrix sketches, Matrix approximation, Low rank approximation</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">425</Data></Cell><Cell><Data ss:Type="String">Discovering Latent Influence in Online Social Activities via Shared Cascade Poisson Processes</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Many people share their activities with others through online communities. These shared activities have an impact on other users' activities. For example, users are likely to become interested in items that are adopted (e.g. liked, bought and shared) by their friends. In this paper, we propose a probabilistic model for discovering latent influence from sequences of item adoption events. An inhomogeneous Poisson process is used for modeling a sequence, in which adoption by a user triggers the subsequent adoption of the same item by other users. For modeling adoption of multiple items, we employ multiple inhomogeneous Poisson processes, which share parameters, such as influence for each user and relations between users. The proposed model can be used for finding influential users, discovering relations between users and predicting item popularity in the future. We present an efficient Bayesian inference procedure of the proposed model based on the stochastic EM algorithm. The effectiveness of the proposed model is demonstrated by using real data sets in a social bookmark sharing service.</Data></Cell><Cell><Data ss:Type="String">Tomoharu Iwata*, NTT Communication Science Laboratories; Amar Shah, University of Cambridge; Zoubin Ghahramani, Cambridge University</Data></Cell><Cell><Data ss:Type="String">Unsupervised learning\Topic, graphical and latent variable models*; Mining rich data types\Temporal / time series; Probabilistic methods; Social; User modeling</Data></Cell><Cell><Data ss:Type="String">Poisson processes, latent variable models, Bayesian inference</Data></Cell></Row><Row ss:Height="147.0024"><Cell><Data ss:Type="String">427</Data></Cell><Cell><Data ss:Type="String">Metric All-Pairs Similarity Search on MapReduce</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Given a set of entities, the all-pairs similarity search aims at
identifying all pairs of entities that have similarity greater
than (or distance smaller than) some user-defined threshold.
In this article, we propose a MapReduce-based framework
for solving this problem in metric spaces. Novel elements of
our solution include: i) flexible support for multiple metrics
of interest; ii) an autonomic approach to partition the in-
put dataset with minimal redundancy to achieve good load-
balance in the presence of limited computing resources; iii)
an on-the-fly lossless compression strategy to reduce both
the run time and the final output. We validate the utility,
scalability and the effectiveness of the approach on hundreds of machines using real and synthetic datasets.
</Data></Cell><Cell><Data ss:Type="String">Ye Wang*, OSU; Ahmed Metwally, Google Inc.; Srinivasan Parthasarathy, The Ohio State University</Data></Cell><Cell><Data ss:Type="String">Big Data*; Big Data\Distributed computing --- cloud, map-reduce, MPI, others; Big Data\Large scale optimization; Big Data\Scalable methods</Data></Cell><Cell><Data ss:Type="String">all-pair similarity search, scalability, MapReduce, streaming algorithm</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">434</Data></Cell><Cell><Data ss:Type="String">Indexed Block Coordinate Descent for Large-Scale Linear Classiﬁcation with Limited Memory</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Linear Classification has achieved complexity linear to the data size. However, in many applications, data contain large amount of samples that does not help improve the quality of model, but still cost much I/O and memory to process. In this paper, we show how a Block Coordinate Descent method based on Nearest-Neighbor Index can significantly reduce such cost when learning a dual-sparse model. In particular, we employ truncated loss function to induce a series of convex programs with superior dual sparsity, and solve each dual using Indexed Block Coordinate Descent, which makes use of Approximate Nearest Neighbor (ANN) search to select active dual variables without I/O cost on irrelevant samples. We prove that, despite the bias and weak guarantee from ANN query, the proposed algorithm has global convergence to the solution defined on entire dataset, with sublinear complexity in each iteration. Experiments in both sufficient and limited memory conditions show that the proposed approach learns times faster than other state-of-the-art solvers without sacrificing accuracy.</Data></Cell><Cell><Data ss:Type="String">En-Hsu Yen*, National Taiwan University; Chun-Fu Chang, National Taiwan University; Ting-Wei Lin, National Taiwan University; Shan-Wei Lin, National Taiwan University; Shou-De Lin, National Taiwan University</Data></Cell><Cell><Data ss:Type="String">Big Data\Large scale optimization*; Big Data; Big Data\Scalable methods; Nearest neighbors; Supervised learning\Classification; Supervised learning\Support vector machines</Data></Cell><Cell><Data ss:Type="String">Limited-Memory,Large-Scale,Linear Classification,Ramp-Loss,SVM,Block Coordinate Descent,Nearest-Neighbor</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">444</Data></Cell><Cell><Data ss:Type="String">Active Learning and Search on Low-Rank Matrices</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Low-rank matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. Using variational approximations or Markov chain Monte Carlo sampling to approximate the posterior distribution over models, we can choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many "positive" data points as possible. We evaluate our methods on simulated data, and also show their applicability to movie ratings prediction and the discovery of drug-target interactions.</Data></Cell><Cell><Data ss:Type="String">Dougal Sutherland*, Carnegie Mellon University; Barnabás Póczos, Carnegie Mellon University; Jeff Schneider, </Data></Cell><Cell><Data ss:Type="String">Recommender systems\Collaborative filtering*; Adaptive learning\Active learning; Recommender systems; Recommender systems\Cold-start; Unsupervised learning\Matrix/tensor factorization</Data></Cell><Cell><Data ss:Type="String">Active matrix factorization, collaborative prediction, drug discovery</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">454</Data></Cell><Cell><Data ss:Type="String">Massively Parallel Expectation Maximization Using Graphics Processing Units</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Composed of several hundreds of processors, the Graphics Processing Unit(GPU) has become a very interesting platform for computationally demanding tasks on massive data. A special hierarchy of processors and fast memory units allows very powerful and efficient parallelization but also demands novel parallel algorithms. Expectation Maximization (EM) is a widely used technique for maximum likelihood estimation. In this paper, we propose an innovative EM clustering algorithm particularly suited for the GPU platform on NVIDIA's Fermi architecture. The central idea of our algorithm is to allow the parallel threads exchanging their local information in an asynchronous way and thus updating their cluster representatives on demand by a technique called Asynchronous Model Updates (Async-EM). Async-EM enables our algorithm not only to accelerate convergence but also to reduce the overhead induced by memory bandwidth limitations and synchronization requirements. We demonstrate (1) how to reformulate the EM algorithm to be able to exchange information using Async-EM and (2) how to exploit the special memory and processor architecture of a modern GPU in order to share this information among threads in an optimal way. As a perspective Async-EM is not limited to EM but can be applied to a variety of algorithms.</Data></Cell><Cell><Data ss:Type="String">Muzaffer Can Altinigneli*, University of Munich; Claudia Plant, Helmholtz Center Munich; Christian Böhm, University of Munich</Data></Cell><Cell><Data ss:Type="String">Unsupervised learning\Clustering*; Big Data\Novel statistical techniques for big data; Unsupervised learning</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Index="57" ss:Height="12.6432"><Cell><Data ss:Type="String">478</Data></Cell><Cell><Data ss:Type="String">Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Many different machine learning algorithms exist; taking into account each algorithm’s hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.</Data></Cell><Cell><Data ss:Type="String">Chris Thornton*, UBC; Frank Hutter, UBC; Holger Hoos, UBC; Kevin Leyton-Brown, UBC</Data></Cell><Cell><Data ss:Type="String">Supervised learning*; Supervised learning\Classification</Data></Cell><Cell><Data ss:Type="String">model selection, hyperparameter optimization, WEKA</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">479</Data></Cell><Cell><Data ss:Type="String">Direct Optimization of Ranking Measures for Learning to Rank Models</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">We present a novel learning algorithm, DirectRank, which directly and exactly optimizes ranking measures without resorting to any upper bounds or approximations. Our approach is essentially an iterative coordinate ascent method in optimization. In each iteration, we choose one coordinate and only update the corresponding parameter, with all others remaining fixed. Since the ranking measure is a step-wise function of a single parameter, we propose an novel line search algorithm which could locate the interval with the best ranking measure along this coordinate quite efficiently. In order to stabilize our system in small datasets, we construct a probabilistic framework for document-query pairs to maximize the likelihood of objective permutation of top-τ documents. This iterative procedure ensures convergence. More, we integrate regression trees as our weak learners in order to gain more power. Experiments on Microsoft LETOR datasets and two large datasets, Yahoo challenge and Microsoft 30K web, show our improvements over state-of-the-art systems.</Data></Cell><Cell><Data ss:Type="String">Ming Tan*, Wright State University; Tian Xia, Wright State University; Lily Guo, Wright State University; Shaojun Wang, Wright State University</Data></Cell><Cell><Data ss:Type="String">Supervised learning\Learning to rank*; Big Data\Large scale optimization; Big Data\Scalable methods; Supervised learning; Web mining</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">503</Data></Cell><Cell><Data ss:Type="String">A Phrase Mining Framework for Recursive Construction of a Topical Hierarchy</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">A high quality hierarchical organization of the concepts in a dataset at different levels of granularity has many valuable applications such as search, summarization, and content browsing. In this paper we propose an algorithm for recursively constructing a hierarchy of topics from a collection of content-representative documents. We characterize each topic in the hierarchy by an integrated ranked list of mixed-length phrases. Our mining framework is based on a phrase-centric view for clustering, extracting, and ranking topical phrases. Experiments with datasets from three different domains illustrate our ability to generate hierarchies of high quality topics represented by meaningful phrases.</Data></Cell><Cell><Data ss:Type="String">Chi Wang, University of Illinois; Marina Danilevsky*, University of Illinois; Nihit Desai, University of Illinois at Urbana-Champaign; Yinan Zhang, University of Illinois at Urbana-Champaign; Phuong Nguyen, University of Illinois at Urbana-Champaign; Thrivikrama Taula, University of Illinois at Urbana-Champaign; Jiawei Han, University of Illinois at Urbana-Champaign</Data></Cell><Cell><Data ss:Type="String">Unsupervised learning*; Graph mining; Mining rich data types\Text; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">topical hierarchy, phrase mining, hierarchy construction, topical phrases, topical keyphrase extraction, phrase ranking, link-based mining</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">510</Data></Cell><Cell><Data ss:Type="String">Multi-space Probabilistic Sequence Modeling</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Learning algorithms that embed objects into Euclidean space have become the methods of choice for a wide range of problems, ranging from recommendation and image search to playlist prediction and language modeling. Probabilistic embedding methods provide elegant approaches to these problems, but can be expensive to train and store as a large monolithic model. In this paper, we propose a method that trains not one monolithic model, but multiple local embeddings for a class of pairwise conditional models especially suited for sequence and co-occurrence modeling. We show that computation and memory for training these multi-space models can be efficiently parallelized over many nodes of a cluster. Focusing on sequence modeling for music playlists, we show that the method substantially speeds up training while maintaining high model quality.</Data></Cell><Cell><Data ss:Type="String">Shuo Chen*, Dept. of Computer Science, Cornell University; Jiexun Xu, Dept. of Computer Science, Cornell University; Thorsten Joachims, Cornell</Data></Cell><Cell><Data ss:Type="String">Recommender systems*; Big Data\Distributed computing --- cloud, map-reduce, MPI, others</Data></Cell><Cell><Data ss:Type="String">Music Playlists, Recommendation, Embedding, Sequences, Parallel Computing</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">511</Data></Cell><Cell><Data ss:Type="String">DTW-D: Time Series Semi-Supervised Learning from a Single Example</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Classification of time series data is an important problem with applications in virtually every scientific endeavor. The large research community working on time series classification has typically used the UCR Archive to test their algorithms. In this work we argue that the availability of this resource has isolated much of the research community from the following reality, labeled time series data is often very difficult to obtain. 
The obvious solution to this problem is the application of semi-supervised learning; however, as we shall show, direct applications of off-the-shelf semi-supervised learning algorithms do not typically work well for time series. In this work we explain why semi-supervised learning algorithms typically fail for time series problems, and we introduce a simple but very effective fix. We demonstrate our ideas on diverse real word problems. 
</Data></Cell><Cell><Data ss:Type="String">Yanping Chen*, UCR; Bing Hu, ; Eamonn Keogh, University of California - Riverside; Gustavo  Batista, </Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Temporal / time series*; Nearest neighbors; Semi-supervised learning; Supervised learning\Classification</Data></Cell><Cell><Data ss:Type="String">Time Series, Semi-Supervised Learning, Classification</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">516</Data></Cell><Cell><Data ss:Type="String">Towards Never-Ending Learning from Time Series Streams </Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Time series classification has been an active area of research in the data mining community for over a decade, and significant progress has been made in the tractability and accuracy of learning.  However, virtually all work assumes a one-time training session in which labeled examples of all the concepts to be learned are provided. This assumption may be valid in a handful of situations, but it does not hold in most medical and scientific applications where we initially may have only the vaguest understanding of what concepts can be learned. Based on this observation, we propose a never-ending learning framework for time series in which an agent examines an unbounded stream of data and occasionally asks a teacher (which may be a human or an algorithm) for a label. We demonstrate the utility of our ideas with experiments that consider real world problems in domains as diverse as medicine, entomology, wildlife monitoring and human behavior analyses.       </Data></Cell><Cell><Data ss:Type="String">Yuan Hao*, Univ of California, Riverside; Yanping Chen, UCR; Jesin Zakaria, ; Bing Hu, ; Thanawin Rakthanmanon, ; Eamonn Keogh, University of California - Riverside</Data></Cell><Cell><Data ss:Type="String">Data streams*; Adaptive learning\Active learning; Mining rich data types\Temporal / time series; Supervised learning\Classification</Data></Cell><Cell><Data ss:Type="String">Never-Ending Learning, Classification, Data Streams, Time Series</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">526</Data></Cell><Cell><Data ss:Type="String">Constrained Stochastic Gradient Descent for Large-scale Least Squares Problem</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The least squares problem is one of the most important regression problems in statistics, machine learning and data mining. In this paper, we present the Constrained Stochastic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by imposing a provable constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound $O(\log{T})$, and fastest convergence speed among all first order approaches. Empirical studies justify the effectiveness of CSGD by comparing it with SGD and other state-of-the-art approaches. An example is also given to show how to use CSGD to optimize SGD based least squares problems to achieve a better performance.</Data></Cell><Cell><Data ss:Type="String">Yang Mu*, UMass Boston; Wei Ding, University of Massachusetts Boston; Tianyi Zhou, University of Technology Sydney; Dacheng Tao, University of Technology Sydney</Data></Cell><Cell><Data ss:Type="String">Big Data\Large scale optimization*; Supervised learning\Classification; Supervised learning\Neural networks; Supervised learning\Regression</Data></Cell><Cell><Data ss:Type="String">stochastic optimization, least squares, regression</Data></Cell></Row><Row ss:Height="225.3528"><Cell><Data ss:Type="String">528</Data></Cell><Cell><Data ss:Type="String">Diversity Maximization Under Matroid Constraints</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Aggregator websites typically present documents in the form of representative clusters. 
In order for users to get a broader perspective, it is important to deliver a 
diversified set of representative documents in those clusters. 
One approach to diversification is to maximize 
the average dissimilarity among documents. 
Another way to capture diversity is to 
avoid showing several documents from the same category (e.g. from the same news channel). 
We model the latter approach as a (partition) matroid constraint, and 
study diversity maximization problems under matroid constraints. 
We present the first constant-factor approximation algorithm for this problem, 
using a new technique. 
Our local search $0.5$-approximation algorithm 
is also the first constant-factor approximation for the max-dispersion problem under matroid constraints. Our combinatorial proof technique for maximizing diversity under matroid constraints 
uses the existence of a family of Latin squares which may also be of independent interest. 

In order to apply these diversity maximization algorithms in the context of aggregator websites and as a preprocessing step for our diversity maximization tool, we develop 
greedy clustering algorithms that maximize weighted coverage of a predefined set of topics. Our algorithms are based on computing a set of cluster centers, where clusters are formed around them. 
We show the better performance of our algorithms for diversity 
and coverage maximization by running experiments on real (Twitter) and synthetic data in the context of real-time search over micro-posts. 
Finally we perform a user study validating our algorithms and diversity metrics.</Data></Cell><Cell><Data ss:Type="String">Zeinab Abbassi*, Columbia University; Vahab Mirrokni, Google; Mayur Thakur, Google</Data></Cell><Cell><Data ss:Type="String">Foundations*; Graph mining; Recommender systems</Data></Cell><Cell><Data ss:Type="String">Diversity Maximization,
Coverage Maximization,
Product Search,
Microblog Mining,
Online Clustering,
Matroid Constraints</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">543</Data></Cell><Cell><Data ss:Type="String">Succinct Interval-Splitting Tree for Scalable Similarity Search of Compound-Protein Pairs with Property Constraints</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Analyzing functional interactions between small compounds and proteins is indispensable in genomic drug discovery. Since rich information on various compound-protein interactions is available in recent molecular databases, strong demands for making best use of such databases require to invent powerful methods to help us find new functional compound- protein pairs on a large scale. We present the succinct interval-splitting tree algorithm (SITA) that efficiently performs similarity search in databases for compound-protein pairs with respect to both binary fingerprints and real-valued properties. SITA achieves both time and space efficiency by developing the data structure called interval-splitting trees, which enables to efficiently prune the useless portions of search space, and by incorporating the ideas behind wavelet tree, a succinct data structure to compactly represent trees. We experimentally test SITA on the ability to retrieve similar compound-protein pairs/substrate-product pairs for a query from large databases with over 200 million compound- protein pairs/substrate-product pairs and show that SITA performs better than other possible approaches.</Data></Cell><Cell><Data ss:Type="String">Yasuo Tabei*, JST; Akihiro Kishimoto, IBM Research, Dublin; Masaaki Kotera, Kyoto University; Yoshihiro Yamanishi, Kyushu university</Data></Cell><Cell><Data ss:Type="String">Applications\Healthcare and medicine*; Bioinformatics</Data></Cell><Cell><Data ss:Type="String">drug discovery, drug-target interaction prediction</Data></Cell></Row><Row ss:Index="66" ss:Height="57.456"><Cell><Data ss:Type="String">550</Data></Cell><Cell><Data ss:Type="String">Making Recommendations from Multiple Domains</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Given the vast amount of information on theWorldWideWeb, recommender systems are increasingly being used to help filter irrelevant data and suggest information that would interest users. Traditional systems make recommendations based on a single domain e.g., movie or book domain. Recent work examine the correlations in different domains and design models that exploit user preferences on a source domain to predict user preferences on a target
domain. However, these methods are based on matrix factorization and can only be applied to two-dimensional data. Transferring high dimensional data from one domain to another requires decomposing the high dimensional data to binary relations which results in information loss. Furthermore, this decomposition creates a large
number of matrices that need to be transferred and combining them in the target domain is non-trivial. Separately, researchers have looked into using social network information to improve recommendation. However, this social network information has not been explored in cross domain collaborative filtering. In this work, we
propose a generalized cross domain collaborative filtering framework that integrates social network information seamlessly with cross domain data. This is achieved by utilizing tensor factorization with topic based social regularization. This framework is able to transfer high dimensional data without the need for decomposition by finding shared implicit cluster-level tensor from multiple
domains. Extensive experiments conducted on real world datasets indicate that the proposed framework outperforms state-of-art algorithms for item recommendation, user recommendation and tag recommendation.</Data></Cell><Cell><Data ss:Type="String">Wei Chen, National University of Singapore; Wynne Hsu, National University of Singapore; Mong-Li Lee*, National University of Singapore</Data></Cell><Cell><Data ss:Type="String">Recommender systems*; Unsupervised learning\Matrix/tensor factorization</Data></Cell><Cell><Data ss:Type="String">Cross domain</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">554</Data></Cell><Cell><Data ss:Type="String">Cascading Outbreak Prediction in Networks: A Data-Driven Approach</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Cascades are ubiquitous in various network environments such as epidemic networks, traffic networks, water distribution networks and social networks. The outbreaks of cascades will often bring bad or even devastating effects. How to accurately predict the cascading outbreaks in early stage is of paramount importance for people to avoid these bad effects. Although there have been some pioneering works on cascading outbreaks detection, how to predict, rather than detect, the cascading outbreaks is still an open problem. In this paper, we attempt harnessing historical cascade data, propose a novel data driven approach to select important nodes as sensors, and predict the outbreaks based on the cascading behaviors of these sensors. In particular, we propose Orthogonal Sparse LOgistic Regression (OSLOR) method to jointly optimize node selection and outbreak prediction, where the prediction loss are combined with an orthogonal regularizer and L1 regularizer to guarantee good prediction accuracy and the sparsity and low-redundancy of selected sensors. We evaluate the proposed method on a real online social network dataset including 182.7 million information cascades. The experimental results show that the proposed OSLOR significantly and consistently outperform topological measure based method and other data driven methods in prediction performances.</Data></Cell><Cell><Data ss:Type="String">Shifei JIN, Tsinghua University; Peng Cui*, Tsinghua University; Linyun Yu, Tsinghua University; Fei Wang, IBM T. J. Watson Research Lab; Shiqiang Yang, Tsinghua University</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Social\Social media</Data></Cell><Cell><Data ss:Type="String">Information Cascades, Outbreak Prediction, Social Network, Data Driven Approach</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">556</Data></Cell><Cell><Data ss:Type="String">Social Influence Based Clustering of Heterogeneous Information Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Social networks continue to grow in size and the type of information hosted. We witness a growing interest in clustering a social network of people based on both their social relationships and their participations in activity based information networks. In this paper, we present a social influence based clustering framework for analyzing heterogeneous information networks with three unique features. First, we introduce a novel social influence based vertex similarity metric in terms of both self-influence similarity and co-influence similarity. We compute self-influence and co-influence based similarity in terms of propagating heat diffusion kernel on social graph and on its associated activity graphs and influence graphs respectively. Second, we compute the combined social influence based similarity between each pair of vertices by unifying the self-similarity and multiple co-influence similarity scores through a weight function with an iterative update method. Third, we design an iterative learning algorithm, SI-Cluster, for social influence based graph clustering. It can dynamically refine the K clusters by continuously quantifying and adjusting the weights on self-influence similarity and on multiple co-influence similarity scores towards the clustering convergence. To make SI-Cluster converge fast, we transformed a sophisticated nonlinear fractional programming problem of multiple weights into a straightforward nonlinear parametric programming problem of single variable. Our experiment results show that SI-Cluster not only achieves a better balance between self-influence and co-influence similarities but also scales extremely well for large graph clustering. </Data></Cell><Cell><Data ss:Type="String">Yang Zhou*, Georgia Institute of Technolog; Ling Liu, Georgia Institute of Technology</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Social</Data></Cell><Cell><Data ss:Type="String">Graph Clustering, Social Influence, Heterogeneous Information Network</Data></Cell></Row><Row ss:Height="247.7448"><Cell><Data ss:Type="String">569</Data></Cell><Cell><Data ss:Type="String">Selective Sampling on Graphs for Classification</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Selective sampling is an active variant of online learning in which
the learner is allowed to adaptively query the label of an observed
example. The goal of selective sampling is to achieve a good
trade-off between prediction performance and the number of queried
labels. Existing selective sampling algorithms are designed for
vector-based data. In this paper, motivated by the ubiquity of graph
representations in real-world applications, we propose to study
selective sampling on graphs. We first present an online version of
the well-known Learning with Local and Global Consistency method
(OLLGC). It is essentially a second-order online learning algorithm,
and can be seen as an online ridge regression in the Hilbert space
of functions defined on graphs. We prove its regret bound in terms
of the structural property (cut size) of a graph, or the spectral
property of graph Laplaican. Based on OLLGC, we present a selective
sampling algorithm, namely Selective Sampling with Local and Global
Consistency (SSLGC), which queries the label of each node based on the
confidence of the linear function on graphs. Its bound on the label
complexity is also derived. We analyze the low-rank approximation of graph kernels, which enables the online algorithms scale to large graphs. Experiments on benchmark graph datasets
show that OLLGC outperforms the state-of-the-art first-order
algorithm significantly, and SSLGC achieves comparable or even better
results than OLLGC while querying substantially fewer nodes.
Moreover, SSLGC is overwhelmingly better than random sampling.</Data></Cell><Cell><Data ss:Type="String">Quanquan Gu*, CS, UIUC; Charu Aggarwal, IBM Research; Jialu Liu, UIUC; Jiawei Han, University of Illinois at Urbana-Champaign</Data></Cell><Cell><Data ss:Type="String">Supervised learning*; Adaptive learning; Sampling; Semi-supervised learning\Learning with partial labels; Supervised learning\Classification</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">571</Data></Cell><Cell><Data ss:Type="String">WiseMarket: A New Paradigm for Managing Wisdom of Online Social Users</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The benefits of crowdsourcing are well-recognized today for an increasingly broad range of problems. Meanwhile, the rapid development of social media makes it possible to seek the wisdom of a crowd in a broader sphere. However, it is not trivial to implement crowdsourcing platform on social media, specifically to make social media users as workers, we need to address the following two challenges: 1) how to motivate users to participate in tasks, and 2) how to aggregate their opinions. In this paper, we present Wise Market as an effective institution of crowds on social media to motivate users to participate in a task with care and correctly aggregate their opinions on pairwise choice problems. The Wise Market consists of a set of investors each with an associated individual confidence in their prediction, and after investment, only the ones whose choices are the same as the whole market are granted rewards. Therefore, the social media user has to give his/her “best” answer in order to get reward, discouraging careless answers from sloppy users.Using this framework, we define the Effective Market Problem (EMP) as an optimization problem to minimize expected cost of paying out rewards while guaranteeing aminimum confidence level. We propose exact algorithms for calculating the market confidence and the expected cost with O(n log2 n) time cost in a Wise Market with n investors. Especially in the real situations of social media where the numbers of users are enormous, we design a Central Limit Theorem-based approximation algorithm for calculating the market confidence with O(n) time cost, as well as a bounded approximation algorithm for calculating the expected cost with O(n) time cost. Finally, we conducted empirical studies to validate effectiveness of the proposed algorithms on real and synthetic data.</Data></Cell><Cell><Data ss:Type="String">Chen Cao, HKUST; Yongxin Tong, HKUST; Lei Chen*, HKUST; H.V.  Jagadish, University of Michigan</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Economy, markets</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">572</Data></Cell><Cell><Data ss:Type="String">Querying Discriminative and Representative Samples for Batch Mode Active Learning</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Empirical risk minimization (ERM) provides a principal guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label and these data may have different distribution. In this paper, we generalize the empirical risk minimization principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound we develop a practical batch mode active learning method. The proposed formulation involves a non-convex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying most uncertain and representative queries. Experiments on benchmark data sets and real-world applications demonstrate the superior performance of our proposed method in comparison with the state-of-the-art methods.</Data></Cell><Cell><Data ss:Type="String">Zheng Wang*, Arizona State University; Jieping Ye, Arizona State University</Data></Cell><Cell><Data ss:Type="String">Adaptive learning\Active learning*; Supervised learning\Classification</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">577</Data></Cell><Cell><Data ss:Type="String">Recursive Regularization for Large-scale Classification with Hierarchical and Graphical Dependencies</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The two key challenges in hierarchical classification are to leverage the hierarchical dependencies between the class-labels for improving performance, and, at the same time maintaining scalability across large hierarchies. In this paper we propose a regularization framework for large-scale hierarchical classification that addresses both the problems. Specifically, we incorporate the hierarchical dependencies between the class-labels into the regularization structure of the parameters thereby encouraging classes nearby in the hierarchy to share similar model parameters. Furthermore, we extend our approach to scenarios where the dependencies between the class-labels are encoded in the form of a graph rather than a hierarchy. To enable large-scale training, we develop a parallel-iterative optimization scheme that can comfortably handle the largest datasets with hundreds of thousands of classes and millions of instances and learning terabytes of parameters. Our experiments showed consistent and significant performance improvements over other competing approaches and achieved state-of-the-art results on leading benchmark datasets.</Data></Cell><Cell><Data ss:Type="String">Siddharth Gopal*, CMU; Yiming Yang, CMU</Data></Cell><Cell><Data ss:Type="String">Supervised learning\Classification*; Big Data\Distributed computing --- cloud, map-reduce, MPI, others; Big Data\Large scale optimization</Data></Cell><Cell><Data ss:Type="String">Hierarchical Classification, Recursive Regularization, Parallel Optimization, Large-scale Evaluation</Data></Cell></Row><Row ss:Height="169.4016"><Cell><Data ss:Type="String">583</Data></Cell><Cell><Data ss:Type="String">Denser than the densest subgraph: extracting optimal quasi-cliques with quality guarantees</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Finding dense subgraphs is an important graph-mining task with many applications.
Given that the direct optimization of edge density is not meaningful, as even a single edge achieves maximum density,
research has focused on optimizing alternative density functions.
A very popular among such functions is the average degree, whose maximization leads to the well-known \emph{densest-subgraph} notion. 
Surprisingly enough, however, densest subgraphs are typically large graphs, with small edge density, and large diameter.

In this paper, we define a novel density function, which gives subgraphs of much higher quality than densest subgraphs: the graphs found by our method are compact, dense, and with smaller diameter.
We show that the proposed function can be derived from a general framework, which includes other important density functions as subcases and for which we show interesting general theoretical properties.

We evaluate our algorithms on real and synthetic datasets,
and we also devise several application studies as variants
of our original problem. When compared with the method
that finds the subgraph of the largest average degree, our
algorithms return denser subgraphs with smaller diameter.
Finally, our work opens new research directions which we discuss in detail.</Data></Cell><Cell><Data ss:Type="String">Charalampos Tsourakakis*, Carnegie Mellon University; Francesco Bonchi, Yahoo! Research; Aristides Gionis, Aalto University; Francesco Gullo, Yahoo! Research; Maria Tsiarli, University of Pittsburgh</Data></Cell><Cell><Data ss:Type="String">Graph mining*; Social\Social and information networks</Data></Cell><Cell><Data ss:Type="String">dense subgraph, algorithm engineering, experimentation</Data></Cell></Row><Row ss:Index="74" ss:Height="46.26"><Cell><Data ss:Type="String">589</Data></Cell><Cell><Data ss:Type="String">Combining Latent Factor Model with Location Features for Event-based Group Recommendation</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Groups play an essential role in many social websites which promote users' interactions and accelerate the diffusion of information. Recommending groups that users are really interested to join is significant for both users and social media. While traditional group recommendation problem has been extensively studied, we focus on a new type of the problem, i.e., event-based group recommendation. Unlike the other forms of groups, users join this type of groups mainly for participating offline events organized by group members or inviting other users to attend events sponsored by them. These characteristics determine that previously proposed approaches for group recommendation cannot adapt to the new problem easily as they ignore the geographical influence and other explicit features of groups and users.

In this paper, we propose a method called Pairwise Tag-enhAnced and featuRe-based Matrix factorIzation for Group recommendAtioN (PTARMIGAN), which considers location features, social features, and implicit patterns
simultaneously in a unified model. More specifically, we exploit matrix factorization to model interactions between users and groups. Meanwhile, we incorporate their profile information into pairwise enhanced latent factors respectively. We also utilize the linear model to capture explicit features. Due to the reinforcement between explicit features and implicit patterns, our approach can provide better group recommendations. We conducted a comprehensive performance evaluation on real word data sets and the experimental results demonstrate the effectiveness of our method.</Data></Cell><Cell><Data ss:Type="String">Wei Zhang*, Department of Computer Science; Jianyong Wang, Tsinghua University</Data></Cell><Cell><Data ss:Type="String">Recommender systems*; Social; User modeling</Data></Cell><Cell><Data ss:Type="String">Event-based Group Recommendation, Location Feature, Latent Factor Model</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">596</Data></Cell><Cell><Data ss:Type="String">Cost-Sensitive Online Active Learning with Application to Malicious URL Detection</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Malicious Uniform Resource Locator (URL) detection is an important problem in web search and mining, which plays a critical role in network security. In literature, most existing studies have attempted to formulate the problem as a regular supervised classification task which typically aims to optimize the prediction accuracy. However, in a real-world malicious URL detection task, the ratio between the number of malicious URLs and legitimate URLs is highly imbalanced, making it very inappropriate for simply optimizing the prediction accuracy. Besides, another key limitation of the existing work is to assume a large amount of training data is available, which is impractical as the human labeling cost could be potentially quite expensive. To overcome these limitations, in this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL), which only queries a small fraction of training data for labeling and directly optimizes two cost-sensitive measures to address the class-imbalance issue. In particular, we propose two CSOAL algorithms and analyze their theoretical performance in terms of cost-sensitive bounds. We conduct an extensive set of experiments to examine the empirical performance of the proposed algorithms for a large-scale challenging malicious URL detection task, in which the encouraging results showed that the proposed technique by querying an extremely small-sized labeled data (about 0.5\% out of 1-million instances) can achieve better or highly comparable classification performance in comparison to the state-of-the-art regular and cost-sensitive online classification algorithms using a huge amount of labeled data. </Data></Cell><Cell><Data ss:Type="String">Peilin ZHAO, Nanyang Technological Universi; Steven Hoi*, Nanyang Technological University, Singapore.</Data></Cell><Cell><Data ss:Type="String">Adaptive learning\Active learning*; Big Data\Scalable methods; Data streams; Security and privacy; Semi-supervised learning\Anomaly/novelty detection; Web mining</Data></Cell><Cell><Data ss:Type="String">Malicious URL Detection, Cost-Sensitive learning, Online Learning, Active Learning</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">611</Data></Cell><Cell><Data ss:Type="String">Connecting Users across Social Media Sites: A Behavioral-Modeling Approach</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">People use various social media for different purposes. The information on each site is often partial. When sources of complementary information are integrated, a better profile of a user can be built to improve online services such as verifying online information. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem.  We introduce a methodology (MOBIUS) for finding a mapping among identities of individuals across social media sites. It consists of three key components: the first component identifies users' unique behavioral patterns that lead to information redundancies across sites; the second component constructs features that exploit information redundancies due to these behavioral patterns; and the third component employs machine learning for effective user identification. We formally define the cross-media user identification problem, show that MOBIUS is effective in identifying users across social media sites, and discuss how further improvements can be made. This study paves the way for analysis and mining across social media sites, and facilitates the creation of novel online services across social media sites.</Data></Cell><Cell><Data ss:Type="String">Reza Zafarani*, Arizona State University; Huan Liu, Arizona State University</Data></Cell><Cell><Data ss:Type="String">Social\Social media*; User modeling; Web mining</Data></Cell><Cell><Data ss:Type="String">user identification
connecting users across sites
behavioral modeling</Data></Cell></Row><Row ss:Height="57.456"><Cell><Data ss:Type="String">618</Data></Cell><Cell><Data ss:Type="String">The Bang for the Buck: Fair Competitive Viral Marketing from the Host Perspective</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The key algorithmic problem in viral marketing is identifying a set of influential users (called seeds) in a social network, who, when convinced to adopt a product, shall influence other users in the network, leading to a large number of adoptions. When two or more players compete with similar products on the same network we talk about competitive viral marketing, which so far has been studied exclusively from the perspective of one of the competing players.

In this paper we propose and study the novel problem of competitive viral marketing from the perspective of the host, i.e., the third party owning the social networks. The host sells viral marketing campaign as a service to its customers, keeping control of the selection of seeds. Each company specifies its seed budget and the host allocates the seeds accordingly. From the host’s perspective, it is important not only to choose the seeds to maximize the collective expected spread, but also to assign seeds to companies so that it guarantees the “bang for the buck" for all companies is nearly identical, which we formalize as the fair seed allocation problem.

We propose a new diffusion model that extends the classical Linear Threshold model to capture the competitive aspect in viral marketing. Our model is intuitive and retains the desired properties of monotonicity and submodularity. We show that the fair seed allocation problem is NP-hard, and develop an efficient algorithm called Needy Greedy. We run experiments on three real-world social networks, showing that our algorithm is effective and scalable.</Data></Cell><Cell><Data ss:Type="String">Wei Lu*, University of British Columbia; Francesco Bonchi, Yahoo! Research; Amit Goyal, University of British Columbia; Laks V.S. Lakshmanan, </Data></Cell><Cell><Data ss:Type="String">Economy, markets\Viral marketing*; Social\Social and information networks</Data></Cell><Cell><Data ss:Type="String">influence diffusion; influence propagation; competitive influence maximization; host perspective; fair allocation; Linear Threshold model; submodularity</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">623</Data></Cell><Cell><Data ss:Type="String">A General Bootstrap Performance Diagnostic</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">As datasets become larger, more complex, and more available to diverse groups of analysts, it would be quite useful to be able to automatically and generically assess the quality of estimates, much as we are able to automatically train and evaluate predictive models such as classifiers. However, despite the fundamental importance of estimator quality assessment in data analysis, this task has eluded highly automatic solutions.  While the bootstrap provides perhaps the most promising step in this direction, its level of automation is limited by the difficulty of evaluating its finite sample performance and even its asymptotic consistency.  Thus, we present here a general diagnostic procedure which directly and automatically evaluates the accuracy of the bootstrap's outputs, determining whether or not the bootstrap is performing satisfactorily when applied to a given dataset and estimator.  We show via an extensive empirical evaluation on a variety of estimators and simulated and real datasets that our proposed diagnostic is effective. Our experiments on a real-world query workload from Conviva Inc. involving 1.7TB of data (n=500M) show that the diagnostic correctly identifies that the bootstrap is applicable on 219 out of the 268 SQL-like queries, with 7 false positives and 12 false negatives.</Data></Cell><Cell><Data ss:Type="String">Ariel Kleiner, ; Ameet Talwalkar*, UC Berkeley; Sameer Agarwal, ; Michael Jordan, ; Ion Stoica, </Data></Cell><Cell><Data ss:Type="String">Unsupervised learning*; Big Data\Novel statistical techniques for big data</Data></Cell><Cell><Data ss:Type="String">bootstrap diagnostic, quantifying estimator uncertainty</Data></Cell></Row><Row ss:Height="68.652"><Cell><Data ss:Type="String">627</Data></Cell><Cell><Data ss:Type="String"> MI$^2$LS:  Multi-Instance Learning from Multiple Information Sources</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">In Multiple Instance Learning (MIL), each entity  is normally expressed as a set of instances.    Most of the current MIL methods only deal with the case when each instance is represented by one type of features. However, in many real world applications, entities are often
described from several different information sources (views). For example, when applying MIL to image categorization, the characteristics of each image can be derived from both its RGB features and SIFT features. Previous research work has shown that, in traditional learning methods, by considering the consistencies between different information sources, the classification performance can be improved.

 To incorporate  the consistencies between different  information sources into MIL, we propose a novel research framework --  Multi-Instance Learning from Multiple Information Sources (MI$^2$LS).  Based on this framework,
an algorithm -- Fast MI$^2$LS (FMI$^2$LS) is designed for this research framework, which combines Concave-Convex Constraint Programming (CCCP) method and an adapted Stoachastic Gradient Descent (SGD) method. Some theoretical analysis on time complexity and generalization error bound are given based on the proposed method. Experimental results on three datasets, i.e., Reuters21578, WebKB,
 and a novel application -- insider threat detection dataset, demonstrate the superior performance of the proposed method against several  state-of-the-art MIL techniques on both efficiency and effectiveness.</Data></Cell><Cell><Data ss:Type="String">Dan Zhang*, Purdue University; Jingrui He, Stevens Institute of Technolog; Richard Lawrence, IBM Research</Data></Cell><Cell><Data ss:Type="String">Supervised learning\Classification*; Big Data\Large scale optimization</Data></Cell><Cell><Data ss:Type="String">Multiple Instance Learning, Large Scale, Multi-View Learning</Data></Cell></Row><Row ss:Height="12.1032"><Cell><Data ss:Type="String">629</Data></Cell><Cell><Data ss:Type="String">Modeling the Dynamics of Composite Social Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Online social networks are becoming increasingly popular. Modeling the dynamics of these networks over time not only helps us understand the evolution of network structures and user behaviors, but also improves the performance of other analysis tasks, such as link prediction and community detection. Nowadays, users engage in multiple networks and the common users serve as bridges to connect different social networks and form a ``composite social network''. State-of-the-art network dynamics analysis is performed in isolation for individual networks, but users' interactions in one network can influence their behaviors in other networks, and in an individual network, different types of user interactions also affect each other. Without considering the influences across networks, one may not be able to model the dynamics in a given network correctly due to the lack of information. In this paper, we study the problem of modeling the dynamics of composite networks, where the evolution processes of different networks are jointly considered. However, due to the difference in network properties, simply merging multiple networks into a single one is not ideal because individual evolution patterns may be ignored and network differences may bring negative impacts. The proposed solution is a nonparametric Bayesian model, which models each user's common latent features to extract the cross-network influences, and use network-specific factors to describe different networks' evolution patterns. Empirical studies on large-scale dynamic composite social networks demonstrate that the proposed approach improves the performance of link prediction over several state-of-the-art baselines significantly and unfolds the network evolution accurately.</Data></Cell><Cell><Data ss:Type="String">Erheng Zhong*, HKUST; Wei Fan, IBM Research; Yin Zhu, ; Qiang Yang, Hong Kong University of Science and Technology</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Social\Link prediction; Social\Social media; Transfer learning</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell><Data ss:Type="String">631</Data></Cell><Cell><Data ss:Type="String">Learning to question: Leveraging user preferences for shopping advice</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">We present ShoppingAdvisor, a novel recommender system that helps users in shopping technical products in an e-commerce website. ShoppingAdvisor leverages both user preferences and technical product attributes in order to generate its suggestions. The system elicits user preferences via a binary-tree-shaped flowchart, where each node is a question to the user. At each node, ShoppingAdvisor suggests a ranked list of products that match the preferences of the user, and that gets progressively re fined along the path from the root of the tree to one of its leafs. In this paper we show (i) how to learn the structure of the tree, i.e., which questions to ask at each node, and (ii) how to produce a suitable ranking at each node. First, we adapt the classical top-down strategy for building decision trees in order to find the best user attribute to ask at each node. Differently than decision trees, ShoppingAdvisor partitions the user space rather than the product space. Second, we show how to employ a learning-to-rank approach in order to learn, at each node of the tree, a ranking of products appropriate to the users who reach at that node. We experiment with two real-world datasets for cars and cameras, and a synthetic one. We use mean reciprocal rank to evaluate ShoppingAdvisor, and show how the performance increases by more than 50% along the path from root to leaf. We also show how collaborative recommendation algorithms such as k-nearest neighbor benefits from feature selection done by the ShoppingAdvisor tree. Our experiments show that ShoppingAdvisor produces good quality interpretable recommendations, while requiring less input from users and being able to handle the cold-start problem. </Data></Cell><Cell><Data ss:Type="String">Mahashweta Das*, Univ of Texas at Arlington; Gianmarco De Francisci Morales, Yahoo! Research; Aristides Gionis, Aalto University; Ingmar Weber, Qatar Computing Research Institute</Data></Cell><Cell><Data ss:Type="String">Recommender systems*; Applications\E-commerce; Social\Social media; Web mining</Data></Cell><Cell><Data ss:Type="String">recommendation, learning, ranking, collaborative content</Data></Cell></Row><Row ss:Index="82" ss:Height="12.1032"><Cell><Data ss:Type="String">668</Data></Cell><Cell><Data ss:Type="String">Mining High Utility Episodes in Complex Event Sequences</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Frequent episode mining (FEM) is an interesting research topic in data mining with wide range of applications. However, the traditional framework of FEM treats all events as having the same importance/utility and assumes that a same type of event appears at most once at a time point. These simplifying assumptions do not reflect the characteristics of scenarios in real applications and thus the useful information of episodes in terms of utilities such as profits is lost. Furthermore, most studies on FEM focused on mining episodes in simple event sequences and few considered the scenario of complex event sequences, where different events can occur simultaneously. To address these issues, in this paper, we incorporate the concept of utility into episode mining and address a new problem of mining high utility episodes from complex event sequences, which has not been explored so far. In the proposed framework, the importance/utility of different events is considered and multiple events can appear simultaneously. Several novel features are incorporated into the proposed framework to resolve the challenges raised by this new problem, such as the absence of anti-monotone property and the huge set of candidate episodes. Moreover, an efficient algorithm named UP-Span (Utility ePisodes mining by Spanning prefixes) is proposed for mining high utility episodes with several strategies incorporated for pruning the search space to achieve high efficiency. Experimental results on real and synthetic datasets show that UP-Span has excellent performance and serves as an effective solution to the new problem of mining high utility episodes from complex event sequences.</Data></Cell><Cell><Data ss:Type="String">Cheng-Wei Wu, National Cheng Kung University; Yu Feng Lin, National Cheng Kung University, Taiwan, ROC; Philip Yu, University of Illinois; Vincent Tseng*, National Cheng Kung University</Data></Cell><Cell><Data ss:Type="String">Rule and pattern mining*; Mining rich data types; Mining rich data types\Sequence</Data></Cell><Cell><Data ss:Type="String">Utility mining, episode mining, high utility episodes, complex event sequences</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">685</Data></Cell><Cell><Data ss:Type="String">Time-Dependent Loss Enhanced SVM for Time Series Regression</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Support Vector Machines (SVMs) are a leading tool in machine learning and have been used with considerable success for the task of time series forecasting. However,
a key challenge when using SVMs for time series is the question of how to deeply integrate time elements into the learning process. To meet this challenge, we investigated the distribution of errors in the forecasts delivered by standard SVMs. Once we identified the samples that produced the largest errors, we are able to derive their distinction from outliers/noisy samples and detect their correlation with distribution shifts that occur in the time series. This is why we propose a time-dependent loss function which allows the inclusion of the information about the distribution shifts in the series directly into the SVM learning process. We present experimental results which 
indicate that using a time-dependent loss function is highly promising, reducing the overall variance of the errors, as well as delivering more accurate predictions.</Data></Cell><Cell><Data ss:Type="String">Goce Ristanoski*, The University of Melbourne; Wei Liu, The University of Melbourne; James Bailey, The University of Melbourne</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Temporal / time series*; Supervised learning\Regression; Supervised learning\Support vector machines</Data></Cell><Cell><Data ss:Type="String">Time Series, Support Vector Machine, Loss Function.</Data></Cell></Row><Row ss:Height="23.8392"><Cell><Data ss:Type="String">688</Data></Cell><Cell><Data ss:Type="String">A New Collaborative Filtering Approach for Increasing the Aggregate Diversity of Recommender Systems</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">In order to satisfy and positively surprise the users, a recommender system needs to recommend items the users will like and most probably would not have found on their own. This requires the recommender system to recommend a broader range of items including niche items as well. Such an approach also support online-stores that often offer more items than traditional stores and need recommender systems to enable users to find the not so popular items as well. However, popular items that hold a lot of usage data are more easy to recommend and, thus, niche items are often excluded from the recommendations. In this paper, we propose a new collaborative filtering approach that is based on the items' usage contexts. The approach increases the rating predictions for niche items with fewer usage data available and improves the aggragate diversity of the recommendations.</Data></Cell><Cell><Data ss:Type="String">Katja Niemann*, Fraunhofer FIT; Martin Wolpers, Fraunhofer Institute for Applied Information Technology</Data></Cell><Cell><Data ss:Type="String">Recommender systems*; Recommender systems\Cold-start; Recommender systems\Collaborative filtering</Data></Cell><Cell><Data ss:Type="String">Recommender Systems, Item-Item Similarity, Usage Context, Aggregate Diversity, Long Tail
Niche Items</Data></Cell></Row><Row ss:Height="124.6104"><Cell><Data ss:Type="String">707</Data></Cell><Cell><Data ss:Type="String">STRIP: Stream Learning of Influence Probabilities</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Influence-driven diffusion of information is a fundamental process in social networks.
Learning the latent variables of such process, i.e., the influence strength along each link, is a central question towards understanding the structure and function of complex networks, modeling information cascades, and developing applications such as viral marketing.

Motivated by modern microblogging platforms, such as twitter, in this paper we study the problem of learning influence probabilities in a data-stream scenario, in which the network topology is relatively stable and the challenge of a learning algorithm is to keep up with a continuous stream of tweets.
Our contribution is a number of randomized approximation algorithms, categorized according to the available space (superlinear, linear, and sublinear in the number of nodes $n$) and according to different models (landmark and sliding window).
Among several results, we show that we can learn influence probabilities with one pass over the data,
using O(n log n) space, in both the landmark model and the sliding-window model, showing that our algorithm is within a logarithmic factor of optimal.

For truly large graphs, when one needs to operate with sublinear space, we show that we can still learning influence probabilities in one pass, assuming that we restrict our attention to the most active users.

Our thorough experimental evaluation on large social graph demonstrates that the empirical performance of our algorithms agrees with that predicted by the theory.</Data></Cell><Cell><Data ss:Type="String">Konstantin Kutzkov, University of Copenhagen; Albert Bifet, Yahoo! Research; Francesco Bonchi*, Yahoo! Research; Aristides Gionis, Aalto University</Data></Cell><Cell><Data ss:Type="String">Economy, markets\Viral marketing*; Data streams</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">720</Data></Cell><Cell><Data ss:Type="String">Scalable Inference in Max-margin Supervised Topic Models</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Latent topic models have played a pivotal role in analyzing large collections of complex data ranging from text documents to social networks. Besides discovering latent semantic structures, supervised topic models can make predictions on unseen test data. By marrying with advanced machine learning techniques, the predictive strengths of supervised topic models have been dramatically enhanced, such as max-margin supervised topic models, a state-of-the-art supervised topic model that integrates max-margin learning with probabilistic topic models. Though powerful, max-margin supervised topic models have a hard non-smooth learning problem. Existing algorithms rely on solving multiple latent SVM subproblems in an EM-type procedure, which can be too slow to be applicable to large-scale text categorization tasks.

In this paper, we present a highly scalable approach to building max-margin supervised topic models. Our approach builds on three key innovations: 1) {\it a new formulation of  Gibbs max-margin supervised topic models} for both multi-class and multi-label classification; 2) {\it a simple ``augment-and-collapse" Gibbs sampling algorithm} without making restricting assumptions on the posterior distributions; 3) {\it an efficient parallel implementation} that can easily tackle data sets with hundreds of categories and millions of documents. Furthermore, our algorithm does not need to solve SVM subproblems. Though performing the two tasks of topic discovery and learning predictive models jointly, which significantly improves the classification performance, our methods have comparable scalability as the state-of-the-art parallel algorithms for the standard LDA topic models which perform the single task of topic discovery only. Finally, an open-source implementation is also provided.</Data></Cell><Cell><Data ss:Type="String">Jun Zhu*, Tsinghua University; Xun Zheng, Beihang University; Li Zhou, Tsinghua University; Zhang Bo, Tsinghua University</Data></Cell><Cell><Data ss:Type="String">Unsupervised learning\Topic, graphical and latent variable models*; Big Data\Scalable methods; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">Inference; Topic Models; Large-scale Systems; Max-margin Learning</Data></Cell></Row><Row ss:Height="68.652"><Cell><Data ss:Type="String">722</Data></Cell><Cell><Data ss:Type="String">Automatic selection of social media responses to news</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Social media responses to news have increasingly gained in importance as they can enhance a consumer's news reading experience, promote information sharing and aid journalists in assessing their readership's response to a story.
Given that the number of responses to an online news article may be huge, a common challenge is that of selecting only the most interesting responses for display. 
This paper addresses this challenge by casting message selection as an optimization problem. We define an objective function which jointly models the messages' utility scores and their entropy. We propose a near-optimal solution to the underlying optimization problem which  leverages the submodularity property of the objective function. Our solution first learns the utility of individual messages in isolation and then produces a diverse selection of interesting messages by maximizing the defined objective function. 
The intuition behind our work is that an interesting selection of messages contains diverse, informative, opinionated and popular messages referring to the news article, written mostly by users that have authority on the topic. Our intuitions are embodied by a rich set of content, social and user features capturing the aforementioned aspects.
We evaluate our approach through both human and automatic experiments, and demonstrate it outperforms the state of the art. 
Additionally, we perform an in-depth analysis of the annotated ``interesting'' responses, shedding light on the subjectivity around the selection process and the perception of interestingness.</Data></Cell><Cell><Data ss:Type="String">Tadej Štajner*, Jožef Stefan Institute; Bart Thomee, Yahoo! Research; Ana Maria Popescu, Yahoo! Labs; Marco Pennacchiotti, eBay Inc.; Alejandro Jaimes, Yahoo!</Data></Cell><Cell><Data ss:Type="String">Social\Social media*; Mining rich data types\Text; Sampling; Social</Data></Cell><Cell><Data ss:Type="String">Social Media, Microblogging, Information Overload, Information Filtering, Information Reduction, Sampling, Summarization</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">724</Data></Cell><Cell><Data ss:Type="String">A Data-driven Method for In-game Decision Making in MLB</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Professional sports is a roughly $500 billion dollar industry that is increasingly data-driven.  In this paper we show how machine learning can be applied to generate a model that would lead to better on-field decisions by managers of professional baseball teams. Specifically we show how to use regularized linear regression to learn pitcher-specific predictive models that can be used to decide when a starting pitcher should be replaced. A key step in the process is our method of converting categorical variables (e.g., the venue in which a game is played) into continuous variables suitable for the regression. Another key step is dealing with situations in which there is an insufficient amount of data to compute measures such as the effectiveness of a pitcher against specific batters.

For each season we trained on the first 80% of the games, and tested on the rest. The results suggest that using our model would have led to better decisions than those made by major league managers. Applying our model would have led to a different decision 48% of the time.  For those games in which a manager left a pitcher in that our model would have removed, the pitcher ended up performing poorly 60% of the time.</Data></Cell><Cell><Data ss:Type="String">Gartheeban Ganeshapillai*, Massachusetts Institute of Tec; John Guttag, Massachusetts Institute of Technology</Data></Cell><Cell><Data ss:Type="String">Applications*; Causal discovery; Feature selection; Other; Supervised learning; Supervised learning\Regression</Data></Cell><Cell><Data ss:Type="String">Machine Learning Applications, Major League Baseball, Predictive Modeling</Data></Cell></Row><Row ss:Height="12.1032"><Cell><Data ss:Type="String">730</Data></Cell><Cell><Data ss:Type="String">Collaborative Boosting for Activity Classification in Microblogs</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Users' daily activities, such as dining and shopping, inherently reflect their habits, intents and preferences, thus provide invaluable information for services such as personalized information recommendation and targeted advertising. Users' activity information, although ubiquitous on social media, has largely been unexploited. This paper addresses the task of user activity classification in microblogs, where users can publish short messages and maintain social networks online. We identify the importance of modeling a user's individuality, and that of exploiting opinions of the user's friends for accurate activity classification. In this light, we propose a novel collaborative boosting framework comprising a text-to-activity classifier for each user, and a mechanism for collaboration between classifiers of users having social connections. The collaboration between two classifiers includes exchanging their own training instances and their dynamically changing labeling decisions. We propose an iterative learning procedure that is formulated as gradient descent in learning function space, while opinion exchange between classifiers is implemented with a weighted voting in each learning iteration. We show through experiments that on real-world data from Sina Weibo, our method outperforms existing off-the-shelf algorithms that do not take users' individuality or social connections into account.</Data></Cell><Cell><Data ss:Type="String">Yangqiu Song*, HKUST; Zhengdong Lu, Huawei; Cane Wing-Ki Leung, Huawei; Qiang Yang, Hong Kong University of Science and Technology</Data></Cell><Cell><Data ss:Type="String">Transfer learning*; Semi-supervised learning</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Index="90" ss:Height="12.6432"><Cell><Data ss:Type="String">743</Data></Cell><Cell><Data ss:Type="String">Exploiting User Clicks for Automatic Seed Set Generation for Entity Matching</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Matching entities from different information sources is a very important problem in data analysis and data integration. It is however challenging due to the number and diversity of information sources involved, and the significant editorial efforts required to collect large-scale training data. In this paper, we show how user click behavior on Web search can be leveraged for automatically generating training data for entity matching. The basic intuition is that Web pages clicked for a given query are likely to be about the same entity. We use random walk with restart to reduce data sparseness, rely on co-clustering to group queries and Web pages, and exploit page similarity to improve matching precision. Experimental results show that: (i) With 360K pages from 6 major travel websites, we obtain 84K matchings (of 179K pages) that refer to the same entities, with an average precision of 0.826; (ii) The quality of matching obtained from a classifier trained on the resulted seed data is promising: the performance matches that of editorial data at small size and improves with size.</Data></Cell><Cell><Data ss:Type="String">Xiao Bai*, Yahoo! Research Barcelona; Srinivasan Sengamedu, Komli Labs; Flavio Junqueira, Microsoft Research</Data></Cell><Cell><Data ss:Type="String">Web mining*; Big Data\Scalable methods; Unsupervised learning\Clustering</Data></Cell><Cell><Data ss:Type="String">Entity matching, unsupervised, automatic seed generation, user clicks mining, Web search logs, random walk, co-clustering</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">748</Data></Cell><Cell><Data ss:Type="String">Silence is also evidence: Interpreting dwell time for recommendation from Psychological Perspective</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String"> Social media is a platform for people to share and vote content. From the analysis of the social media data we found that users are quite inactive in rating/voting. For example, a user on average only votes 2 out of 100 accessed items. Traditional recommendation methods are mostly based on users' votes and thus can not cope with this situation. Based on the observation that the dwell time on an item may reflect the opinion of a user, we aim to enrich the user-vote matrix by converting the dwell time on items into users' ``pseudo votes'' and then help improve recommendation performance. However, it is challenging to correctly interpret the dwell time since many subjective human factors, e.g. user expectation, sensitivity to various item qualities, reading speed, are involved into the casual behavior of online reading. In psychology, it is assumed that people have choice threshold in decision making. The time spent on making decision reflects the decision maker's threshold. This idea inspires us to develop a View-Voting model, which can estimate how much the user likes the viewed item according to her dwell time, and thus make recommendations even if there is no voting data available. Finally, our experimental evaluation shows that the traditional rate-based recommendation's performance is greatly improved with the support of VV model.</Data></Cell><Cell><Data ss:Type="String">Peifeng Yin*, Pennsylvania State University; Ping Luo, HP Lab; Wang-Chien Lee, ; Min Wang, Google Research</Data></Cell><Cell><Data ss:Type="String">User modeling*; Applications\Mobile; Sentiment and opinion mining</Data></Cell><Cell><Data ss:Type="String">dwell time, recommendation</Data></Cell></Row><Row ss:Height="68.652"><Cell><Data ss:Type="String">752</Data></Cell><Cell><Data ss:Type="String">Trace Complexity of Network Inference</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The network inference problem consists of reconstructing the edge set of a network given traces representing the chronology of infection times as epidemics spread through the network. This problem is a paradigmatic representative of prediction tasks in machine learning that require deducing a latent structure from observed patterns of activity in a network, which often require an unrealistically large number of resources (e.g., amount of available data, or computational time). A fundamental question is to understand which properties we can predict with a reasonable degree of accuracy with the available resources, and which we cannot.
We define the trace complexity as the number of distinct traces required to achieve high fidelity in reconstructing the topology of the hidden network or, more generally, some of its properties. We give simpler and more efficient algorithms for the objective of perfect reconstruction with high probability.
Moreover, we prove that our algorithms are nearly optimal, by proving an information-theoretic lower bound on the
number of traces that an optimal reconstruction algorithm requires for performing this task in the worst case.
Given these strong lower bounds, we turn our attention to special cases, such as trees and bounded-degree graphs, and to property recovery tasks, such as reconstructing the degree distribution of an unobserved network.
We show that these problems require a much smaller (and more realistic) number of traces, making them potentially solvable in practice.</Data></Cell><Cell><Data ss:Type="String">Bruno Abrahao*, Cornell; Flavio Chierichetti, Sapienza University; Robert Kleinberg, Cornell; Alessandro Panconesi, Sapienza University of Rome</Data></Cell><Cell><Data ss:Type="String">Graph mining*; Probabilistic methods; Sampling; Social\Social and information networks</Data></Cell><Cell><Data ss:Type="String">network inference; information cascades</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">761</Data></Cell><Cell><Data ss:Type="String">Efficient Single-Source Shortest Path and Distance Queries on Large Graphs</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">This paper investigates two types of graph queries: single source distance (SSD) queries and single source shortest path (SSSP) queries. Given a node v in a graph G, an SSD query from v asks for the distance from v to any other node in G, while an SSSP query retrieves the shortest path from v to any other node. These two types of queries are fundamental building blocks for numerous graph algorithms, and they find important applications in graph analysis, especially in the computation of graph measures. Most of the existing solutions for SSD and SSSP queries, however, require that the input graph fits in the main memory, which renders them inapplicable for the massive disk-resident graphs commonly used in web and social applications. There are several techniques that are designed to be I/O efficient, but they all focus on undirected and/or unweighted graphs, and they provide rather sub-optimal query efficiency.

To address the deficiency of existing work, this paper presents Highways-on-Disk (HoD), a disk-based index that supports both SSD and SSSP queries on directed and weighted graphs. The key idea of HoD is to augment the input graph with a set of auxiliary edges, and exploit them during query processing to reduce I/O and computation costs. We experimentally evaluate HoD on a variety of real-world graphs with up to billions of nodes and edges, and we demonstrate that HoD significantly outperforms alternative solutions in terms of query efficiency.</Data></Cell><Cell><Data ss:Type="String">Xiaokui Xiao*, Nanyang Technological University ; Andy Diwen Zhu, Nanyang Technological University; Sibo Wang, Nanyang Technological University; Wenqing Lin, Nanyang Technological University</Data></Cell><Cell><Data ss:Type="String">Big Data\Scalable methods*</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="57.456"><Cell><Data ss:Type="String">765</Data></Cell><Cell><Data ss:Type="String">On Community Detection in Real-World Networks and the Importance of Degree Assortativity</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Graph clustering, often addressed as  community detection, is a prominent task in the domain of graph data mining with dozens of
algorithms proposed in recent years. In this paper, we focus on several popular community detection algorithms with low computational complexity and
with decent performance on the artificial benchmarks, and we study their behaviour on real-world networks. Motivated by the observation that there is a class of networks for which the community detection methods fail do deliver good community structure, we examine the assortativity coefficient of ground-truth communities and show that
assortativity of a community structure can be very different from the assortativity of the original network. 
We then examine the possibility of exploiting the latter by weighting edges of a network with the aim to improve the community detection outputs for networks with assortative community structure. The evaluation shows that the proposed weighting can significantly improve the results of community detection methods on networks with assortative community structure.</Data></Cell><Cell><Data ss:Type="String">Marek Ciglan*, IISAS; Kjetil Nørvåg, NTNU; Michal Laclavík, IISAS</Data></Cell><Cell><Data ss:Type="String">Social\Community detection*; Graph mining; Unsupervised learning\Clustering</Data></Cell><Cell><Data ss:Type="String">graph clustering, community detection, degree assortativity, assortative communities</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">770</Data></Cell><Cell><Data ss:Type="String">Robust Sparse Estimation of Multiresponse Regression and Inverse Covariance Matrix via the L2 distance</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">We propose a robust framework to jointly perform two key modeling tasks involving high dimensional data: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among responses while adjusting for their predictors. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecifica- tion. This issue is exacerbated when dealing with high dimensional noisy data. In this work, we propose instead to minimize a regularized distance criterion, which is motivated by the minimum distance functionals used in nonparametric methods for their excellent robustness properties. The proposed estimates can be obtained efficiently by leveraging a sequential quadratic programming algorithm. We provide theoretical justification such as estimation consistency for the proposed estimator. Additionally, we shed light on the robustness of our estimator through its linearization, which yields a combination of weighted lasso and graphical lasso with the sample weights providing an intuitive explanation of the robustness. We demonstrate the merits of our framework through simulation study and the analysis of real financial and genetics data.</Data></Cell><Cell><Data ss:Type="String">Aurelie Lozano*, IBM Research; Huijing Jiang, IBM Research; Xinwei Deng, Virginia Tech</Data></Cell><Cell><Data ss:Type="String">Supervised learning*; Bioinformatics; Feature selection; Supervised learning\Regression</Data></Cell><Cell><Data ss:Type="String">Robust estimation, sparse learning, variable selection, high dimensional data, multiresponse regression, inverse covariance, L2E</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">773</Data></Cell><Cell><Data ss:Type="String">Comparing Apples to Oranges: A Scalable Solution with Heterogeneous Hashing</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Although hashing techniques have been popular for the large scale similarity search problem, most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment, i.e., the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world applications, there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogenous domains, e.g., recommending relevant posts and images to a certain Facebook user. In this paper, we address the problem of “comparing apples to oranges” under the large scale setting. Specifically, we propose a novel Relation-aware Heterogeneous Hashing(RaHH), which provides a general framework for generating hash codes of data entities sitting in multiple heterogenous domains. Unlike some existing hashing methods that map heterogeneous data in a common Hamming space, the RaHH approach constructs a Hamming space for each type of data entities, and learns optimal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover, the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with improved accuracy. To validate the proposed RaHH method, we conduct extensive evaluations on two large datasets crawled from the popular social media sites, i.e., Flickr and Tencent Weibo. The experimental results clearly demonstrate that the RaHH outperforms several state-of-the-art hashing methods with significant performance gains.</Data></Cell><Cell><Data ss:Type="String">Mingdong Ou, Tsinghua University; Peng Cui*, Tsinghua University; Fei Wang, IBM T. J. Watson Research Lab; Jun Wang, IBM Research</Data></Cell><Cell><Data ss:Type="String">Big Data\Scalable methods*; Recommender systems; Social</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="113.4144"><Cell><Data ss:Type="String">795</Data></Cell><Cell><Data ss:Type="String">Trial and Error in Influential Social Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">In this paper, we introduce a trial-and-error model to study information diffusion in a social network.
Specifically, in every discrete period, all individuals in the network concurrently try a new technology or product with certain respective probabilities.
If it turns out that an individual observes a better utility, he will then adopt the trial;
otherwise, the individual continues to choose his prior selection.

We first demonstrate that the trial and error behavior of individuals characterizes certain global community structures of a social network, from which we are able to detect macro-communities through the observation of micro-behavior of individuals. We run simulations on classic benchmark testing graphs, and quite surprisingly, the results show that the trial and error dynamics even outperforms the Louvain method (a popular modularity maximization approach) if individuals have dense connections within communities. This gives a solid justification of the model.

We then study the influence maximization problem in the trial-and-error dynamics. We give a heuristic algorithm based on community detection and provide experiments on both testing and large scale collaboration networks.
Simulation results show that our algorithm significantly outperforms several well-studied heuristics including degree centrality and distance centrality in almost all of the scenarios.
Our results reveal the relation between the budget that an advertiser invests and marketing strategies, and indicate that the mixing parameter, a benchmark evaluating network community structures, plays a critical role for information diffusion.</Data></Cell><Cell><Data ss:Type="String">Xiaohui Bei, Nanyang Technological University; Ning Chen*, Nanyang Technological Univ; Liyu Dou, Nanyang Technological University; Xiangru Huang, Shanghai Jiao Tong University; Ruixin Qiang, Shanghai Jiao Tong University</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Economy, markets\Viral marketing; Social\Community detection</Data></Cell><Cell><Data ss:Type="String">social networks, trial and error, community detection, influence maximization</Data></Cell></Row><Row ss:Index="98" ss:Height="12.6432"><Cell><Data ss:Type="String">797</Data></Cell><Cell><Data ss:Type="String">Collaborative Matrix Factorization with Multiple Similarities for Predictin Drug-Target Interactions</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">We address the problem of predicting new drug-target interactions from three inputs: known interactions, similarities over drugs and those over targets. This setting has been considered by many methods, which however have a common problem of allowing to have only one similarity matrix over drugs and that over targets. The key idea of our approach is to use more than one similarity matrices over drugs as well as those over targets, where weights over the multiple similarity matrices are estimated from data to automatically select similarities, which are effective for improving the performance of predicting drug-target interactions. We propose a factor model, named Multiple Similarities Collaborative Matrix Factorization~(MSCMF), which projects drugs and targets into a common low-rank feature space, which is further consistent with weighted similarity matrices over drugs and those over targets. These two low-rank matrices and weights over similarity matrices are estimated by an alternating least squares algorithm. Our approach allows to predict drug-target interactions by the two low-rank matrices collaboratively and to detect similarities which are important for predicting drug-target interactions. This approach is general and applicable to any binary relations with similarities over elements, being found in many applications, such as recommender systems. In fact, MSCMF is an extension of weighted low-rank approximation for one-class collaborative filtering. We extensively evaluated the performance of MSCMF by using both synthetic and real datasets. Experimental results showed nice properties of MSCMF on selecting similarities useful in improving the predictive performance and the performance advantage of MSCMF over six state-of-the-art methods for predicting drug-target interactions.</Data></Cell><Cell><Data ss:Type="String">Xiaodong Zheng, Fudan University; Hao Ding, Fudan University; Hiroshi Mamitsuka, Kyoto University; Shanfeng Zhu*, Fudan University</Data></Cell><Cell><Data ss:Type="String">Bioinformatics*; Dimensionality reduction; Unsupervised learning\Matrix/tensor factorization</Data></Cell><Cell><Data ss:Type="String">Drug-target interaction,Weighted low-rank approximation</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">801</Data></Cell><Cell><Data ss:Type="String">FeaFiner: Biomarker Identification from Medical Data through Feature Generalization and Selection</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Traditionally, feature construction and feature selection are two important but separate processes in data mining. However, many real world applications require an integrated approach for creating, refining and selecting features. To address this problem, we propose FeaFiner (short for Feature Refiner), an efficient formulation that simultaneously generalizes low-level features into higher level concepts and then selects relevant concepts based on the target variable. Specifically, we formulate a double sparsity optimization problem that identifies groups in the low-level features, generalizes higher level features using the groups and performs feature selection. Since in many clinical researches nonoverlapping groups are preferred for better interpretability, we further improve the formulation to generalize features using mutually exclusive feature groups. The proposed formulation is challenging to solve due to the orthogonality constraints, non-convexity objective and non-smoothness penalties. We apply a recently developed augmented Lagrangian method to solve this formulation in which each subproblem is solved by a non-monotone spectral projected gradient method. Our numerical experiments show that this approach is computationally efficient and also capable of producing solutions of high quality. We also present a generalization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation. Finally, the proposed FeaFiner method is validated on Alzheimer's Disease Neuroimaging Initiative dataset, where low-level biomarkers are automatically generalized into robust higher level concepts which are then selected for predicting the disease status measured by Mini Mental State Examination and Alzheimer's Disease Assessment Scale cognitive subscore. Compared to existing predictive modeling methods, FeaFiner provides intuitive and robust feature concepts and competitive predictive accuracy.</Data></Cell><Cell><Data ss:Type="String">Jiayu Zhou*, Arizona State University; Zhaosong Lu, Simon Fraser University; Jimeng Sun, IBM Research ; Lei Yuan, Arizona State University; Fei Wang, IBM T. J. Watson Research Lab; Jieping Ye, Arizona State University</Data></Cell><Cell><Data ss:Type="String">Applications\Healthcare and medicine*; Supervised learning</Data></Cell><Cell><Data ss:Type="String">Feature generalization, feature selection, sparse learning, augmented Lagrangian, spectral gradient descent, biomarkers</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">811</Data></Cell><Cell><Data ss:Type="String">Text-Based Measures of Document Diversity</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Quantitative notions of diversity are central to scientific disciplines ranging from conservation biology to the reflexive study of science. However, there has been relatively little work on measuring the diversity of text documents via their content. In this paper we present a text-based framework for quantifying the diversity of documents. We learn a topic model over a corpus of documents, and compute a distance matrix between pairs of topics based on measures such as topic co-occurrence. We then use these pairwise distance measures, combined with the distribution of topics in each document, to estimate each document's diversity relative to the rest of the corpus. Our approach provides several advantages over existing methods.  It is fully data-driven, requiring only the text from a corpus of documents as input, it produces human-readable explanations, and it can be generalized to score diversity of other entities such as authors, academic departments, or journals.  We describe experimental results on several large data sets which suggest that our approach does accurately capture document diversity.</Data></Cell><Cell><Data ss:Type="String">Kevin Bache*, University of California, Irvine; Padhraic Smyth, UC Irvine; David Newman, University of California, Irvine</Data></Cell><Cell><Data ss:Type="String">Unsupervised learning\Exploratory analysis*; Mining rich data types\Text; Probabilistic methods; Semi-supervised learning\Anomaly/novelty detection; Unsupervised learning; Unsupervised learning\Topic, graphical and latent variable models</Data></Cell><Cell><Data ss:Type="String">Diversity, Text Mining, Interdisciplinarity</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">854</Data></Cell><Cell><Data ss:Type="String">Learning Geographical Preferences for Point-of-Interest Recommendation</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The problem of point of interest (POI) recommendation is to provide personalized recommendations of places of interests, such as restaurants, for mobile users. Due to its complexity and its connection to location based social networks (LBSNs), the decision process of a user choose a POI is complex and can be influenced by various factors, such as user preferences, geographical influences, and user mobility behaviors. While there are some studies on POI recommendations, it lacks of integrated analysis of the joint effect of multiple factors. To this end, in this paper, we propose a novel geographical probabilistic factor analysis framework which strategically takes various factors into consideration. Specifically, this framework allows to capture the geographical influences on a user's check-in behavior. Also, the user mobility behaviors can be effectively exploited in the recommendation model. Moreover, the recommendation model can effectively make use of user check-in count data as implicity user feedback for modeling user preferences. Finally, experimental results on real-world LBSNs data show that the proposed recommendation method outperforms state-of-the-art latent factor models with a significant margin.</Data></Cell><Cell><Data ss:Type="String">Bin Liu*, Rutgers Univ; Yanjie Fu, Rutgers University; ZIjun Yao, Rutgers Univ; Hui Xiong, Rutgers, the State University of New Jersey</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Spatial*; Recommender systems</Data></Cell><Cell><Data ss:Type="String">POI recommendation, LBSNs</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">863</Data></Cell><Cell><Data ss:Type="String">SVM_{pAUC}^{tight}: A New Support Vector Method for Optimizing Partial AUC Based on a Tight Convex Upper Bound</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The area under the ROC curve (AUC) is a well known performance measure in machine learning and data mining. In an increasing number of applications, however, ranging from ranking applications to a variety of important bioinformatics applications, performance is measured in terms of the partial area under the ROC curve between two specified false positive rates. In recent work, we proposed a structural SVM based approach for optimizing this performance measure (Narasimhan and Agarwal, 2013). In this paper, we develop a new support vector method, SVM_{pAUC}^{tight}, that optimizes a tighter convex upper bound on the partial AUC loss, which leads to both improved accuracy and reduced computational complexity. In particular, by rewriting the empirical partial AUC risk as a maximum over subsets of negative instances, we derive a new formulation, where a modified form of the earlier optimization objective is evaluated on each of these subsets, leading to a tighter hinge relaxation on the partial AUC loss. As with our previous method, the resulting optimization problem can be solved using a cutting-plane algorithm, but the new method has better run time guarantees. We also discuss a projected subgradient method for solving this problem, which offers additional computational savings in certain settings. We demonstrate on a wide variety of bioinformatics tasks, ranging from protein-protein interaction prediction to drug discovery tasks, that the proposed method does, in many cases, perform significantly better on the partial AUC measure than the previous structural SVM approach. In addition, we also develop extensions of our method to learn sparse and group sparse models, often of interest in biological applications.</Data></Cell><Cell><Data ss:Type="String">Harikrishna Narasimhan*, Indian Institute of Science; Shivani Agarwal, Indian Institute of Science, Bangalore</Data></Cell><Cell><Data ss:Type="String">Supervised learning*; Bioinformatics; Supervised learning\Learning to rank; Supervised learning\Support vector machines</Data></Cell><Cell><Data ss:Type="String">Partial AUC, SVM, Cutting-plane Method, ROC Curve</Data></Cell></Row><Row ss:Height="23.8392"><Cell><Data ss:Type="String">884</Data></Cell><Cell><Data ss:Type="String">Learning Mixed Kronecker Product Graph Models with Simulated Method of Moments</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">There has recently been a great deal of work focused on developing statistical models of graph structure—with the goal of modeling probability distributions over graphs from which new, similar graphs can be generated by sampling from the estimated distributions. Although current graph models can capture several important characteristics of social network graphs (e.g., degree, path lengths), many of them do not generate graphs with sufficient variation to reflect the natural variability in real world graph domains. One exception is the Mixed Kronecker Product Graph Model (mKPGM), a generalization of the Kronecker Product Graph Model, which uses parameter tying to capture variance in the underlying distribution. The enhanced representation of mKPGMs enables them to match both the mean graph statistics and their spread as observed in real network populations, but unfortunately to date, the only method to estimate mKPGMs involves an exhaustive search over the parameters.
In this work, we present the first learning algorithm for mKPGMs. The O(|E|) algorithm searches over the continuous parameter space using constrained line search and is based on simulated method of moments, where the objective function minimizes the distance between the observed moments in the training graph and the empirically estimated moments of the model. We evaluate the mKPGM learning algorithm by comparing to several different graph models, including KPGMs. We use multidimensional KS distance to compare the generated graphs to the observed graphs and the results show mKPGMs are able to produce a closer match to real-world graphs (10-90% reduction in KS distance), while still providing natural variation in the generated graphs.</Data></Cell><Cell><Data ss:Type="String">Sebastian Moreno*, Purdue University; Jennifer Neville, Purdue University; Sergey Kirshner, Purdue University</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Recommender systems\Evaluation and metrics; Social; Supervised learning</Data></Cell><Cell><Data ss:Type="String">Network analysis, statistical graph models, Kronecker models, method of moments</Data></Cell></Row><Row ss:Height="57.456"><Cell><Data ss:Type="String">897</Data></Cell><Cell><Data ss:Type="String">Subsampling for Efficient and Effective Unsupervised Outlier Detection Ensembles</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Outlier detection and ensemble learning are well established research directions in data mining yet the application of ensemble techniques to outlier detection has been rarely studied.
Here, we propose and study subsampling as a technique to induce diversity among individual outlier detectors.
We show analytically and experimentally that an outlier detector based on a subsample per se, besides inducing diversity, can, under certain conditions, already improve upon the results of the same outlier detector on the complete dataset.
Building an ensemble on top of several subsamples is further improving the results. While in the literature so far the intuition that ensembles improve over single outlier detectors has just been transferred from the classification literature, here we also justify analytically why ensembles are also expected to work in the unsupervised area of outlier detection. 
As a side effect, running an ensemble of several outlier detectors on subsamples of the dataset is more efficient than ensembles based on other means of introducing diversity and, depending on the sample rate and the size of the ensemble, can be even more efficient than just the single outlier detector on the complete data.  </Data></Cell><Cell><Data ss:Type="String">Arthur Zimek*, University of Alberta; Matthew Gaudet, University of Alberta; Ricardo J. G. Campello, University of Alberta; Jörg Sander, University of Alberta</Data></Cell><Cell><Data ss:Type="String">Semi-supervised learning\Anomaly/novelty detection*</Data></Cell><Cell><Data ss:Type="String">ensemble outlier detection</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">914</Data></Cell><Cell><Data ss:Type="String">Big Data Analytics with Small Footprint: Squaring the Cloud</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost and power. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported *cluster* implementations for common benchmark problems. A key design criterion is *rapid exploration* of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computational kernels (iii), BIDMach, a machine learning system that includes efficient model optimizers, (iv) Butterfly mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative update algorithms. We present several benchmark problems as case studies to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem.</Data></Cell><Cell><Data ss:Type="String">John Canny*, UC Berkeley; Huasha Zhao, UC Berkeley</Data></Cell><Cell><Data ss:Type="String">Big Data*; Dimensionality reduction; Probabilistic methods; Supervised learning; Unsupervised learning</Data></Cell><Cell><Data ss:Type="String">Matrix Toolkit, Machine Learning Toolkit, Stochastic Gradient Descent, Factor Models, Regression, Matrix Completion, Distributed Inference, Benchmarks.</Data></Cell></Row><Row ss:Index="106" ss:Height="68.652"><Cell><Data ss:Type="String">921</Data></Cell><Cell><Data ss:Type="String">A space efficient streaming algorithm for triangle counting using the birthday paradox</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">We study the problem of counting the number of triangles in a massive streamed graph. Modern massive graphs are truly \emph{dynamic graphs} and can be accurately characterized as a \emph{stream of edges}. How does one store a small sample of the past and still answer meaningful questions about the entire graph? This is the aim of a small space streaming algorithm. 

We design a space efficient algorithm that approximates the transitivity (global clustering coefficient) with only a single pass through a graph of $n$ vertices. Our procedure is based on the classic probabilistic result, \emph{the birthday paradox}. Under conditions commonly held by social networks, we can prove that our algorithm requires $O(\sqrt{n})$ space to provide accurate estimates for the transitivity and number of triangles in the graph.

We run a detailed set of experiments on a variety of real graphs and demonstrate that the storage of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 50,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains \emph{a real-time estimate} of the transitivity/number of triangles of a graph, by storing a miniscule fraction of edges.
</Data></Cell><Cell><Data ss:Type="String">Madhav Jha, Pennsylvania State University; C. Seshadhri*, Sandia National Labs; Ali Pinar, Sandia National Labs</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Big Data\Novel statistical techniques for big data; Data streams; Graph mining; Sampling</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="23.8392"><Cell><Data ss:Type="String">928</Data></Cell><Cell><Data ss:Type="String">Measuring spontaneous devaluations in user preferences</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Spontaneous devaluation in preferences is ubiquitous, where yesterday's hit is today's affliction. Devaluation in preferences or boredom can be temporary such that one's perceived liking for objects are temporarily reordered to devalue familiarity or can be permanent causing a complete shift in one's interests. One of the psychological theories explaining spontaneous devaluation relates to stimulus satiation arising on repeated exposure causing disinterest in the activity. Despite technological advances facilitating access to a wide range of commodities, finding engaging content is a major enterprise. Systems tracking spontaneous devaluation in user preferences can allow prediction of the onset of boredom in users potentially catering to their changed needs.
In this work, we study the music listening histories of Last.fm users focusing on the changes in their preferences based on their choices for different artists at different points in time. A hazard function, commonly used in statistics for survival analysis, is used to capture the rate at which a user returns to an artist as a function of exposure to the artist. The analysis provides the first evidence of spontaneous devaluation in preferences of music listeners. Better understanding of the temporal dynamics of this phenomenon can inform solutions to similarity-dissimilarity dilemma in recommender systems. </Data></Cell><Cell><Data ss:Type="String">Komal Kapoor*, University of Minnesota Twin C; Nisheeth Srivastava, ; Jaideep Srivastava, University of Minnesota; Paul Schrater, University of Minnesota Twin Cities</Data></Cell><Cell><Data ss:Type="String">User modeling*; Recommender systems</Data></Cell><Cell><Data ss:Type="String">User Preferences, Dynamic user preferences, User Choice Modeling, Recommendations, Dynamics of boredom</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">963</Data></Cell><Cell><Data ss:Type="String">Mining Evidences for Named Entity Disambiguation</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Named entity disambiguation is the task of disambiguating named entity mentions in natural language text to their corresponding entries in a knowledge base such as Wikipedia. Such disambiguation can help enhance readability and add semantics to plain text.  It is also a central step in constructing high-quality information network or knowledge graph from unstructured text.  Previous research has tackled this problem by making use of various textual and structural features from a knowledge base.  Most of the proposed algorithms assume that a knowledge base can provide enough explicit and useful information to help disambiguate a mention to the right entity.  However, the existing knowledge bases are rarely complete (likely will never be), thus leading to poor performance on short queries with not well-known contexts.  In such cases, we need to collect additional evidences scattered in internal and external corpus to augment the knowledge bases and enhance their disambiguation power.  In this work, we propose a generative model and an incremental algorithm to automatically mine useful evidences across documents.  With a specific modeling of "background topic" and "unknown entities", our model is able to harvest useful evidences without introducing noisy information. Experimental results show that our proposed method outperforms the state-of-the-art approaches significantly: boosting the disambiguation accuracy from 43% (baseline) to 86% on short queries derived from tweets.</Data></Cell><Cell><Data ss:Type="String">Yang Li*, University of California Santa Barbara; Chi Wang, University of Illinois; Fangqiu Han, University of California Santa Barbara; Jiawei Han, University of Illinois at Urbana-Champaign; Dan Roth, UIUC; Xifeng Yan, University of California at Santa Barbara</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Text*; Information extraction; Probabilistic methods; Unsupervised learning\Topic, graphical and latent variable models</Data></Cell><Cell><Data ss:Type="String">Entity Disambiguation, Knowledge Base, Generative Model, Evidence Mining</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">980</Data></Cell><Cell><Data ss:Type="String">One Theme in All Views: Modeling Consensus Topics in Multiple Contexts</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">New challenges have been presented to classical topic models when applied to social media, as user-generated content suffers from significant problems of data sparseness. A variety of heuristic adjustments to these models have been proposed, with many based on the use of context information to improve the performance of topic modeling. Existing contextualized topic models rely on arbitrary manipulation of the model structure, by incorporating various context variables into the generative process of classical topic models in an ad hoc manner. Such manipulations usually result in much more sophisticated model structures and inference procedures, and substantial difficulty to generalize any of them to accommodate arbitrary types or combinations of contexts. In this paper we explore a different direction. We propose a general solution that is able to exploit multiple types of contexts without arbitrary manipulation of the structure of classical topic models. We formulate different types of contexts as multiple views of the partition of the corpus. A co-regularization framework is proposed to let the views collaborate with each other, vote for the consensus topics, and distinguish them from view-specific topics. Experiments with real world datasets prove that the proposed method is both effective and flexible to utilize arbitrary types of contexts.</Data></Cell><Cell><Data ss:Type="String">Jian Tang*, Peking University; Ming Zhang, ; Qiaozhu Mei, University of Michigan</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Text*; Probabilistic methods; Unsupervised learning\Clustering; Unsupervised learning\Topic, graphical and latent variable models; Web mining</Data></Cell><Cell><Data ss:Type="String">topic modeling, co-regularization, user-generated content</Data></Cell></Row><Row ss:Height="57.456"><Cell><Data ss:Type="String">1006</Data></Cell><Cell><Data ss:Type="String">Information Cascade at Group Scale</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Identifying the k most influential individuals in a social network is a well-studied problem. The objective is to detect k individuals in a (social) network who will influence the maximum number of people, if they are independently convinced of adopting a new strategy (product, idea, etc). There are cases  in real life, however, where we aim to instigate groups instead of individuals to trigger network diffusion. Such cases abound, e.g.,  billboards, TV commercials and  newspaper ads are utilized extensively
to boost the popularity and raise awareness.

In this paper, we generalize the ``influential nodes'' problem. Namely we are interested to locate the most ``influential groups'' in a network. As the first paper to address this problem: we (1) propose a fine-grained model of information diffusion for the group-based problem, (2) show that the process is submodular and present an algorithm to determine the influential groups under this model (with a precise approximation bound), (3) propose a coarse-grained model that inspects the network at group level (not individuals) significantly speeding up calculations for large networks, (4) show that the diffusion function we design here is submodular in general case, and propose an approximation algorithm for this coarse-grained model,
and finally by conducting experiments on real datasets, (5) demonstrate that seeding members of selected groups to be the first adopters can broaden diffusion (when compared to the influential individuals case). Moreover, we can identify these influential groups much faster (up to 12 million times speedup), delivering a practical solution to this problem.</Data></Cell><Cell><Data ss:Type="String">Milad Eftekhar*, University of Toronto; Yashar Ganjali, University of Toronto; Nick Koudas, University of Toronto and Sysomos Inc</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Economy, markets\Viral marketing; Social</Data></Cell><Cell><Data ss:Type="String">Social networks, Information cascade</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">1024</Data></Cell><Cell><Data ss:Type="String">Debiasing Social Wisdom</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">With the explosive growth of social networks, many applications are increasingly harnessing the pulse of online crowds for a variety of tasks such as marketing, advertising, and opinion mining. An important example is the wisdom of crowd effect that has been well studied for such tasks when the crowd is non-interacting. However, these studies don't explicitly address the network effects in social networks. A key difference in this setting is the presence of social influences that arise from these interactions and undermine the wisdom of the crowd~\cite{LRS+11}.  

Using a natural model of opinion formation, we analyze the effect of these interactions on an individual's opinion and estimate her propensity to conform. We then propose efficient sampling algorithms incorporating these conformity values to arrive at a debiased estimate of the wisdom of a crowd. We analyze the trade-off between the sample size and estimation error and validate our algorithms using both real data obtained from online user experiments and synthetic data.</Data></Cell><Cell><Data ss:Type="String">Abhimanyu Das, Microsoft; Sreenivas Gollapudi*, Microsoft Research; Rina Panigrahy, Microsoft Research; Mahyar Salek, Microsoft</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Graph mining; Sampling</Data></Cell><Cell><Data ss:Type="String">social influence, wisdom of crowd, social sampling, opinion formation, debiasing</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">1041</Data></Cell><Cell><Data ss:Type="String">Estimating Unbiased Sharer Reputation via Social Data Calibration</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Online social networks have become important channels for users to share content with their connections and diffuse information. Although much work has been done to identify socially influential users, the problem of finding ``reputable'' sharers, {\em who share good content}, has received relatively little attention.  Availability of such reputation scores can be useful for various applications like recommending people to follow, procuring high quality content in a scalable way, creating a content reputation economy to incentivize high quality sharing, and many more.
To estimate sharer reputation, it is intuitive to leverage data that records how recipients respond (through clicking, liking, etc.) to content items shared by a sharer.  However, such data is usually biased --- it has a {\em selection bias} since the shared items can only be seen and responded to by users connected to the sharer in most social networks, and it has a {\em response bias} since the response is usually influenced by the relationship between the sharer and the recipient (which may not indicate whether the shared content is good).  To correct for such biases, we propose to utilize an additional data source that provides {\em unbiased} goodness estimates for a small set of shared items, and calibrate biased social data through a novel multi-level hierarchical model that describes how the unbiased data and biased data are jointly generated according to sharer reputation scores. The unbiased data also provides the ground truth for quantitative evaluation of different methods.  Experiments based on such ground-truth data show that our proposed model significantly outperforms existing methods that estimate social influence using biased social data.
</Data></Cell><Cell><Data ss:Type="String">Jaewon Yang, Stanford University; Bee-Chung Chen*, LinkedIn; Deepak Agarwal, LinkedIn</Data></Cell><Cell><Data ss:Type="String">User modeling*; Social\Social media</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">1045</Data></Cell><Cell><Data ss:Type="String">Linking Named Entities in Tweets with Knowledge Base via User Interest Modeling</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Twitter has become an increasingly important source of information, with more than 400 million tweets posted per day. The task to link the named entity mentions detected from tweets with the corresponding real world entities in the knowledge base is called tweet entity linking. This task is of practical importance and can facilitate many different tasks, such as personalized recommendation and user interest discovery. The tweet entity linking task is challenging due to the noisy, short, and informal nature of tweets. Previous methods focus on linking entities in Web documents, and largely rely on the context around the entity mention and the topical coherence between entities in the document. However, these methods cannot be effectively applied to the tweet entity linking task due to the insufficient context information contained in a tweet. In this paper, we propose KAURI, a novel graph-based unified framework to collectively link all the named entity mentions in all tweets posted by a user via modeling the user's topics of interest. Our assumption is that each user has an underlying topic interest distribution over various named entities. To model this information, we propose a graph-based user interest propagation algorithm which integrates the global user interest information across tweets with the intra-tweet local information. We extensively evaluated the performance of KAURI over manually annotated tweet corpus, and the experimental results show that KAURI significantly outperforms the baseline methods in terms of accuracy, and KAURI is efficient and scales well to tweet stream.</Data></Cell><Cell><Data ss:Type="String">Wei Shen*, Tsinghua University; Jianyong Wang, Tsinghua University; Ping Luo, HP Lab; Min Wang, Google Research</Data></Cell><Cell><Data ss:Type="String">Mining rich data types\Unstructured*; Social\Social media; User modeling</Data></Cell><Cell ss:Index="7"/></Row><Row ss:Index="114" ss:Height="258.948"><Cell><Data ss:Type="String">1052</Data></Cell><Cell><Data ss:Type="String">Privacy-Preserving Data Exploration in Genome-Wide Association Studies</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Genome-wide association studies (GWAS) have become a popular method
for analyzing sets of DNA sequences in order to discover the genetic
basis of disease.  Unfortunately, statistics published as the result of
GWAS can be used to identify individuals participating in the study.
To prevent privacy breaches, even previously published results have
been removed from public databases, impeding researchers' access to
the data and hindering collaborative research.  Existing techniques for
privacy-preserving GWAS focus on answering specific questions, such as
correlations between a given pair of SNPs (DNA sequence variations).
This does not fit the typical GWAS process, where the analyst may not
know in advance which SNPs to consider and which statistical tests to use,
how many SNPs are significant for a given dataset, etc.

We present a set of practical, privacy-preserving data mining algorithms
for GWAS datasets.  Our framework supports \emph{exploratory} data
analysis, where the analyst does not know a priori how many and which SNPs
to consider.  We develop privacy-preserving algorithms for computing the
number and location of SNPs that are significantly associated with the
disease, the significance of any statistical test between a given SNP
and the disease, any measure of correlation between SNPs and the block
structure of correlations.  We evaluate our algorithms on real-world
datasets and demonstrate that they produce significantly more accurate
results than prior techniques while guaranteeing differential privacy.</Data></Cell><Cell><Data ss:Type="String">Aaron Johnson*, U.S. Naval Research Laboratory; Vitaly Shmatikov, The University of Texas at Austin</Data></Cell><Cell><Data ss:Type="String">Security and privacy*; Adaptive learning\Adaptive experimentation; Applications\Healthcare and medicine; Security and privacy\Anonymization</Data></Cell><Cell><Data ss:Type="String">genetic privacy, differential privacy, genome-wide association study, data mining</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">1076</Data></Cell><Cell><Data ss:Type="String">Synthetic Review Spamming and Defense</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Online reviews have been popularly adopted in many applications.  Since they can either promote or harm the reputation of a product or a service, buying and selling fake reviews becomes a profitable business and a big threat. In this paper, we introduce a very simple, but powerful review spamming technique that could fail the existing feature-based detection algorithms easily. It uses one truthful review as a template, and replaces its sentences with those from other reviews in a repository. Fake reviews generated by this mechanism are extremely hard to detect: Both the state-of-the-art computational approaches and human readers acquire an error rate of 35%-48%, just slightly better than a random guess. While it is challenging to detect such fake reviews, we have made solid progress in suppressing them. A novel defense method that leverages the difference of semantic flows between synthetic and truthful reviews is developed, which is able to reduce the detection error rate to approximately 22%, a significant improvement over the performance of existing approaches. Nevertheless, it is still a challenging research task to further decrease the error rate.</Data></Cell><Cell><Data ss:Type="String">Alex Morales, ; Huan Sun*, UCSB; Xifeng Yan, University of California at Santa Barbara</Data></Cell><Cell><Data ss:Type="String">Security and privacy\Spam detection*; Mining rich data types\Text; Web mining</Data></Cell><Cell><Data ss:Type="String">Review Spam, Spam Detection, Classification</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">1083</Data></Cell><Cell><Data ss:Type="String">Redundancy-Aware Maximal Cliques</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Recent research efforts have made notable progress in improving the performance of (exhaustive) maximal clique enumeration (MCE). However, exiting algorithms still suffer from exploring the huge search space of MCE. Furthermore, their results are often undesirable as many of the returned maximal cliques have large overlapping parts. This redundancy leads to problems in both computational efficiency and usefulness of MCE.

In this paper, we aim at providing a concise and complete summarization of the set of maximal cliques, which is useful to many applications. We propose the notion of \tau-visible MCE to achieve this goal and design algorithms to realize the notion. Based on the refined output space, we further consider an efficient computation of the top-$k$ results with guaranteed quality and diversity. Our experimental results demonstrate that our approach is capable of producing output of high usability and our algorithms achieve superior efficiency over classic MCE algorithms.</Data></Cell><Cell><Data ss:Type="String">Jia Wang*, Chinese University of Hong Kong; James Cheng, Chinese University of Hong Kong; Ada Wai-Chee Fu, Chinese University of Hong Kong </Data></Cell><Cell><Data ss:Type="String">Graph mining*; Mining rich data types\Unstructured; Sampling; Social\Community detection</Data></Cell><Cell><Data ss:Type="String">Maximal clique enumeration, clique summarization, clique concise representation, redundancy-aware cliques</Data></Cell></Row><Row ss:Height="90.9936"><Cell><Data ss:Type="String">1097</Data></Cell><Cell><Data ss:Type="String">Information Cartography: Creating Zoomable, Large-Scale Maps of Information</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">When information is abundant, users need support to understand complex stories, such as presidential elections. 
We propose a methodology for creating structured summaries of information, which we call metro maps. Just as cartographic maps have been relied upon for centuries to help us understand our surroundings, metro maps can help us understand the relationships between pieces of information.

Our proposed algorithm generates a concise structured set of documents that explicitly  captures story development. As different users might be interested in different granularities, the maps are zoomable, with each level of zoom showing finer details and interactions.

In this work we formalize characteristics of good maps and formulate their construction as an optimization problem. We provide efficient, scalable methods with theoretical guarantees for generating maps. Pilot user studies over real-world datasets demonstrate that the method is able to produce maps which help users acquire knowledge efficiently.
    
</Data></Cell><Cell><Data ss:Type="String">Dafna Shahaf*, Stanford; Jaewon Yang, Stanford University; Caroline  Suen, ; Jeff Jacobs, ; Heidi Wang, ; Jure Leskovec, Stanford University</Data></Cell><Cell><Data ss:Type="String">Mining rich data types*; Mining rich data types\Text; Social\Social media</Data></Cell><Cell><Data ss:Type="String">Metro Maps, Information Organization, Information Interfaces/Presentation</Data></Cell></Row><Row ss:Height="46.26"><Cell><Data ss:Type="String">1136</Data></Cell><Cell><Data ss:Type="String">Confluence: Conformity Influence in Large Social Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Conformity is a type of social influence involving a change in opinion or behavior in order to fit in with a group. Employing several social networks as the source for our experimental data, we study how the effect of conformity plays a role in changing users' online behaviors. We formally define several major types of conformity in individual, peer, and group levels.
We propose {\it Confluence} model  to formalize the effects of social conformity into a probabilistic model. Confluence can distinguish and quantify the effects of the different types of conformities. To scale up to large social networks, we propose a distributed learning method that can construct the Confluence model efficiently with near linear speedup. 

Our experimental results on four different types of large social networks, i.e., Flickr, Gowalla, Weibo and Co-Author, verify the existence of the conformity phenomena. Leveraging the conformity information, Confluence can accurately predict actions of users. Our experiments show that  Confluence can significantly improve the prediction accuracy by up to 5-10\% compared with several alternative methods. </Data></Cell><Cell><Data ss:Type="String">Jie Tang*, Tsinghua University; Sen Wu, Tsinghua University; Jimeng Sun, IBM Research </Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Social\Social media</Data></Cell><Cell><Data ss:Type="String">Conformity, Social influence, Social network</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">1148</Data></Cell><Cell><Data ss:Type="String">Mining Discriminative Subgraphs from Global-state Networks</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Global-state networks provide a powerful mechanism to model the increasing heterogeneity in data generated by current systems. Such a network comprises of a series of network snapshots with dynamic local states at nodes, and a global network state indicating the occurrence of an event. Mining discriminative subgraphs from global-state networks allows us to identify the influential sub-networks that have maximum impact on the global state and unearth the complex relationships between the local entities of a network and their collective behavior. In this paper, we explore this problem and design a technique called MINDS to mine minimally discriminative subgraphs from large global-state networks. To combat the exponential subgraph search space, we derive the concept of an edit map and perform Metropolis Hastings sampling on the map to compute the answer set. Furthermore, we formulate the idea of network-constrained decision trees to learn prediction models on a subgraph without compromising on the underlying network structure. Extensive experiments on real datasets demonstrate excellent accuracy in terms of prediction quality. Additionally, MINDS achieves a speed-up of at least two orders of magnitude over baseline techniques.</Data></Cell><Cell><Data ss:Type="String">Sayan Ranu*, IBM; Minh Hoang, UC Santa Barbara; Ambuj Singh, UC Santa Barbara</Data></Cell><Cell><Data ss:Type="String">Graph mining*; Bioinformatics; Rule and pattern mining; Sampling</Data></Cell><Cell><Data ss:Type="String">Protein interaction network, bioinformatics, graph mining</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">1162</Data></Cell><Cell><Data ss:Type="String">Scalable Text and Link Analysis with Mixed-Topic Link Models </Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Many data sets contain rich information about objects, as well as pairwise relations between them.  For instance, in networks of websites, scientific papers, and other documents, each node has content consisting of a collection of words, as well as hyperlinks or citations to other nodes. In order to perform inference on such data sets, and make predictions and recommendations, it is useful to have models that are able to capture the processes which generate the text at each node and the links between them. In this paper, we combine classic ideas in topic modeling with a variant of the mixed-membership block model recently developed in the statistical physics community.  The resulting model has the advantage that its parameters, including the mixture of topics of each document and the resulting overlapping communities, can be inferred with a simple and scalable expectation-maximization algorithm. We test our model on three data sets, performing unsupervised topic classification and link prediction. For both tasks, our model outperforms several existing state-of-the-art methods, achieving higher accuracy with significantly less computation, analyzing a data set with 1.3 million words and 44 thousand links in a few minutes.</Data></Cell><Cell><Data ss:Type="String">YAOJIA ZHU*, University of New Mexico; Xiaoran  Yan, University of New Mexico; Lise Getoor, The University of Maryland College Park; Cristopher Moore, Santa Fe Institute</Data></Cell><Cell><Data ss:Type="String">Probabilistic methods*; Social\Community detection; Social\Link prediction; Social\Social and information networks; Unsupervised learning\Clustering; Unsupervised learning\Topic, graphical and latent variable models</Data></Cell><Cell><Data ss:Type="String">Document classification, Topic modeling, Link prediction, Stochastic block model</Data></Cell></Row><Row ss:Height="79.8552"><Cell><Data ss:Type="String">1167</Data></Cell><Cell><Data ss:Type="String">Exact Sparse Recovery with L0 Projections</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Many applications concern sparse signals, for example, detecting anomalies from the differences between consecutive images taken by surveillance cameras. This paper focuses on the problem of  recovering a  $K$-sparse signal $\mathbf{x}\in\mathbb{R}^{1\times N}$, i.e., $K\ll N$ and $\sum_{i=1}^{N} 1\{x_i\neq 0\} = K$. In the  mainstream framework of compressed sensing (CS), $\mathbf{x}$ is recovered from $M$  linear measurements $\mathbf{y} = \mathbf{xS}\in\mathbb{R}^{1\times M}$, where $\mathbf{S}\in\mathbb{R}^{N\times M}$ is often a Gaussian (or Gaussian-like) design matrix.

In our proposed method, the design matrix $\mathbf{S}$ is generated from an $\alpha$-stable distribution with $\alpha\approx 0$. Our decoding algorithm  mainly requires  one linear scan of the coordinates, followed by a few iterations on a small number of coordinates which are ``undetermined'' in the previous iteration. Our practical algorithm consists of two estimators. In the first iteration, the {\em (absolute) minimum estimator} is able to  filter out a majority of the zero coordinates. The {\em gap estimator}, which is applied in each iteration, can accurately recover the magnitudes of the nonzero coordinates.  Comparisons with linear programming (LP) and orthogonal matching pursuit (OMP) demonstrate that our algorithm can be significantly faster in  decoding speed and more accurate in recovery quality, for the task of exact spare recovery. Our procedure is robust against measurement noise. Even when there are no sufficient measurements,  our algorithm can still reliably recover a significant portion of the nonzero coordinates.



</Data></Cell><Cell><Data ss:Type="String">Ping Li*, Cornell University; Cun-Hui Zhang, Rutgers University</Data></Cell><Cell><Data ss:Type="String">Dimensionality reduction*; Big Data\Novel statistical techniques for big data; Data streams; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">Sparsity, compressed sensing, random projections</Data></Cell></Row><Row ss:Index="122" ss:Height="35.0352"><Cell><Data ss:Type="String">1186</Data></Cell><Cell><Data ss:Type="String">Clustered Graph Randomization: Network Exposure to Multiple Universes</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">A/B testing is a standard approach for evaluating the effect of online experiments; the goal is to estimate the `average treatment effect' of a new feature or condition by exposing a sample of the overall population to it. A drawback with A/B testing is that it is poorly suited for experiments involving social interference, when the treatment of individuals spills over to neighboring individuals along an underlying social network. In this work, we propose a novel methodology for using graph clustering to analyze average treatment effects under social interference. To begin, we characterize graph-theoretic conditions under which individuals can be considered to be `network exposed' to an experiment. We then show how clustered graph randomization admits an efficient exact algorithm to compute the probabilities for each vertex being network exposed under several of these exposure conditions. Using these probabilities as inverse weights, a Horvitz-Thompson estimator can then provide an effect estimate that is unbiased, provided that the exposure model has been properly specified. 

Given an estimator that is unbiased, we focus on minimizing the variance. First, we develop simple sufficient conditions for the variance of the estimator to be asymptotically small in n, the size of the graph. However, for general randomization schemes, this variance can be lower bounded by an exponential function of the degrees of a graph. In contrast, we show that if a graph satisfies a condition on the growth rate of neighborhoods, then there exists a natural clustering algorithm, based on vertex neighborhoods, for which the variance of the estimator can be upper bounded by a linear function of the degrees. Thus we show that proper cluster randomization can lead to exponentially lower estimator variance when experimentally measuring average treatment effects under interference.</Data></Cell><Cell><Data ss:Type="String">Johan Ugander*, Cornell University; Brian Karrer, Facebook; Lars Backstrom, Facebook; Jon Kleinberg, Cornell</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Graph mining; Social\Community detection</Data></Cell><Cell><Data ss:Type="String">online experimentation, causal inference, network effects</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">1192</Data></Cell><Cell><Data ss:Type="String">Restreaming Graph Partitioning: Simple Versatile Algorithms for Advanced Balancing</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Partitioning large graphs is difficult, especially when performed in the limited models of computation afforded to modern large scale computing systems. In this work we introduce restreaming graph partitioning and develop algorithms that scale similarly to streaming partitioning algorithms yet empirically perform as well as fully offline algorithms. In streaming partitioning, graphs are partitioned serially in a single pass. Restreaming partitioning is motivated by scenarios where approximately the same dataset is routinely streamed, making it possible to transform streaming partitioning algorithms into an iterative procedure.
This combination of simplicity and powerful performance allows restreaming algorithms to be easily adapted to efficiently tackle more challenging partitioning objectives. In particular, we consider the problem of stratified graph partitioning, a multi-constrained partitioning problem where each of many node strata require balancing. Stratified partitioning is motivated by the challenge of studying network effects on social networks, where it is desirable to isolate disjoint dense subgraphs with matching user demographics. As such, we partition large social networks such that all partition subgraphs exhibit matching degree distributions --- a novel achievement for non-regular graphs. We again observe that restreaming partitioning can match fully offline approaches while utilizing only a fraction of the memory.
As part of our results, we observe a fundamental difference in the ease with which social graphs are partitioned when compared to web graphs. Namely, the modular structure of web graphs appears to motivate full offline optimization, whereas the locally dense structure of social graphs precludes significant gains from global manipulations. Indeed, it is on the increasingly important, and increasingly large social graphs that restreaming graph partition algorithms outperform their complex offline competitors.</Data></Cell><Cell><Data ss:Type="String">Joel Nishimura, Cornell University; Johan Ugander*, Cornell University</Data></Cell><Cell><Data ss:Type="String">Social\Social and information networks*; Big Data\Scalable methods; Graph mining; Social\Community detection</Data></Cell><Cell><Data ss:Type="String">graph partitioning, stratified sampling, stratified graph sampling</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">1199</Data></Cell><Cell><Data ss:Type="String">Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">In the internet era there has been an explosion in the amount of digital text information available, leading to difficulties of scale for traditional inference algorithms for topic models.  Recent advances in stochastic variational inference algorithms for latent Dirichlet allocation (LDA) have made it feasible to learn topic models on large-scale corpora, but these methods do not currently take full advantage of the collapsed representation of the model.  We propose a stochastic algorithm for collapsed variational Bayesian inference for LDA, which is simpler and more efficient than the state of the art method.  In experiments on large-scale text corpora, the algorithm was found to converge faster and often to a better solution than the previous method.  Human-subject experiments also demonstrated that the method can learn coherent topics in seconds on small corpora, facilitating the use of topic models in interactive document analysis software.</Data></Cell><Cell><Data ss:Type="String">James Foulds*, UC Irvine; Levi Boyles, UC Irvine; Christopher Dubois, UC Irvine; Padhraic Smyth, UC Irvine; max Welling, University of Amsterdam</Data></Cell><Cell><Data ss:Type="String">Unsupervised learning\Topic, graphical and latent variable models*; Big Data\Scalable methods; Mining rich data types\Text; Probabilistic methods; Unsupervised learning</Data></Cell><Cell><Data ss:Type="String">Topic models, latent Dirichlet allocation, online inference, stochastic inference, variational Bayesian inference</Data></Cell></Row><Row ss:Height="68.652"><Cell><Data ss:Type="String">1202</Data></Cell><Cell><Data ss:Type="String">Understanding Evolution of Research Themes: A Probabilistic Generative Model for Citations</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Understanding how research themes evolve over time in a research community is useful in many ways (e.g., revealing important milestones and discovering emerging major research trends). 
In this paper, we propose a novel way of analyzing literature citation to explore the research topics and the theme evolution by modeling article citation relations with a probabilistic generative model. 
The key idea is to represent a research paper by a ``bag of citations'' and model such a ``citation document'' with a probabilistic topic model.
We explore the extension of a particular topic model, i.e., Latent Dirichlet Allocation~(LDA), for citation analysis, and show that such a Citation-LDA can facilitate discovering of 
individual research topics as well as the theme evolution from multiple related topics, both of which in turn lead to the construction of evolution graphs for characterizing research themes. 
We test the proposed citation-LDA on two datasets: the ACL Anthology Network~(AAN) of natural language research literatures and PubMed Central~(PMC) archive of biomedical and life sciences literatures, and demonstrate that Citation-LDA can effectively discover evolution of research themes, with better formed topics than (conventional) Content-LDA.</Data></Cell><Cell><Data ss:Type="String">Xiaolong Wang*, UIUC; ChengXiang Zhai, UIUC; Dan Roth, UIUC</Data></Cell><Cell><Data ss:Type="String">Mining rich data types*; Information extraction; Probabilistic methods</Data></Cell><Cell><Data ss:Type="String">citation analysis; research theme evolution; topic-temporal analysis;</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">1205</Data></Cell><Cell><Data ss:Type="String">Exploring Consumer Psychology for Click Prediction in Sponsored Search</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">Sponsored search thrives to be the major business model for commercial search engines. Precise predicting of the probability that users click on ads plays a key role in sponsored search, since such prediction is widely used in ranking, filtering, and pricing of the ads. Many of previous studies on click prediction usually apply the machine learning approach, and take advantage of two major kinds of features, including semantic features representing relevance between ads and queries as well as historical click-through features. These existing works, however, ignored one important aspect, the commercial nature, of the sponsor search. They make little attempt to understand user clicks on ads from the perspective of consumer psychology. In this work, through a data analysis on a commercial search engine, we find that many ads in sponsored search usually leverage some specific text patterns, e.g, “official site”, “x% off”, “guaranteed return in x days”, to attract consumer psychological desires, , and those text patterns can give rise to significant difference in terms of click-through rate. Based on these observations, it becomes an important problem of how to explore the consumer psychological desires to improve the accuracy of click prediction in sponsored search. To address this issue, we first propose a method to automatically mine patterns representative for  the consumer psychological desires from ads' text. Based on extracted text patterns, we propose new features and incorporate them into the learning framework of click prediction in sponsored search. Large scale evaluations on the click-through log from a commercial search engine demonstrate that our proposed consumer psychological desire features can result in significant improvement in terms of accuracy of click prediction in sponsored search. </Data></Cell><Cell><Data ss:Type="String">Taifeng Wang*, Microsoft; Jiang Bian, ; Tie-Yan Liu, Microsoft Research</Data></Cell><Cell><Data ss:Type="String">User modeling*; Economy, markets\Online advertising; Rule and pattern mining; Supervised learning\Classification; Web mining</Data></Cell><Cell><Data ss:Type="String">Click Prediction, Sponsored Search, Psychological Advertising, Consumer Behavior</Data></Cell></Row><Row ss:Height="12.6432"><Cell><Data ss:Type="String">1287</Data></Cell><Cell><Data ss:Type="String">Model-based Kernel for Efficient Time Series Analysis</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">We present novel, efficient, model based kernels for time series data rooted in the reservoir computation framework. The kernels are implemented by fitting reservoir models sharing the same fixed deterministically constructed state transition part to individual time series. The proposed kernels can naturally handle time series of different length without the need to specify a parametric model class for the time series. Compared with other time series kernels, our kernels are computationally efficient. We show how the model distances used in the kernel can be calculated analytically or efficiently estimated. The experimental results on synthetic and benchmark time series classification tasks confirm the efficiency of the proposed kernel in terms of both generalization accuracy and computational speed. This paper also investigates on-line reservoir kernel construction for extremely long time series.</Data></Cell><Cell><Data ss:Type="String">Huanhuan Chen*, University of Birmingham; Fengzhen Tang, University of Birmingham; Peter Tino, University of Birmingham; Xin Yao, University of Birmingham</Data></Cell><Cell><Data ss:Type="String">Supervised learning\Classification*; Mining rich data types\Temporal / time series; Supervised learning\Neural networks; Supervised learning\Support vector machines</Data></Cell><Cell><Data ss:Type="String">Reservoir Computing, Time Series Classification, Kernel Methods</Data></Cell></Row><Row ss:Height="35.0352"><Cell><Data ss:Type="String">1289</Data></Cell><Cell><Data ss:Type="String">Discriminant Low-Rank Regression: On The Equivalence of Low-Rank Regression and Discriminant Analysis</Data></Cell><Cell><Data ss:Type="String">Research</Data></Cell><Cell><Data ss:Type="String">The low-rank regression model has been studied and applied to capture the underlying classes/tasks correlation patterns, such that the regression/classification results can be enhanced. In this paper, we will prove that the low-rank regression model is equivalent to doing linear regression in the linear discriminant analysis (LDA) subspace. Our new theory reveals the learning mechanism of low-rank regression, and shows that the low-rank structures exacted from classes/tasks are connected to the LDA projection results. Thus, the low-rank regression efficiently works for the high-dimensional data.

Moreover, we will propose new discriminant low-rank ridge regression and sparse low-rank regression methods. Both of them are equivalent to doing regularized regression in the regularized LDA subspace. These new regularized objectives provide better data mining results than existing low-rank regression in both theoretical and empirical validations. We evaluate our discriminant low-rank regression methods by six benchmark datasets. In all empirical results, our discriminant low-rank models consistently show better results than the corresponding full-rank methods.</Data></Cell><Cell><Data ss:Type="String">Xiao Cai, University of Texas at Arlington; Heng Huang*, University of Texas, Arlington ; Chris Ding, University of Texas,Arlington; Feiping Nie, </Data></Cell><Cell><Data ss:Type="String">Supervised learning\Classification*; Dimensionality reduction; Supervised learning</Data></Cell><Cell><Data ss:Type="String">Sparse low-rank regression</Data></Cell></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Height="12.1032"><Cell ss:Index="7"/></Row><Row ss:Index="1048576" ss:Height="12.1032"><Cell ss:Index="7"/></Row></Table><x:WorksheetOptions/></ss:Worksheet></Workbook>
